{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STA 663 Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Auto-Encoding Variational Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this file we have the final codes as well as gradient testing, and testing vs basline version to ensure accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numba import jit, njit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"sigmoid function\"\"\"\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_gradient(x):\n",
    "    \"\"\"gradient of sigmoid function\"\"\"\n",
    "    return x * (1-x)\n",
    "\n",
    "def tanh(x):\n",
    "    \"\"\"tanh function\"\"\"\n",
    "    return np.tanh(x)\n",
    "    \n",
    "def tanh_gradient(x):\n",
    "    \"\"\"gradient of tanh function\"\"\"\n",
    "    return 1-np.power(x,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Batch(M, trainX, trainy):\n",
    "    \"\"\"randomly sample a mini batch of size M from the training data\"\"\"\n",
    "    \n",
    "    N = trainX.shape[0]\n",
    "    sample = np.random.choice(N,M)\n",
    "    \n",
    "    return trainX[sample], trainy[sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_random(dx, dm, dz, option = \"xavier\"):\n",
    "    \"\"\"\n",
    "    parameter initialization\n",
    "    xavier initialization for weights\n",
    "    all zero for bias\n",
    "    can be used to initialize all zero variables for ADAM by setting \"option = zeros\"\n",
    "    \"\"\"\n",
    "    \n",
    "    # weights initialization\n",
    "    if option == \"zeros\":\n",
    "        # only for variables in ADAM algorithm, not to be used for true model parameters\n",
    "        q_W1 = np.zeros((dm, dx))\n",
    "        p_W5 = np.zeros((dx, dm))\n",
    "        q_W2 = np.zeros((dz, dm))\n",
    "        q_W3 = np.zeros((dz, dm))\n",
    "        p_W4 = np.zeros((dm, dz))\n",
    "    elif option == \"xavier\":\n",
    "        bound = np.sqrt(6)/ np.sqrt(dx + dm)\n",
    "        q_W1 = np.random.uniform(-bound, bound, (dm, dx))\n",
    "        p_W5 = np.random.uniform(-bound, bound, (dx, dm))\n",
    "        bound = np.sqrt(6)/ np.sqrt(dm + dz)\n",
    "        q_W2 = np.random.uniform(-bound, bound, (dz, dm))\n",
    "        q_W3 = np.random.uniform(-bound, bound, (dz, dm))\n",
    "        p_W4 = np.random.uniform(-bound, bound, (dm, dz))\n",
    "    \n",
    "    # bias initialization\n",
    "    q_b1 = np.zeros((dm, 1))\n",
    "    p_b5 = np.zeros((dx, 1))\n",
    "    q_b2 = np.zeros((dz, 1))\n",
    "    q_b3 = np.zeros((dz, 1))\n",
    "    p_b4 = np.zeros((dm, 1))\n",
    "    \n",
    "    W = [q_W1, q_W2, q_W3, p_W4, p_W5]\n",
    "    b = [q_b1, q_b2, q_b3, p_b4, p_b5]\n",
    "    \n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_forward_vec(X, W, b):\n",
    "    \"\"\"\n",
    "    encoder forward propagation - vectorized version\n",
    "    X: M by dx\n",
    "    \"\"\"\n",
    "    \n",
    "    q_W1, q_W2, q_W3, d,d = W\n",
    "    q_b1, q_b2, q_b3, d,d = b\n",
    "    \n",
    "    q_a1 = q_W1 @ X.T + q_b1 # dm by M\n",
    "    q_h1 = tanh_nb(q_a1) # dm by M\n",
    "    q_mu = q_W2 @ q_h1 + q_b2 # dz by M\n",
    "    q_a2 = q_W3 @ q_h1 + q_b3 # dz by M\n",
    "    q_s2 = np.exp(q_a2) # dz by M\n",
    "    \n",
    "    return q_h1.T, q_a2.T, q_mu.T, q_s2.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_z_vec(q_mu, q_s2, eps):\n",
    "    \"\"\"sample latent variable z - vectorized version\"\"\"\n",
    "    \n",
    "    M, dz = q_mu.shape \n",
    "\n",
    "    return q_mu.reshape(M,1,dz) + np.sqrt(q_s2).reshape(M,1,dz) * eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_forward_vec(W, b, z):\n",
    "    \"\"\"\n",
    "    decoder forward propagation - vectorized version\n",
    "    z: M by L by dz\n",
    "    \"\"\"\n",
    "    \n",
    "    d,d,d, p_W4, p_W5 = W\n",
    "    d,d,d, p_b4, p_b5 = b\n",
    "    \n",
    "    p_a3 = z @ p_W4.T + p_b4.T # M by L by dm\n",
    "    p_h2 = tanh_nb(p_a3) # M by L by dm\n",
    "            \n",
    "    p_a4 = p_h2 @ p_W5.T + p_b5.T # M by L by dx\n",
    "    y = sigmoid_nb(p_a4) # M by L by dx\n",
    "    \n",
    "    return y, p_h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_loss_vec(X, y, q_a2, q_mu, q_s2):\n",
    "    \"\"\"target total loss function - to minimize - vectorized version\"\"\"\n",
    "    \n",
    "    M, dx, L = X.shape[0], X.shape[1], y.shape[1]\n",
    "    \n",
    "    # reconstruction loss for each sample of latent variable\n",
    "    loss = -np.sum(X.reshape(M,1,dx) * np.log(y) + (1-X.reshape(M,1,dx))* np.log(1-y)) / L\n",
    "    \n",
    "    # KL divergence/ regularization\n",
    "    loss = (loss + np.sum(np.power(q_mu,2) + q_s2 - q_a2 - 1)/2)/M\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def para_update(W, b, dW, db, alpha):\n",
    "    \"\"\"update weights and bias for gradient descent\"\"\"\n",
    "    \n",
    "    assert len(W) == len(b) == len(dW) == len(db)\n",
    "    n = len(W)\n",
    "    \n",
    "    for i in range(n):\n",
    "        W[i] = W[i] - alpha * dW[i]\n",
    "        b[i] = b[i] - alpha * db[i]\n",
    "    \n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid_nb = jit(sigmoid, nopython=True, cache=True)\n",
    "sigmoid_gradient_nb = jit(sigmoid_gradient, nopython=True, cache=True)\n",
    "tanh_nb = jit(tanh, nopython=True, cache=True)\n",
    "tanh_gradient_nb = jit(tanh_gradient, nopython=True, cache=True)\n",
    "get_Batch_nb = jit(get_Batch, nopython=True)\n",
    "init_random_nb1 = jit(init_random, nopython=True)\n",
    "sample_z_vec_nb = jit(sample_z_vec, nopython=True, cache=True)\n",
    "encoder_forward_vec_nb1 = jit(encoder_forward_vec, nopython=True, cache=True)\n",
    "decoder_forward_vec_nb1 = jit(decoder_forward_vec, cache=True)\n",
    "total_loss_vec_nb = jit(total_loss_vec, nopython=True, cache=True)\n",
    "para_update_nb = jit(para_update, nopython=True, cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(cache=True)\n",
    "def batch_forward_vec_nb2(Spec, X, W, b, eps):\n",
    "    \"\"\"forward propagation for one mini batch - full vectorized version\"\"\"\n",
    "    d, M, L, d, dx, dm, dz, d, d = Spec\n",
    "\n",
    "    p_h2 = np.zeros((M, L, dm))\n",
    "    z = np.zeros((M, L, dz))\n",
    "    y = np.zeros((M, L, dx))\n",
    "    \n",
    "    q_h1, q_a2, q_mu, q_s2 = encoder_forward_vec_nb1(X, W, b)\n",
    "    z = sample_z_vec_nb(q_mu, q_s2, eps)\n",
    "    y, p_h2 = decoder_forward_vec_nb1(W, b, z)\n",
    "    \n",
    "    loss = total_loss_vec_nb(X, y, q_a2, q_mu, q_s2)\n",
    "\n",
    "    return y, q_h1, p_h2, q_mu, q_s2, z, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def grad_vec2_nb2(X, y, q_W1, q_W2, q_W3, p_W4, p_W5, q_b1, q_b2, q_b3, p_b4, p_b5, q_h1, p_h2, q_mu, q_s2, z, eps):\n",
    "    \"\"\"\"\n",
    "    batch gradient calculation - vectorized version\n",
    "    not using lists for parameters to enable numba nopython\n",
    "    \n",
    "    inputs:\n",
    "        X: Data [M by dx]\n",
    "        y: Model results [M by L by dx]\n",
    "        q_W1, q_W2, q_W3: Weights for encoder\n",
    "        p_W4, p_W5: Weights for decoder\n",
    "        q_b1, q_b2, q_b3: Bias for encoder\n",
    "        p_b4, p_b5: Bias for decoder\n",
    "        q_h1 (M by dm), p_h2 (M by L by dm): intermediate activation variables\n",
    "        eps (L by dz), z (M by L by dz), q_s2 (M by dz), q_mu (M by dz): for sampling latent variables from posterior\n",
    "    \"\"\"\n",
    "    \n",
    "    M = X.shape[0]\n",
    "    L = y.shape[1]\n",
    "    \n",
    "    # initialize gradient variables\n",
    "    \n",
    "    # L: loss; R: regularization; J: total target\n",
    "    dL_dW1 = dJ_dW1 = dR_dW1 = np.zeros_like(q_W1)\n",
    "    dL_db1 = dJ_db1 = dR_db1 = np.zeros_like(q_b1)\n",
    "    dL_dW2 = dJ_dW2 = dR_dW2 = np.zeros_like(q_W2)\n",
    "    dL_db2 = dJ_db2 = dR_db2 = np.zeros_like(q_b2)\n",
    "    dL_dW3 = dJ_dW3 = dR_dW3 = np.zeros_like(q_W3)\n",
    "    dL_db3 = dJ_db3 = dR_db3 = np.zeros_like(q_b3)\n",
    "    dL_dW4 = dJ_dW4 = np.zeros_like(p_W4)\n",
    "    dL_db4 = dJ_db4 = np.zeros_like(p_b4)    \n",
    "    dL_dW5 = dJ_dW5 = np.zeros_like(p_W5)\n",
    "    dL_db5 = dJ_db5 = np.zeros_like(p_b5)\n",
    "\n",
    "    # back propagation for loss\n",
    "    for iL in range(L):\n",
    "        y_iL = y[:,iL,:] # M by dx\n",
    "        p_h2_iL = p_h2[:,iL,:] # M by dm\n",
    "        z_iL = z[:,iL,:] # M by dz\n",
    "        \n",
    "        L_d4 = y_iL - X # M by dx\n",
    "        dL_dW5 = dL_dW5 + L_d4.T @ p_h2_iL # dx by dm\n",
    "        dL_db5 = dL_db5 + np.sum(L_d4, axis = 0).reshape(-1,1) # dx by 1\n",
    "        \n",
    "        L_d3 = L_d4 @ p_W5 * tanh_gradient_nb(p_h2_iL) # M by dm\n",
    "        dL_dW4 = dL_dW4 + L_d3.T @ z_iL # dm by dz\n",
    "        dL_db4 = dL_db4 + np.sum(L_d3, axis = 0).reshape(-1,1) # dm by 1\n",
    "        \n",
    "        L_d22 = L_d3 @ p_W4 * eps[iL,] * np.sqrt(q_s2) / 2  # M by dz\n",
    "        dL_dW3 = dL_dW3 + L_d22.T @ q_h1 # dz by dm\n",
    "        dL_db3 = dL_db3 + np.sum(L_d22, axis = 0).reshape(-1,1) # dz by 1\n",
    "        \n",
    "        L_d21 = L_d3 @ p_W4 # M by dz\n",
    "        dL_dW2 = dL_dW2 + L_d21.T @ q_h1 # dz by dm\n",
    "        dL_db2 = dL_db2 + np.sum(L_d21, axis = 0).reshape(-1,1) # dz by 1\n",
    "\n",
    "        L_d1 = (L_d21 @ q_W2 + L_d22 @ q_W3) * tanh_gradient_nb(q_h1) # M by dm\n",
    "        dL_dW1 = dL_dW1 + L_d1.T @ X # dm by dx\n",
    "        dL_db1 = dL_db1 + np.sum(L_d1, axis = 0).reshape(-1,1) # dm by 1\n",
    "\n",
    "    # back propagation for regularization\n",
    "    R_d22 = (q_s2 - 1)/2 # M by dz\n",
    "    dR_dW3 = dR_dW3 + R_d22.T @ q_h1 # dz by dm\n",
    "    dR_db3 = dR_db3 + np.sum(R_d22, axis = 0).reshape(-1,1) # dz by 1\n",
    "    \n",
    "    R_d21 = q_mu # M by dz\n",
    "    dR_dW2 = dR_dW2 + R_d21.T @ q_h1 # dz by dm\n",
    "    dR_db2 = dR_db2 + np.sum(R_d21, axis = 0).reshape(-1,1) # dm by 1\n",
    "\n",
    "    R_d1 = (R_d22 @ q_W3 + R_d21 @ q_W2) * tanh_gradient_nb(q_h1) # M by dm\n",
    "    dR_dW1 = dR_dW1 + R_d1.T @ X # dm by dx\n",
    "    dR_db1 = dR_db1 + np.sum(R_d1, axis = 0).reshape(-1,1) # dm by 1\n",
    "    \n",
    "    dJ_dW1 = dL_dW1 / L / M + dR_dW1 / M\n",
    "    dJ_db1 = dL_db1 / L / M + dR_db1 / M\n",
    "    dJ_dW2 = dL_dW2 / L / M + dR_dW2 / M\n",
    "    dJ_db2 = dL_db2 / L / M + dR_db2 / M    \n",
    "    dJ_dW3 = dL_dW3 / L / M + dR_dW3 / M\n",
    "    dJ_db3 = dL_db3 / L / M + dR_db3 / M\n",
    "    dJ_dW4 = dL_dW4 / L / M\n",
    "    dJ_db4 = dL_db4 / L / M \n",
    "    dJ_dW5 = dL_dW5 / L / M\n",
    "    dJ_db5 = dL_db5 / L / M\n",
    "    \n",
    "    return dJ_dW1, dJ_dW2, dJ_dW3, dJ_dW4, dJ_dW5, dJ_db1, dJ_db2, dJ_db3, dJ_db4, dJ_db5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(parallel=True, cache=True)\n",
    "def train_AEVB(trainX, trainy, nBatch, M = 100, L = 1, std_const = 255, dm = 500, dz = 3, alpha = 0.005, beta1 = 0.9, beta2 = 0.999, eps_stable = 1e-8, W = \"\", b = \"\", nP = 0):\n",
    "    \"\"\"\n",
    "    AEVB model as described in the paper\n",
    "    Diederik P Kingma, Max Welling\n",
    "    Auto-Encoding Variational Bayes (2013).\n",
    "    \n",
    "    Training using ADAM algorithm as described in the paper\n",
    "    Diederik P Kingma, Jimmy Ba\n",
    "    Adam: A Method for Stochastic Optimization (2014).\n",
    "    \n",
    "    Input parameters：\n",
    "    ----------\n",
    "    trainX: array_like\n",
    "            Training dataset inputs.\n",
    "            Dimension: number of sample by dim1 by dim2 ...\n",
    "    trainy: array_like\n",
    "            Training dataset labels. \n",
    "            This variable is not currently used in the function. For further developments.\n",
    "    nBatch: integer\n",
    "            Number of mini-batch to train.\n",
    "    M: integer, optional\n",
    "            Size of mini-batch.\n",
    "            Default at 100 as recommended in the paper.\n",
    "    L: integer, optional\n",
    "            Number of latent variable to sample.\n",
    "            Default at 1 as recommended in the paper.\n",
    "    std_const: scalar, optional\n",
    "            Normlizing constant for data.\n",
    "            Currently default at 255 which is usually used for black and white image data.\n",
    "    dm: integer, optional\n",
    "            Dimension for middle layer of the encoder and decoder.\n",
    "            Default at 500 which used for MNIST dataset in the paper.\n",
    "    dz: integer, optional\n",
    "            Dimension for latent variables. \n",
    "            Currently default at 3\n",
    "    alpha: float, optional\n",
    "            Learning rate.\n",
    "            Default at 0.005.\n",
    "    beta1: float, optional\n",
    "            Parameter for ADAM.\n",
    "            Default at 0.9.\n",
    "    beta2: float, optional\n",
    "            Parameter for ADAM.\n",
    "            Default at 0.999.\n",
    "    eps_stable: float, optional\n",
    "            Parameter for ADAM.\n",
    "            Default at 1e-08.\n",
    "    W: list, optional\n",
    "            List of model weights parameters, same format as function output variable W.\n",
    "            In case user wants to start training from existing parameters.\n",
    "    b: list, optional\n",
    "            List of model bias parameters, same format as function output variable b.\n",
    "            In case user wants to start training from existing parameters.\n",
    "    nP: integer, optional\n",
    "            If specified with non-zero number, function will print out \n",
    "            status message after completing every nP batches.\n",
    "    \n",
    "    Returns:\n",
    "    ----------\n",
    "    W: list\n",
    "            List of model weights parameters in the order of: q_W1, q_W2, q_W3, p_W4, p_W5.\n",
    "            q_W1, q_W2, q_W3: weights for Gaussian MLP encoder as specified in paper Appendix C.\n",
    "            p_W4, p_W5: weights for Bernoulli MLP decoder as specified in paper Appendix C\n",
    "    b: list\n",
    "            List of model bias parameters in the order of q_b1, q_b2, q_b3, p_b4, p_b5.\n",
    "            q_b1, q_b2, q_b3: bias for Gaussian MLP encoder as specified in paper Appendix C.\n",
    "            p_b4, p_b5: weights for Bernoulli MLP decoder as specified in paper Appendix C.\n",
    "    loss: array-like\n",
    "            Array which stores total loss for each mini-batch.\n",
    "    \"\"\"\n",
    "    \n",
    "    dx = trainX.shape[1]\n",
    "    Spec = [nBatch, M, L, std_const, dx, dm, dz, alpha, nP]\n",
    "    \n",
    "    # initiate parameters for ADAM\n",
    "    # need to use separate lines of codes otherwise they share the same reference\n",
    "    v_dW, v_db = init_random_nb1(dx, dm, dz, option = \"zeros\")\n",
    "    s_dW, s_db = init_random_nb1(dx, dm, dz, option = \"zeros\")\n",
    "    vc_dW, vc_db = init_random_nb1(dx, dm, dz, option = \"zeros\")\n",
    "    sc_dW, sc_db = init_random_nb1(dx, dm, dz, option = \"zeros\")\n",
    "    num_para = len(v_dW)\n",
    "    \n",
    "    # weights and bias initialization\n",
    "    if len(W) == len(b) == 0:\n",
    "        W, b = init_random_nb1(dx, dm, dz)\n",
    "    \n",
    "    q_W1, q_W2, q_W3, p_W4, p_W5 = W\n",
    "    q_b1, q_b2, q_b3, p_b4, p_b5 = b\n",
    "\n",
    "    # loss\n",
    "    loss = np.zeros(nBatch)\n",
    "    \n",
    "    for iB in range(nBatch):\n",
    "        # sample a random batch\n",
    "        batchX, batchy = get_Batch_nb(M, trainX, trainy)\n",
    "        X = batchX.reshape(M, dx) / std_const\n",
    "\n",
    "        # sample random noise for latent variable, assuming each batch uses the same random noise for now\n",
    "        eps = np.random.randn(L, dz)\n",
    "\n",
    "        y, q_h1, p_h2, q_mu, q_s2, z, loss[iB] = batch_forward_vec_nb2(Spec, X, W, b, eps)\n",
    "        \n",
    "        dJ_dW1, dJ_dW2, dJ_dW3, dJ_dW4, dJ_dW5, dJ_db1, dJ_db2, dJ_db3, dJ_db4, dJ_db5 =\\\n",
    "        grad_vec2_nb2(X, y, q_W1, q_W2, q_W3, p_W4, p_W5, q_b1, q_b2, q_b3, p_b4, p_b5, q_h1, p_h2, q_mu, q_s2, z, eps)\n",
    "        \n",
    "        dW = [dJ_dW1, dJ_dW2, dJ_dW3, dJ_dW4, dJ_dW5]\n",
    "        db = [dJ_db1, dJ_db2, dJ_db3, dJ_db4, dJ_db5]\n",
    "        \n",
    "        # ADAM\n",
    "        for i in range(num_para):\n",
    "            v_dW[i] = beta1*v_dW[i] + (1-beta1)*dW[i]\n",
    "            v_db[i] = beta1*v_db[i] + (1-beta1)*db[i]\n",
    "            s_dW[i] = beta2*s_dW[i] + (1-beta2)*np.power(dW[i],2)\n",
    "            s_db[i] = beta2*s_db[i] + (1-beta2)*np.power(db[i],2)\n",
    "        \n",
    "            vc_dW[i] = v_dW[i]/(1-beta1**(iB+1))\n",
    "            vc_db[i] = v_db[i]/(1-beta1**(iB+1))\n",
    "            sc_dW[i] = s_dW[i]/(1-beta2**(iB+1))\n",
    "            sc_db[i] = s_db[i]/(1-beta2**(iB+1))\n",
    "        \n",
    "            dW[i] = vc_dW[i] / (np.sqrt(sc_dW[i]) + eps_stable)\n",
    "            db[i] = vc_db[i] / (np.sqrt(sc_db[i]) + eps_stable)\n",
    "        \n",
    "        W, b = para_update_nb(W, b, dW, db, alpha)\n",
    "        \n",
    "        if (nP != 0) and (iB+1) % nP == 0:\n",
    "            print(\"Batch \" + str(iB+1) + \" completed.\")\n",
    "\n",
    "    return W, b, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_samples(trainX, trainy, W, b, Xdim1=0, Xdim2=0, std_const = 255):\n",
    "    \"\"\"\n",
    "    Randomly sample 9 figures from training data, reconstruct based on \n",
    "    user specified model parameters and plot for comparison.\n",
    "    \n",
    "    Input parameters：\n",
    "    ----------\n",
    "    trainX: array_like\n",
    "            Training dataset inputs.\n",
    "            Dimension: number of sample by dim1 by dim2 ...\n",
    "    trainy: array_like\n",
    "            Training dataset labels. \n",
    "            This variable is not currently used in the function. For further developments.\n",
    "    W: list\n",
    "            List of model weights parameters, same format as train_AEVB function output variable W.\n",
    "    b: list\n",
    "            List of model bias parameters, same format as train_AEVB function output variable b.\n",
    "    std_const: integer, optional\n",
    "            Normlizing constant to reconstruct data.\n",
    "            Currently default at 255 which is usually used for black and white image data.            \n",
    "    \n",
    "    Output:\n",
    "    ----------\n",
    "    9 random sampled figures from training data and the model-reconstructed ones for comparison\n",
    "    \"\"\"\n",
    "\n",
    "    dx = trainX.shape[1]\n",
    "    M = 9\n",
    "    L = 1\n",
    "    dz, dm = W[1].shape[0], W[1].shape[1]\n",
    "    if Xdim1 == 0 and Xdim2 == 0:\n",
    "        Xdim1 = Xdim2 = int(np.sqrt(dx))\n",
    "    Spec = [1, M, L, 255, dx, dm, dz, 0.005, 0]\n",
    "\n",
    "    batchX, batchy = get_Batch_nb(M, trainX, trainy)\n",
    "    X = batchX.reshape(M, dx) / std_const\n",
    "    eps = np.zeros((L, dz))\n",
    "\n",
    "    y, q_h1, p_h2, q_mu, q_s2, z, loss = batch_forward_vec_nb2(Spec, X, W, b, eps)\n",
    "    \n",
    "    for i in range(M):\n",
    "        # define subplot\n",
    "        plt.subplot(330 + 1 + i)\n",
    "        # plot raw pixel data\n",
    "        plt.imshow(batchX[i].reshape(Xdim1, Xdim2), cmap=plt.get_cmap('gray'))\n",
    "    # show the figure\n",
    "    plt.show()\n",
    "    \n",
    "    for i in range(M):\n",
    "        # define subplot\n",
    "        plt.subplot(330 + 1 + i)\n",
    "        # plot raw pixel data\n",
    "        plt.imshow(y[i,L-1,].reshape(Xdim1, Xdim2) * std_const, cmap=plt.get_cmap('gray'))\n",
    "    # show the figure\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is to plot random 9 figures from MNIST dataset based on our own learned parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "(trainX, trainy), (testX, testy) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_para(filename, **kwargs):\n",
    "    \"\"\"save weights and bias parameters\"\"\"\n",
    "    file = open(filename, 'wb')\n",
    "    pickle.dump(kwargs, file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Gradient checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 0.00001\n",
    "nBatch = 1 # number of mini-batch to train\n",
    "M = 100 # batch size\n",
    "L = 2 # sample size\n",
    "std_const = 255 # to standardize data\n",
    "\n",
    "Xdim1, Xdim2 = trainX[0].shape[0], trainX[0].shape[1]\n",
    "dx = Xdim1 * Xdim2 # dimension of the input\n",
    "dm = 500 # dimension of the hidden layer\n",
    "dz = 3 # dimension of latent variable\n",
    "\n",
    "alpha = 0.005 # learning rate\n",
    "\n",
    "nP = 1000 # print out status every nP batches\n",
    "\n",
    "Spec = [nBatch, M, L, std_const, dx, dm, dz, alpha, nP]\n",
    "\n",
    "W,b = init_random(dx, dm, dz)\n",
    "q_W1, q_W2, q_W3, p_W4, p_W5 = W\n",
    "q_b1, q_b2, q_b3, p_b4, p_b5 = b\n",
    "eps = np.random.randn(L, dz)\n",
    "batchX, batchy = get_Batch(M, trainX, trainy)\n",
    "X = batchX.reshape(M, dx) / std_const"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_forward(Spec, X, W, b, eps):\n",
    "    \"\"\"forward propagation for one batch\"\"\"\n",
    "    d, M, L, d, d, dm, dz, d, d = Spec\n",
    "    \n",
    "    # initialize variables for calculating gradients/ loss by batch\n",
    "    q_h1 = np.zeros((M, dm))\n",
    "    q_mu = np.zeros((M, dz))\n",
    "    q_s2 = np.zeros((M, dz))\n",
    "    q_a2 = np.zeros((M, dz))\n",
    "    p_h2 = np.zeros((M, L, dm))\n",
    "    z = np.zeros((M, L, dz))\n",
    "    y = np.zeros((M, L, dx))\n",
    "    \n",
    "    for iD in range(M):\n",
    "        q_h1[iD,], q_a2[iD,], q_mu[iD,], q_s2[iD,] = encoder_forward(X[iD,].reshape(-1,1), W, b)\n",
    "\n",
    "        for iL in range(L):\n",
    "            z[iD,iL,] = sample_z(q_mu[iD,], q_s2[iD,], eps[iL,])\n",
    "            y[iD,iL,], p_h2[iD,iL,] = decoder_forward(W, b, z[iD,iL,].reshape(-1,1))\n",
    "    \n",
    "    H = [q_h1, p_h2]\n",
    "    Lt = [q_mu, q_s2, z, eps]\n",
    "    loss = total_loss(X, y, q_a2, q_mu, q_s2)\n",
    "    return y, H, Lt, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_forward(X, W, b):\n",
    "    \"\"\"\n",
    "    encoder forward propagation for each data point\n",
    "    X: dx by 1\n",
    "    \"\"\"\n",
    "    \n",
    "    q_W1, q_W2, q_W3, d,d = W\n",
    "    q_b1, q_b2, q_b3, d,d = b\n",
    "    \n",
    "    q_a1 = q_W1 @ X + q_b1\n",
    "    q_h1 = tanh(q_a1)\n",
    "    q_mu = q_W2 @ q_h1 + q_b2\n",
    "    q_a2 = q_W3 @ q_h1 + q_b3\n",
    "    q_s2 = np.exp(q_a2)\n",
    "    \n",
    "    return q_h1.T, q_a2.T, q_mu.T, q_s2.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_z(q_mu, q_s2, eps):\n",
    "    \"\"\"sample z\"\"\"\n",
    "    return q_mu + np.sqrt(q_s2) * eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_forward(W, b, z):\n",
    "    \"\"\"\n",
    "    decoder forward propagation for each data point and each sample latent variable\n",
    "    z: dz by 1\n",
    "    \"\"\"\n",
    "    \n",
    "    d,d,d, p_W4, p_W5 = W\n",
    "    d,d,d, p_b4, p_b5 = b\n",
    "    \n",
    "    p_a3 = p_W4 @ z + p_b4\n",
    "    p_h2 = tanh(p_a3)\n",
    "            \n",
    "    p_a4 = p_W5 @ p_h2 + p_b5\n",
    "    y = sigmoid(p_a4)\n",
    "    \n",
    "    return y.T, p_h2.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_loss(X, y, q_a2, q_mu, q_s2):\n",
    "    \"\"\"target total loss function - to minimize\"\"\"\n",
    "    \n",
    "    M = X.shape[0]\n",
    "    L = y.shape[1]\n",
    "    \n",
    "    loss = 0\n",
    "    for iD in range(M):\n",
    "        for iL in range(L):\n",
    "            # reconstruction loss for each sample of latent variable\n",
    "            loss = loss - np.sum(X[iD,] * np.log(y[iD,iL,]) + (1-X[iD,]) * np.log(1-y[iD,iL,])) / L\n",
    "        \n",
    "        # KL divergence/ regularization\n",
    "        loss = loss + np.sum(np.power(q_mu[iD,],2) + q_s2[iD,] - q_a2[iD,] - 1)/2\n",
    "    \n",
    "    return loss / M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(X, y, W, b, H, Lt):\n",
    "    \"\"\"\"\n",
    "    batch gradient\n",
    "    inputs:\n",
    "        X: M by dx\n",
    "        y: M by L by dx\n",
    "        W: weights\n",
    "        b: bias\n",
    "        H: q_h1 (M by dm), p_h2 (M by L by dm)\n",
    "        Lt: eps (L by dz), z (M by L by dz), q_s2 (M by dz), q_mu (M by dz)\n",
    "    \"\"\"\n",
    "    q_W1, q_W2, q_W3, p_W4, p_W5 = W\n",
    "    q_b1, q_b2, q_b3, p_b4, p_b5 = b\n",
    "    q_h1, p_h2 = H\n",
    "    q_mu, q_s2, z, eps = Lt\n",
    "    \n",
    "    M = X.shape[0]\n",
    "    L = y.shape[1]\n",
    "    \n",
    "    # initialize gradient variables\n",
    "    \n",
    "    # L: loss; R: regularization; J: total target\n",
    "    dL_dW1 = dJ_dW1 = dR_dW1 = np.zeros_like(q_W1)\n",
    "    dL_db1 = dJ_db1 = dR_db1 = np.zeros_like(q_b1)\n",
    "    dL_dW2 = dJ_dW2 = dR_dW2 = np.zeros_like(q_W2)\n",
    "    dL_db2 = dJ_db2 = dR_db2 = np.zeros_like(q_b2)\n",
    "    dL_dW3 = dJ_dW3 = dR_dW3 = np.zeros_like(q_W3)\n",
    "    dL_db3 = dJ_db3 = dR_db3 = np.zeros_like(q_b3)\n",
    "    dL_dW4 = dJ_dW4 = np.zeros_like(p_W4)\n",
    "    dL_db4 = dJ_db4 = np.zeros_like(p_b4)    \n",
    "    dL_dW5 = dJ_dW5 = np.zeros_like(p_W5)\n",
    "    dL_db5 = dJ_db5 = np.zeros_like(p_b5)\n",
    "    \n",
    "    for iD in range(M):\n",
    "        for iL in range(L):\n",
    "            # back propagation for loss\n",
    "\n",
    "            #L_d4 = (np.divide(y[iD,iL,]-X[iD,], y[iD,iL,] * (1-y[iD,iL,])) * sigmoid_gradient(y[iD,iL,])).reshape(-1,1)\n",
    "            \n",
    "            # simplied L_d4\n",
    "            L_d4 = (y[iD,iL,]-X[iD,]).reshape(-1,1)\n",
    "            dL_dW5 = dL_dW5 + L_d4 @ p_h2[iD,iL,].reshape(1,-1)\n",
    "            dL_db5 = dL_db5 + L_d4\n",
    "            \n",
    "            L_d3 = p_W5.T @ L_d4 * tanh_gradient(p_h2[iD,iL,]).reshape(-1,1)\n",
    "            dL_dW4 = dL_dW4 + L_d3 @ z[iD,iL,].reshape(1,-1)\n",
    "            dL_db4 = dL_db4 + L_d3\n",
    "            \n",
    "            L_d22 = p_W4.T @ L_d3 * eps[iL,].reshape(-1,1) * np.sqrt(q_s2[iD,]).reshape(-1,1) / 2\n",
    "            dL_dW3 = dL_dW3 + L_d22 @ q_h1[iD,].reshape(1,-1)\n",
    "            dL_db3 = dL_db3 + L_d22\n",
    "            \n",
    "            L_d21 = p_W4.T @ L_d3\n",
    "            dL_dW2 = dL_dW2 + L_d21 @ q_h1[iD,].reshape(1,-1)\n",
    "            dL_db2 = dL_db2 + L_d21\n",
    "            \n",
    "            L_d1 = (q_W2.T @ L_d21 + q_W3.T @ L_d22) * tanh_gradient(q_h1[iD,]).reshape(-1,1)\n",
    "            dL_dW1 = dL_dW1 + L_d1 @ X[iD,].reshape(1,-1)\n",
    "            dL_db1 = dL_db1 + L_d1\n",
    "        \n",
    "        # back propagation for regularization\n",
    "        R_d22 = ((q_s2[iD,]-1)/2).reshape(-1,1)\n",
    "        dR_dW3 = dR_dW3 + R_d22 @ q_h1[iD,].reshape(1,-1)\n",
    "        dR_db3 = dR_db3 + R_d22\n",
    "\n",
    "        R_d21 = q_mu[iD,].reshape(-1,1)\n",
    "        dR_dW2 = dR_dW2 + R_d21 @ q_h1[iD,].reshape(1,-1)\n",
    "        dR_db2 = dR_db2 + R_d21\n",
    "\n",
    "        R_d1 = (q_W3.T @ R_d22 + q_W2.T @ R_d21) * tanh_gradient(q_h1[iD,]).reshape(-1,1)\n",
    "        dR_dW1 = dR_dW1 + R_d1 @ X[iD,].reshape(1,-1)\n",
    "        dR_db1 = dR_db1 + R_d1\n",
    "    \n",
    "    dJ_dW1 = dL_dW1 / L / M + dR_dW1 / M\n",
    "    dJ_db1 = dL_db1 / L / M + dR_db1 / M\n",
    "    dJ_dW2 = dL_dW2 / L / M + dR_dW2 / M\n",
    "    dJ_db2 = dL_db2 / L / M + dR_db2 / M    \n",
    "    dJ_dW3 = dL_dW3 / L / M + dR_dW3 / M\n",
    "    dJ_db3 = dL_db3 / L / M + dR_db3 / M\n",
    "    dJ_dW4 = dL_dW4 / L / M\n",
    "    dJ_db4 = dL_db4 / L / M \n",
    "    dJ_dW5 = dL_dW5 / L / M\n",
    "    dJ_db5 = dL_db5 / L / M\n",
    "    \n",
    "    dW = [dJ_dW1, dJ_dW2, dJ_dW3, dJ_dW4, dJ_dW5]\n",
    "    db = [dJ_db1, dJ_db2, dJ_db3, dJ_db4, dJ_db5]\n",
    "    \n",
    "    return dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, H, Lt, loss1 = batch_forward(Spec, X, W, b, eps)\n",
    "q_h1, p_h2 = H\n",
    "q_mu, q_s2, z, eps = Lt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-b95d3c7e2b3a>:42: NumbaPerformanceWarning: '@' is faster on contiguous arrays, called on (array(float64, 2d, F), array(float64, 2d, A))\n",
      "  dL_dW5 = dL_dW5 + L_d4.T @ p_h2_iL # dx by dm\n",
      "<ipython-input-12-b95d3c7e2b3a>:46: NumbaPerformanceWarning: '@' is faster on contiguous arrays, called on (array(float64, 2d, F), array(float64, 2d, A))\n",
      "  dL_dW4 = dL_dW4 + L_d3.T @ z_iL # dm by dz\n",
      "/opt/conda/lib/python3.6/site-packages/numba/typing/npydecl.py:958: NumbaPerformanceWarning: '@' is faster on contiguous arrays, called on (array(float64, 2d, F), array(float64, 2d, A))\n",
      "  warnings.warn(NumbaPerformanceWarning(msg))\n",
      "/opt/conda/lib/python3.6/site-packages/numba/typing/npydecl.py:958: NumbaPerformanceWarning: '@' is faster on contiguous arrays, called on (array(float64, 2d, F), array(float64, 2d, A))\n",
      "  warnings.warn(NumbaPerformanceWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "dL_dW1, dL_dW2, dL_dW3, dL_dW4, dL_dW5, dL_db1, dL_db2, dL_db3, dL_db4, dL_db5 =\\\n",
    "grad_vec2_nb2(X, y, q_W1, q_W2, q_W3, p_W4, p_W5, q_b1, q_b2, q_b3, p_b4, p_b5, q_h1, p_h2, q_mu, q_s2, z, eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var manual grad func\n",
      "W5:  742.0446729668128 742.0446729570698\n",
      "b5:  297.2612658709295 297.2612658696403\n",
      "W4:  15.756503574948509 15.75650358300687\n",
      "b4:  -8.287481102797756 -8.287481116570488\n",
      "W3:  5.787541698509812 5.787541682365959\n",
      "b3:  1.800083208536307 1.8000831970383162\n",
      "W2:  -8.895183037793686 -8.895183020498987\n",
      "b2:  -3.4511241267409782 -3.451124101918673\n",
      "W1:  -352.4493544034612 -352.4495336590897\n",
      "b1:  -3.7860770135011985 -3.7860770110999162\n"
     ]
    }
   ],
   "source": [
    "# gradient checking\n",
    "p_W5_add = p_W5 + delta\n",
    "p_W5_red = p_W5 - delta\n",
    "\n",
    "p_b5_add = p_b5 + delta\n",
    "p_b5_red = p_b5 - delta\n",
    "\n",
    "p_W4_add = p_W4 + delta\n",
    "p_W4_red = p_W4 - delta\n",
    "\n",
    "p_b4_add = p_b4 + delta\n",
    "p_b4_red = p_b4 - delta\n",
    "\n",
    "q_W3_add = q_W3 + delta\n",
    "q_W3_red = q_W3 - delta\n",
    "\n",
    "q_b3_add = q_b3 + delta\n",
    "q_b3_red = q_b3 - delta\n",
    "\n",
    "q_W2_add = q_W2 + delta\n",
    "q_W2_red = q_W2 - delta\n",
    "\n",
    "q_b2_add = q_b2 + delta\n",
    "q_b2_red = q_b2 - delta\n",
    "\n",
    "q_W1_add = q_W1 + delta\n",
    "q_W1_red = q_W1 - delta\n",
    "\n",
    "q_b1_add = q_b1 + delta\n",
    "q_b1_red = q_b1 - delta\n",
    "\n",
    "W5_add = [q_W1, q_W2, q_W3, p_W4, p_W5_add]\n",
    "W5_red = [q_W1, q_W2, q_W3, p_W4, p_W5_red]\n",
    "\n",
    "b5_add = [q_b1, q_b2, q_b3, p_b4, p_b5_add]\n",
    "b5_red = [q_b1, q_b2, q_b3, p_b4, p_b5_red]\n",
    "\n",
    "W4_add = [q_W1, q_W2, q_W3, p_W4_add, p_W5]\n",
    "W4_red = [q_W1, q_W2, q_W3, p_W4_red, p_W5]\n",
    "\n",
    "b4_add = [q_b1, q_b2, q_b3, p_b4_add, p_b5]\n",
    "b4_red = [q_b1, q_b2, q_b3, p_b4_red, p_b5]\n",
    "\n",
    "W3_add = [q_W1, q_W2, q_W3_add, p_W4, p_W5]\n",
    "W3_red = [q_W1, q_W2, q_W3_red, p_W4, p_W5]\n",
    "\n",
    "b3_add = [q_b1, q_b2, q_b3_add, p_b4, p_b5]\n",
    "b3_red = [q_b1, q_b2, q_b3_red, p_b4, p_b5]\n",
    "\n",
    "W2_add = [q_W1, q_W2_add, q_W3, p_W4, p_W5]\n",
    "W2_red = [q_W1, q_W2_red, q_W3, p_W4, p_W5]\n",
    "\n",
    "b2_add = [q_b1, q_b2_add, q_b3, p_b4, p_b5]\n",
    "b2_red = [q_b1, q_b2_red, q_b3, p_b4, p_b5]\n",
    "\n",
    "W1_add = [q_W1_add, q_W2, q_W3, p_W4, p_W5]\n",
    "W1_red = [q_W1_red, q_W2, q_W3, p_W4, p_W5]\n",
    "\n",
    "b1_add = [q_b1_add, q_b2, q_b3, p_b4, p_b5]\n",
    "b1_red = [q_b1_red, q_b2, q_b3, p_b4, p_b5]\n",
    "\n",
    "d,d,d,loss_W5_add = batch_forward(Spec, X, W5_add, b, eps)\n",
    "d,d,d,loss_W5_red = batch_forward(Spec, X, W5_red, b, eps)\n",
    "\n",
    "d,d,d,loss_b5_add = batch_forward(Spec, X, W, b5_add, eps)\n",
    "d,d,d,loss_b5_red = batch_forward(Spec, X, W, b5_red, eps)\n",
    "\n",
    "d,d,d,loss_W4_add = batch_forward(Spec, X, W4_add, b, eps)\n",
    "d,d,d,loss_W4_red = batch_forward(Spec, X, W4_red, b, eps)\n",
    "\n",
    "d,d,d,loss_b4_add = batch_forward(Spec, X, W, b4_add, eps)\n",
    "d,d,d,loss_b4_red = batch_forward(Spec, X, W, b4_red, eps)\n",
    "\n",
    "d,d,d,loss_W3_add = batch_forward(Spec, X, W3_add, b, eps)\n",
    "d,d,d,loss_W3_red = batch_forward(Spec, X, W3_red, b, eps)\n",
    "\n",
    "d,d,d,loss_b3_add = batch_forward(Spec, X, W, b3_add, eps)\n",
    "d,d,d,loss_b3_red = batch_forward(Spec, X, W, b3_red, eps)\n",
    "\n",
    "d,d,d,loss_W2_add = batch_forward(Spec, X, W2_add, b, eps)\n",
    "d,d,d,loss_W2_red = batch_forward(Spec, X, W2_red, b, eps)\n",
    "\n",
    "d,d,d,loss_b2_add = batch_forward(Spec, X, W, b2_add, eps)\n",
    "d,d,d,loss_b2_red = batch_forward(Spec, X, W, b2_red, eps)\n",
    "\n",
    "d,d,d,loss_W1_add = batch_forward(Spec, X, W1_add, b, eps)\n",
    "d,d,d,loss_W1_red = batch_forward(Spec, X, W1_red, b, eps)\n",
    "\n",
    "d,d,d,loss_b1_add = batch_forward(Spec, X, W, b1_add, eps)\n",
    "d,d,d,loss_b1_red = batch_forward(Spec, X, W, b1_red, eps)\n",
    "\n",
    "print(\"var\", \"manual\", \"grad func\")\n",
    "print(\"W5: \", (loss_W5_add - loss_W5_red)/2/delta, np.sum(dL_dW5))\n",
    "print(\"b5: \", (loss_b5_add - loss_b5_red)/2/delta, np.sum(dL_db5))\n",
    "print(\"W4: \", (loss_W4_add - loss_W4_red)/2/delta, np.sum(dL_dW4))\n",
    "print(\"b4: \", (loss_b4_add - loss_b4_red)/2/delta, np.sum(dL_db4))\n",
    "print(\"W3: \", (loss_W3_add - loss_W3_red)/2/delta, np.sum(dL_dW3))\n",
    "print(\"b3: \", (loss_b3_add - loss_b3_red)/2/delta, np.sum(dL_db3))\n",
    "print(\"W2: \", (loss_W2_add - loss_W2_red)/2/delta, np.sum(dL_dW2))\n",
    "print(\"b2: \", (loss_b2_add - loss_b2_red)/2/delta, np.sum(dL_db2))\n",
    "print(\"W1: \", (loss_W1_add - loss_W1_red)/2/delta, np.sum(dL_dW1))\n",
    "print(\"b1: \", (loss_b1_add - loss_b1_red)/2/delta, np.sum(dL_db1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Functions checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 batch_forward_vec_nb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "nBatch = 1 # number of mini-batch to train\n",
    "M = 100 # batch size\n",
    "L = 2 # sample size\n",
    "std_const = 255 # to standardize data\n",
    "\n",
    "Xdim1, Xdim2 = trainX[0].shape[0], trainX[0].shape[1]\n",
    "dx = Xdim1 * Xdim2 # dimension of the input\n",
    "dm = 500 # dimension of the hidden layer\n",
    "dz = 3 # dimension of latent variable\n",
    "\n",
    "alpha = 0.005 # learning rate\n",
    "\n",
    "nP = 1000 # print out status every nP batches\n",
    "\n",
    "Spec = [nBatch, M, L, std_const, dx, dm, dz, alpha, nP]\n",
    "\n",
    "W,b = init_random(dx, dm, dz)\n",
    "q_W1, q_W2, q_W3, p_W4, p_W5 = W\n",
    "q_b1, q_b2, q_b3, p_b4, p_b5 = b\n",
    "eps = np.random.randn(L, dz)\n",
    "batchX, batchy = get_Batch(M, trainX, trainy)\n",
    "X = batchX.reshape(M, dx) / std_const"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_original, H_original, Lt_original, loss_original = batch_forward(Spec, X, W, b, eps)\n",
    "q_h1_original, p_h2_original = H_original\n",
    "q_mu_original, q_s2_original, z_original, eps_original = Lt_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_new, q_h1_new, p_h2_new, q_mu_new, q_s2_new, z_new, loss_new = batch_forward_vec_nb2(Spec, X, W, b, eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(np.allclose(y_original, y_new))\n",
    "print(np.allclose(q_h1_original, q_h1_new))\n",
    "print(np.allclose(p_h2_original, p_h2_new))\n",
    "print(np.allclose(q_mu_original, q_mu_new))\n",
    "print(np.allclose(q_s2_original, q_s2_new))\n",
    "print(np.allclose(z_original, z_new))\n",
    "print(np.allclose(loss_original, loss_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 grad_vec2_nb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, H, Lt, loss = batch_forward(Spec, X, W, b, eps)\n",
    "dW_ori, db_ori = grad(X, y, W, b, H, Lt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "nBatch, M, L, std_const, dx, dm, dz, alpha, nP = Spec\n",
    "q_W1, q_W2, q_W3, p_W4, p_W5 = W\n",
    "q_b1, q_b2, q_b3, p_b4, p_b5 = b\n",
    "q_h1, p_h2 = H\n",
    "q_mu, q_s2, z, eps = Lt\n",
    "dJ_dW1, dJ_dW2, dJ_dW3, dJ_dW4, dJ_dW5, dJ_db1, dJ_db2, dJ_db3, dJ_db4, dJ_db5 = \\\n",
    "grad_vec2_nb2(X, y, q_W1, q_W2, q_W3, p_W4, p_W5, q_b1, q_b2, q_b3, p_b4, p_b5, q_h1, p_h2, q_mu, q_s2, z, eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(np.allclose(dW_ori[0], dJ_dW1))\n",
    "print(np.allclose(dW_ori[1], dJ_dW2))\n",
    "print(np.allclose(dW_ori[2], dJ_dW3))\n",
    "print(np.allclose(dW_ori[3], dJ_dW4))\n",
    "print(np.allclose(dW_ori[4], dJ_dW5))\n",
    "\n",
    "print(np.allclose(db_ori[0], dJ_db1))\n",
    "print(np.allclose(db_ori[1], dJ_db2))\n",
    "print(np.allclose(db_ori[2], dJ_db3))\n",
    "print(np.allclose(db_ori[3], dJ_db4))\n",
    "print(np.allclose(db_ori[4], dJ_db5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "caltech = loadmat('caltech101_silhouettes_16.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['__header__', '__version__', '__globals__', 'X', 'Y', 'classnames'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caltech.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "calX = caltech['X']\n",
    "caly = caltech['Y'].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "W,b,loss = train_AEVB(calX, caly, 20000, dm=128, std_const = 1, W = W, b=b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_samples(calX, caly, W, b, std_const = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_para(filename, **kwargs):\n",
    "    \"\"\"save weights and bias parameters\"\"\"\n",
    "    file = open(filename, 'wb')\n",
    "    pickle.dump(kwargs, file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = \"2020_04_30_22_41_05_Model_v1.3_parameter_Caltech\"\n",
    "doc = \"model run for Caltech data, 80000 iterations\"\n",
    "save_para(filename, doc = doc, W = W, b = b, loss = loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
