{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Version Log\n",
    "\n",
    "This version: v1.2\n",
    "\n",
    "Log: \n",
    "\n",
    "1. 2020/04/22: first baseline version created.\n",
    "    - alpha: 0.01 does not work, can train with 0.001\n",
    "2. 2020/04/24: \n",
    "    - ADAM\n",
    "        - implemented ADAM\n",
    "        - added functionality to output file status during training\n",
    "        - added flexibility to start training based on user-specified parameters\n",
    "    - profiling\n",
    "3. 2020/04/25:\n",
    "    - speed up\n",
    "        - updated training function to take in grad as function for easier testing\n",
    "4. 2020/04/28:\n",
    "    - \"total_loss\"\n",
    "        - corrected error in \"total_loss\" function though it didn't have any effect before as L has been set to 1\n",
    "5. 2020/04/29:\n",
    "    - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_version = \"v1.2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work in progress\n",
    "\n",
    "Aim: from profiling results can see, the main bottleneck is the function \"grad\". Now will try different methods to optimize it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensions\n",
    "\n",
    "Data\n",
    "- X: M by dx (100 by 784)\n",
    "- y: M by L by dx (100 by 1 by 784)\n",
    "\n",
    "for calculating total loss by batch\n",
    "- q_mu: M by dz (100 by dz)\n",
    "- q_s2: M by dz (100 by dz)\n",
    "- q_a2: M by dz (100 by dz)\n",
    "\n",
    "for calculating gradients by batch\n",
    "- q_h1: M by dm (100 by dm)\n",
    "- p_h2: M by L by dm (100 by 1 by dm)\n",
    "- z: M by L by dz (100 by 1 by dz)\n",
    "- eps: L by dz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: X=(60000, 28, 28), y=(60000,)\n",
      "Test: X=(10000, 28, 28), y=(10000,)\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "(trainX, trainy), (testX, testy) = mnist.load_data()\n",
    "# summarize loaded dataset\n",
    "print('Train: X=%s, y=%s' % (trainX.shape, trainy.shape))\n",
    "print('Test: X=%s, y=%s' % (testX.shape, testy.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot first few images\n",
    "for i in range(9):\n",
    "    # define subplot\n",
    "    plt.subplot(330 + 1 + i)\n",
    "    # plot raw pixel data\n",
    "    plt.imshow(trainX[i], cmap=plt.get_cmap('gray'))\n",
    "# show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load packages\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pytz\n",
    "import pickle\n",
    "import pstats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline python codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Batch(M, trainX, trainy):\n",
    "    \"\"\"randomly sample a mini batch of size M from the training data\"\"\"\n",
    "    \n",
    "    N = trainX.shape[0]\n",
    "    sample = np.random.choice(N,M)\n",
    "    \n",
    "    return trainX[sample], trainy[sample]\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"sigmoid function\"\"\"\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_gradient(x):\n",
    "    \"\"\"gradient of sigmoid function\"\"\"\n",
    "    return x * (1-x)\n",
    "\n",
    "def tanh(x):\n",
    "    \"\"\"tanh function\"\"\"\n",
    "    return np.tanh(x)\n",
    "    \n",
    "def tanh_gradient(x):\n",
    "    \"\"\"gradient of tanh function\"\"\"\n",
    "    return 1-np.power(x,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_random(dx, dm, dz, option = \"xavier\"):\n",
    "    \"\"\"\n",
    "    parameter initialization\n",
    "    xavier initialization for weights\n",
    "    all zero for bias\n",
    "    can be used to initialize all zero variables for ADAM by setting \"option = zeros\"\n",
    "    \"\"\"\n",
    "    \n",
    "    # weights initialization\n",
    "    if option == \"zeros\":\n",
    "        # only for variables in ADAM algorithm, not to be used for true model parameters\n",
    "        q_W1 = np.zeros((dm, dx))\n",
    "        p_W5 = np.zeros((dx, dm))\n",
    "        q_W2 = np.zeros((dz, dm))\n",
    "        q_W3 = np.zeros((dz, dm))\n",
    "        p_W4 = np.zeros((dm, dz))\n",
    "    elif option == \"xavier\":\n",
    "        bound = np.sqrt(6)/ np.sqrt(dx + dm)\n",
    "        q_W1 = np.random.uniform(-bound, bound, (dm, dx))\n",
    "        p_W5 = np.random.uniform(-bound, bound, (dx, dm))\n",
    "        bound = np.sqrt(6)/ np.sqrt(dm + dz)\n",
    "        q_W2 = np.random.uniform(-bound, bound, (dz, dm))\n",
    "        q_W3 = np.random.uniform(-bound, bound, (dz, dm))\n",
    "        p_W4 = np.random.uniform(-bound, bound, (dm, dz))\n",
    "    \n",
    "    # bias initialization\n",
    "    q_b1 = np.zeros((dm, 1))\n",
    "    p_b5 = np.zeros((dx, 1))\n",
    "    q_b2 = np.zeros((dz, 1))\n",
    "    q_b3 = np.zeros((dz, 1))\n",
    "    p_b4 = np.zeros((dm, 1))\n",
    "    \n",
    "    W = [q_W1, q_W2, q_W3, p_W4, p_W5]\n",
    "    b = [q_b1, q_b2, q_b3, p_b4, p_b5]\n",
    "    \n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_loss(X, y, q_a2, q_mu, q_s2):\n",
    "    \"\"\"target total loss function - to minimize\"\"\"\n",
    "    \n",
    "    M = X.shape[0]\n",
    "    L = y.shape[1]\n",
    "    \n",
    "    loss = 0\n",
    "    for iD in range(M):\n",
    "        for iL in range(L):\n",
    "            # reconstruction loss for each sample of latent variable\n",
    "            loss = loss - np.sum(X[iD,] * np.log(y[iD,iL,]) + (1-X[iD,]) * np.log(1-y[iD,iL,])) / L\n",
    "        \n",
    "        # KL divergence/ regularization\n",
    "        loss = loss + np.sum(np.power(q_mu[iD,],2) + q_s2[iD,] - q_a2[iD,] - 1)/2\n",
    "    \n",
    "    return loss / M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_forward(X, W, b):\n",
    "    \"\"\"\n",
    "    encoder forward propagation for each data point\n",
    "    X: dx by 1\n",
    "    \"\"\"\n",
    "    \n",
    "    q_W1, q_W2, q_W3, d,d = W\n",
    "    q_b1, q_b2, q_b3, d,d = b\n",
    "    \n",
    "    q_a1 = q_W1 @ X + q_b1\n",
    "    q_h1 = tanh(q_a1)\n",
    "    q_mu = q_W2 @ q_h1 + q_b2\n",
    "    q_a2 = q_W3 @ q_h1 + q_b3\n",
    "    q_s2 = np.exp(q_a2)\n",
    "    \n",
    "    return q_h1.T, q_a2.T, q_mu.T, q_s2.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_z(q_mu, q_s2, eps):\n",
    "    \"\"\"sample z\"\"\"\n",
    "    return q_mu + np.sqrt(q_s2) * eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_forward(W, b, z):\n",
    "    \"\"\"\n",
    "    decoder forward propagation for each data point and each sample latent variable\n",
    "    z: dz by 1\n",
    "    \"\"\"\n",
    "    \n",
    "    d,d,d, p_W4, p_W5 = W\n",
    "    d,d,d, p_b4, p_b5 = b\n",
    "    \n",
    "    p_a3 = p_W4 @ z + p_b4\n",
    "    p_h2 = tanh(p_a3)\n",
    "            \n",
    "    p_a4 = p_W5 @ p_h2 + p_b5\n",
    "    y = sigmoid(p_a4)\n",
    "    \n",
    "    return y.T, p_h2.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(X, y, W, b, H, Lt):\n",
    "    \"\"\"\"\n",
    "    batch gradient\n",
    "    inputs:\n",
    "        X: M by dx\n",
    "        y: M by L by dx\n",
    "        W: weights\n",
    "        b: bias\n",
    "        H: q_h1 (M by dm), p_h2 (M by L by dm)\n",
    "        Lt: eps (L by dz), z (M by L by dz), q_s2 (M by dz), q_mu (M by dz)\n",
    "    \"\"\"\n",
    "    q_W1, q_W2, q_W3, p_W4, p_W5 = W\n",
    "    q_b1, q_b2, q_b3, p_b4, p_b5 = b\n",
    "    q_h1, p_h2 = H\n",
    "    q_mu, q_s2, z, eps = Lt\n",
    "    \n",
    "    M = X.shape[0]\n",
    "    L = y.shape[1]\n",
    "    \n",
    "    # initialize gradient variables\n",
    "    \n",
    "    # L: loss; R: regularization; J: total target\n",
    "    dL_dW1 = dJ_dW1 = dR_dW1 = np.zeros_like(q_W1)\n",
    "    dL_db1 = dJ_db1 = dR_db1 = np.zeros_like(q_b1)\n",
    "    dL_dW2 = dJ_dW2 = dR_dW2 = np.zeros_like(q_W2)\n",
    "    dL_db2 = dJ_db2 = dR_db2 = np.zeros_like(q_b2)\n",
    "    dL_dW3 = dJ_dW3 = dR_dW3 = np.zeros_like(q_W3)\n",
    "    dL_db3 = dJ_db3 = dR_db3 = np.zeros_like(q_b3)\n",
    "    dL_dW4 = dJ_dW4 = np.zeros_like(p_W4)\n",
    "    dL_db4 = dJ_db4 = np.zeros_like(p_b4)    \n",
    "    dL_dW5 = dJ_dW5 = np.zeros_like(p_W5)\n",
    "    dL_db5 = dJ_db5 = np.zeros_like(p_b5)\n",
    "    \n",
    "    for iD in range(M):\n",
    "        for iL in range(L):\n",
    "            # back propagation for loss\n",
    "\n",
    "            #L_d4 = (np.divide(y[iD,iL,]-X[iD,], y[iD,iL,] * (1-y[iD,iL,])) * sigmoid_gradient(y[iD,iL,])).reshape(-1,1)\n",
    "            \n",
    "            # simplied L_d4\n",
    "            L_d4 = (y[iD,iL,]-X[iD,]).reshape(-1,1)\n",
    "            dL_dW5 = dL_dW5 + L_d4 @ p_h2[iD,iL,].reshape(1,-1)\n",
    "            dL_db5 = dL_db5 + L_d4\n",
    "            \n",
    "            L_d3 = p_W5.T @ L_d4 * tanh_gradient(p_h2[iD,iL,]).reshape(-1,1)\n",
    "            dL_dW4 = dL_dW4 + L_d3 @ z[iD,iL,].reshape(1,-1)\n",
    "            dL_db4 = dL_db4 + L_d3\n",
    "            \n",
    "            L_d22 = p_W4.T @ L_d3 * eps[iL,].reshape(-1,1) * np.sqrt(q_s2[iD,]).reshape(-1,1) / 2\n",
    "            dL_dW3 = dL_dW3 + L_d22 @ q_h1[iD,].reshape(1,-1)\n",
    "            dL_db3 = dL_db3 + L_d22\n",
    "            \n",
    "            L_d21 = p_W4.T @ L_d3\n",
    "            dL_dW2 = dL_dW2 + L_d21 @ q_h1[iD,].reshape(1,-1)\n",
    "            dL_db2 = dL_db2 + L_d21\n",
    "            \n",
    "            L_d1 = (q_W2.T @ L_d21 + q_W3.T @ L_d22) * tanh_gradient(q_h1[iD,]).reshape(-1,1)\n",
    "            dL_dW1 = dL_dW1 + L_d1 @ X[iD,].reshape(1,-1)\n",
    "            dL_db1 = dL_db1 + L_d1\n",
    "        \n",
    "        # back propagation for regularization\n",
    "        R_d22 = ((q_s2[iD,]-1)/2).reshape(-1,1)\n",
    "        dR_dW3 = dR_dW3 + R_d22 @ q_h1[iD,].reshape(1,-1)\n",
    "        dR_db3 = dR_db3 + R_d22\n",
    "\n",
    "        R_d21 = q_mu[iD,].reshape(-1,1)\n",
    "        dR_dW2 = dR_dW2 + R_d21 @ q_h1[iD,].reshape(1,-1)\n",
    "        dR_db2 = dR_db2 + R_d21\n",
    "\n",
    "        R_d1 = (q_W3.T @ R_d22 + q_W2.T @ R_d21) * tanh_gradient(q_h1[iD,]).reshape(-1,1)\n",
    "        dR_dW1 = dR_dW1 + R_d1 @ X[iD,].reshape(1,-1)\n",
    "        dR_db1 = dR_db1 + R_d1\n",
    "    \n",
    "    dJ_dW1 = dL_dW1 / L / M + dR_dW1 / M\n",
    "    dJ_db1 = dL_db1 / L / M + dR_db1 / M\n",
    "    dJ_dW2 = dL_dW2 / L / M + dR_dW2 / M\n",
    "    dJ_db2 = dL_db2 / L / M + dR_db2 / M    \n",
    "    dJ_dW3 = dL_dW3 / L / M + dR_dW3 / M\n",
    "    dJ_db3 = dL_db3 / L / M + dR_db3 / M\n",
    "    dJ_dW4 = dL_dW4 / L / M\n",
    "    dJ_db4 = dL_db4 / L / M \n",
    "    dJ_dW5 = dL_dW5 / L / M\n",
    "    dJ_db5 = dL_db5 / L / M\n",
    "    \n",
    "    dW = [dJ_dW1, dJ_dW2, dJ_dW3, dJ_dW4, dJ_dW5]\n",
    "    db = [dJ_db1, dJ_db2, dJ_db3, dJ_db4, dJ_db5]\n",
    "    \n",
    "    return dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_forward(Spec, X, W, b, eps):\n",
    "    \"\"\"forward propagation for one batch\"\"\"\n",
    "    d, M, L, d, d, dm, dz, d, d = Spec\n",
    "    \n",
    "    # initialize variables for calculating gradients/ loss by batch\n",
    "    q_h1 = np.zeros((M, dm))\n",
    "    q_mu = np.zeros((M, dz))\n",
    "    q_s2 = np.zeros((M, dz))\n",
    "    q_a2 = np.zeros((M, dz))\n",
    "    p_h2 = np.zeros((M, L, dm))\n",
    "    z = np.zeros((M, L, dz))\n",
    "    y = np.zeros((M, L, dx))\n",
    "    \n",
    "    for iD in range(M):\n",
    "        q_h1[iD,], q_a2[iD,], q_mu[iD,], q_s2[iD,] = encoder_forward(X[iD,].reshape(-1,1), W, b)\n",
    "\n",
    "        for iL in range(L):\n",
    "            z[iD,iL,] = sample_z(q_mu[iD,], q_s2[iD,], eps[iL,])\n",
    "            y[iD,iL,], p_h2[iD,iL,] = decoder_forward(W, b, z[iD,iL,].reshape(-1,1))\n",
    "    \n",
    "    H = [q_h1, p_h2]\n",
    "    Lt = [q_mu, q_s2, z, eps]\n",
    "    loss = total_loss(X, y, q_a2, q_mu, q_s2)\n",
    "    return y, H, Lt, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def para_update(W, b, dW, db, alpha):\n",
    "    \"\"\"update weights and bias for gradient descent\"\"\"\n",
    "    \n",
    "    assert len(W) == len(b) == len(dW) == len(db)\n",
    "    n = len(W)\n",
    "    \n",
    "    for i in range(n):\n",
    "        W[i] = W[i] - alpha * dW[i]\n",
    "        b[i] = b[i] - alpha * db[i]\n",
    "    \n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_samples(Spec, n, W, b):\n",
    "    \"\"\"plot n number of random sampled figures as well as the model-reconstructed versions\"\"\"\n",
    "    \n",
    "    d, d, L, std_const, dx, dm, dz, d, d = Spec\n",
    "    Spec[1] = n\n",
    "    \n",
    "    batchX, batchy = get_Batch(n, trainX, trainy)\n",
    "    Xdim1, Xdim2 = batchX[0].shape[0], batchX[0].shape[1]\n",
    "    assert dx == Xdim1 * Xdim2\n",
    "    X = batchX.reshape(n, dx) / std_const\n",
    "    eps = np.zeros((L, dz))\n",
    "    \n",
    "    y, d,d,d = batch_forward(Spec, X, W, b, eps)\n",
    "    \n",
    "    for i in range(n):\n",
    "        # define subplot\n",
    "        plt.subplot(330 + 1 + i)\n",
    "        # plot raw pixel data\n",
    "        plt.imshow(batchX[i], cmap=plt.get_cmap('gray'))\n",
    "    # show the figure\n",
    "    plt.show()\n",
    "    \n",
    "    for i in range(n):\n",
    "        # define subplot\n",
    "        plt.subplot(330 + 1 + i)\n",
    "        # plot raw pixel data\n",
    "        plt.imshow(y[i,L-1,].reshape(Xdim1, Xdim2) * std_const, cmap=plt.get_cmap('gray'))\n",
    "    # show the figure\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_para(filename, **kwargs):\n",
    "    \"\"\"save weights and bias parameters\"\"\"\n",
    "    file = open(filename, 'wb')\n",
    "    pickle.dump(kwargs, file)\n",
    "    file.close()\n",
    "\n",
    "def get_para(filename):\n",
    "    \"\"\"read parameters from file\"\"\"\n",
    "    file = open(filename, 'rb')\n",
    "    dict = pickle.load(file)\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(trainX, trainy, Spec, grad, doc = \"\"):\n",
    "    \"\"\"train model with vanila gradient descent\"\"\"\n",
    "    nBatch, M, L, std_const, dx, dm, dz, alpha, nP = Spec\n",
    "    status_file = get_filename(\"status\")\n",
    "    para_file = get_filename(\"parameter\")\n",
    "    \n",
    "    # weights and bias initialization\n",
    "    W, b = init_random(dx, dm, dz)\n",
    "\n",
    "    # loss\n",
    "    loss = np.zeros(nBatch)\n",
    "    \n",
    "    # create status file\n",
    "    update_status(status_file, \"x\", \"Training starts: \" + get_timestamp() + \"\\n\")\n",
    "    \n",
    "    for iB in range(nBatch):\n",
    "        # sample a random batch\n",
    "        batchX, batchy = get_Batch(M, trainX, trainy)\n",
    "        X = batchX.reshape(M, dx) / std_const\n",
    "\n",
    "        # sample random noise for latent variable, assuming each batch uses the same random noise for now\n",
    "        eps = np.random.randn(L, dz)\n",
    "\n",
    "        y, H, Lt, loss[iB] = batch_forward(Spec, X, W, b, eps)\n",
    "\n",
    "        dW, db = grad(X, y, W, b, H, Lt)\n",
    "        W, b = para_update(W, b, dW, db, alpha)\n",
    "        \n",
    "        if (iB+1) % nP == 0:\n",
    "            # append status file\n",
    "            update_status(status_file, \"a\", \"Batch \" + str(iB+1) + \" : \" + get_timestamp() + \"\\n\")\n",
    "    \n",
    "    # save parameter to file\n",
    "    save_para(para_file, doc = doc, Spec = Spec, W = W, b = b, loss = loss)\n",
    "    \n",
    "    return Spec, W, b, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ADAM(trainX, trainy, Spec, batch_forward, grad, W = \"\", b = \"\", doc = \"\"):\n",
    "    \"\"\"train model with ADAM\"\"\"\n",
    "    \n",
    "    nBatch, M, L, std_const, dx, dm, dz, alpha, nP = Spec\n",
    "    \n",
    "    if doc != \"\":\n",
    "        status_file = get_filename(\"status\")\n",
    "        para_file = get_filename(\"parameter\")\n",
    "        update_status(status_file, \"x\", \"Training starts: \" + get_timestamp() + \"\\n\")\n",
    "    \n",
    "    # parameters for ADAM algorithm\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    eps_stable = 1e-8\n",
    "    \n",
    "    # initiate parameters for ADAM\n",
    "    # need to use separate lines of codes otherwise they share the same reference\n",
    "    v_dW, v_db = init_random(dx, dm, dz, option = \"zeros\")\n",
    "    s_dW, s_db = init_random(dx, dm, dz, option = \"zeros\")\n",
    "    vc_dW, vc_db = init_random(dx, dm, dz, option = \"zeros\")\n",
    "    sc_dW, sc_db = init_random(dx, dm, dz, option = \"zeros\")\n",
    "    num_para = len(v_dW)\n",
    "    \n",
    "    # weights and bias initialization\n",
    "    if len(W) == len(b) == 0:\n",
    "        W, b = init_random(dx, dm, dz)\n",
    "\n",
    "    # loss\n",
    "    loss = np.zeros(nBatch)\n",
    "    \n",
    "    for iB in range(nBatch):\n",
    "        # sample a random batch\n",
    "        batchX, batchy = get_Batch(M, trainX, trainy)\n",
    "        X = batchX.reshape(M, dx) / std_const\n",
    "\n",
    "        # sample random noise for latent variable, assuming each batch uses the same random noise for now\n",
    "        eps = np.random.randn(L, dz)\n",
    "\n",
    "        y, H, Lt, loss[iB] = batch_forward(Spec, X, W, b, eps)\n",
    "        dW, db = grad(X, y, W, b, H, Lt)\n",
    "        \n",
    "        # ADAM\n",
    "        for i in range(num_para):\n",
    "            v_dW[i] = beta1*v_dW[i] + (1-beta1)*dW[i]\n",
    "            v_db[i] = beta1*v_db[i] + (1-beta1)*db[i]\n",
    "            s_dW[i] = beta2*s_dW[i] + (1-beta2)*np.power(dW[i],2)\n",
    "            s_db[i] = beta2*s_db[i] + (1-beta2)*np.power(db[i],2)\n",
    "        \n",
    "            vc_dW[i] = v_dW[i]/(1-beta1**(iB+1))\n",
    "            vc_db[i] = v_db[i]/(1-beta1**(iB+1))\n",
    "            sc_dW[i] = s_dW[i]/(1-beta2**(iB+1))\n",
    "            sc_db[i] = s_db[i]/(1-beta2**(iB+1))\n",
    "        \n",
    "            dW[i] = vc_dW[i] / (np.sqrt(sc_dW[i]) + eps_stable)\n",
    "            db[i] = vc_db[i] / (np.sqrt(sc_db[i]) + eps_stable)\n",
    "        \n",
    "        W, b = para_update(W, b, dW, db, alpha)\n",
    "        \n",
    "        if doc != \"\":\n",
    "            if (iB+1) % nP == 0:\n",
    "                # append status file\n",
    "                update_status(status_file, \"a\", \"Batch \" + str(iB+1) + \" : \" + get_timestamp() + \"\\n\")\n",
    "    \n",
    "    # save parameter to file\n",
    "    if doc != \"\":\n",
    "        save_para(para_file, doc = doc, Spec = Spec, W = W, b = b, loss = loss)\n",
    "\n",
    "    return Spec, W, b, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timestamp(timezone = \"America/New_York\", format = \"%Y/%m/%d %H:%M:%S\"):\n",
    "    \"\"\"formatted date time in local timezone\"\"\"\n",
    "    now = datetime.datetime.now().astimezone(pytz.timezone(\"America/New_York\"))\n",
    "    return now.strftime(format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filename(suffix=\"\", format = \"%Y_%m_%d_%H_%M_%S\", ext = \".txt\"):\n",
    "    \"\"\"create file name\"\"\"\n",
    "    if suffix != \"\":\n",
    "        suffix = \"_\" + suffix\n",
    "    res = get_timestamp(format = format) + \"_Model_\" + Model_version + suffix + ext\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_status(filename, mode, msg):\n",
    "    \"\"\"for training status update\"\"\"\n",
    "    with open(filename, mode) as f:\n",
    "        f.write(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Codes Main Body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters\n",
    "nBatch = 60000 # number of mini-batch to train\n",
    "M = 100 # batch size\n",
    "L = 1 # sample size\n",
    "std_const = 255 # to standardize data\n",
    "\n",
    "Xdim1, Xdim2 = trainX[0].shape[0], trainX[0].shape[1]\n",
    "dx = Xdim1 * Xdim2 # dimension of the input\n",
    "dm = 500 # dimension of the hidden layer\n",
    "dz = 3 # dimension of latent variable\n",
    "\n",
    "alpha = 0.005 # learning rate\n",
    "\n",
    "nP = 1000 # print out status every nP batches\n",
    "\n",
    "Spec = [nBatch, M, L, std_const, dx, dm, dz, alpha, nP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Spec, W, b, loss = train_ADAM(trainX, trainy, Spec, grad, doc = \"testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continue training using existing parameters\n",
    "Spec, W, b, loss = train_ADAM(trainX, trainy, Spec, grad, W = W, b = b, doc = \"continue run from 2020_04_24_15_25_12 parameter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Spec, W, b, loss = train(trainX, trainy, Spec, grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save model parameters after model run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save parameters\n",
    "doc = \"\"\"Second run using ADAM.\"\"\"\n",
    "\n",
    "filename = get_filename(\"parameter\")\n",
    "save_para(filename, doc = doc, Spec = Spec, W = W, b = b, loss = loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read back parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"2020_04_24_15_25_12_Model_v1.1_parameter.txt\"\n",
    "para = get_para(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "para['doc']\n",
    "W = para['W']\n",
    "b = para['b']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To plot a few samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_samples(Spec, 9, W, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pstats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create profiling file name, change batch size to 100 and proceed with profiling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_filename = get_filename(\"profiling\", ext = \".prof\")\n",
    "Spec = [100, M, L, std_const, dx, dm, dz, alpha, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "*** Profile stats marshalled to file '2020_04_25_15_58_54_Model_v1.2_profiling.prof'. \n"
     ]
    }
   ],
   "source": [
    "%prun -q -D $profile_filename train_ADAM(trainX, trainy, Spec, grad_nb, doc = \"Profiling grad_nb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print all the statistics (random order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Apr 25 19:59:39 2020    2020_04_25_15_58_54_Model_v1.2_profiling.prof\n",
      "\n",
      "         276418 function calls (274318 primitive calls) in 43.572 seconds\n",
      "\n",
      "   Random listing order was used\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "       11    0.000    0.000    0.000    0.000 {method 'write' of '_io.TextIOWrapper' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'close' of '_io.BufferedWriter' objects}\n",
      "       12    0.001    0.000    0.001    0.000 {built-in method io.open}\n",
      "       11    0.000    0.000    0.000    0.000 {built-in method _locale.nl_langinfo}\n",
      "        1    0.000    0.000   43.572   43.572 {built-in method builtins.exec}\n",
      "      213    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}\n",
      "    20000    0.013    0.000    0.013    0.000 {built-in method builtins.isinstance}\n",
      "      400    0.001    0.000    0.001    0.000 {built-in method builtins.issubclass}\n",
      "      503    0.000    0.000    0.000    0.000 {built-in method builtins.len}\n",
      "       13    0.000    0.000    0.000    0.000 {built-in method builtins.max}\n",
      "       13    0.000    0.000    0.000    0.000 {method 'encode' of 'str' objects}\n",
      "       26    0.000    0.000    0.000    0.000 {method 'replace' of 'str' objects}\n",
      "       13    0.000    0.000    0.000    0.000 {method 'lower' of 'str' objects}\n",
      "       13    0.000    0.000    0.000    0.000 {method 'upper' of 'str' objects}\n",
      "      100    0.000    0.000    0.000    0.000 {method 'format' of 'str' objects}\n",
      "       13    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}\n",
      "    20200    0.012    0.000    0.012    0.000 {method 'items' of 'dict' objects}\n",
      "        1    2.694    2.694   43.570   43.570 <ipython-input-35-ca2d0ec31713>:1(train_ADAM)\n",
      "        1    0.002    0.002   43.572   43.572 <string>:1(<module>)\n",
      "  200/100   36.859    0.184   36.964    0.370 <ipython-input-31-c1ccc69c24a4>:1(grad_nb)\n",
      "    10000    0.710    0.000    1.188    0.000 <ipython-input-12-317b689125af>:1(decoder_forward)\n",
      "        2    0.000    0.000    0.000    0.000 <ipython-input-21-f0e52850f9cc>:1(get_filename)\n",
      "        1    0.000    0.000    0.012    0.012 <ipython-input-17-72aee39180d4>:1(save_para)\n",
      "      100    0.217    0.002    0.218    0.002 <ipython-input-15-e51089a23302>:1(para_update)\n",
      "    20000    0.373    0.000    0.373    0.000 <ipython-input-7-bb86cfd4142c>:17(tanh)\n",
      "      100    0.007    0.000    0.036    0.000 <ipython-input-7-bb86cfd4142c>:1(get_Batch)\n",
      "    10000    0.313    0.000    0.313    0.000 <ipython-input-7-bb86cfd4142c>:9(sigmoid)\n",
      "    10000    0.038    0.000    0.038    0.000 <ipython-input-11-eefdc651e095>:1(sample_z)\n",
      "      100    0.918    0.009    1.267    0.013 <ipython-input-9-fd1817a619e0>:1(total_loss)\n",
      "    10000    0.700    0.000    0.907    0.000 <ipython-input-10-0aeb9911c8e9>:1(encoder_forward)\n",
      "      100    0.177    0.002    3.627    0.036 <ipython-input-14-f0eb432776af>:1(batch_forward)\n",
      "       13    0.000    0.000    0.000    0.000 /opt/conda/lib/python3.6/site-packages/pytz/__init__.py:47(ascii)\n",
      "       13    0.000    0.000    0.000    0.000 /opt/conda/lib/python3.6/site-packages/pytz/__init__.py:123(timezone)\n",
      "       13    0.000    0.000    0.000    0.000 /opt/conda/lib/python3.6/site-packages/pytz/__init__.py:186(_unmunge_zone)\n",
      "       13    0.000    0.000    0.000    0.000 /opt/conda/lib/python3.6/site-packages/pytz/__init__.py:194(_case_insensitive_zone_lookup)\n",
      "       13    0.000    0.000    0.000    0.000 /opt/conda/lib/python3.6/site-packages/pytz/tzinfo.py:193(fromutc)\n",
      "       13    0.000    0.000    0.000    0.000 /opt/conda/lib/python3.6/site-packages/pytz/tzinfo.py:427(dst)\n",
      "      100    0.013    0.000    0.028    0.000 {method 'choice' of 'numpy.random.mtrand.RandomState' objects}\n",
      "        5    0.012    0.002    0.012    0.002 {method 'uniform' of 'numpy.random.mtrand.RandomState' objects}\n",
      "      100    0.002    0.000    0.002    0.000 {method 'randn' of 'numpy.random.mtrand.RandomState' objects}\n",
      "    20000    0.056    0.000    0.262    0.000 /opt/conda/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2092(sum)\n",
      "      200    0.000    0.000    0.000    0.000 /opt/conda/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2838(_prod_dispatcher)\n",
      "      200    0.001    0.000    0.008    0.000 /opt/conda/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2843(prod)\n",
      "    20000    0.044    0.000    0.350    0.000 <__array_function__ internals>:2(sum)\n",
      "      200    0.001    0.000    0.009    0.000 <__array_function__ internals>:2(prod)\n",
      "     1000    0.002    0.000    0.105    0.000 <__array_function__ internals>:2(zeros_like)\n",
      "     1000    0.002    0.000    0.087    0.000 <__array_function__ internals>:2(copyto)\n",
      "      200    0.001    0.000    0.001    0.000 /opt/conda/lib/python3.6/site-packages/numpy/core/numerictypes.py:293(issubclass_)\n",
      "      100    0.002    0.000    0.003    0.000 /opt/conda/lib/python3.6/site-packages/numpy/core/numerictypes.py:365(issubdtype)\n",
      "     1000    0.001    0.000    0.001    0.000 /opt/conda/lib/python3.6/site-packages/numpy/core/numeric.py:86(_zeros_like_dispatcher)\n",
      "      100    0.000    0.000    0.000    0.000 /opt/conda/lib/python3.6/site-packages/numpy/core/_dtype.py:36(_kind_name)\n",
      "     1000    0.004    0.000    0.100    0.000 /opt/conda/lib/python3.6/site-packages/numpy/core/numeric.py:90(zeros_like)\n",
      "      100    0.001    0.000    0.004    0.000 /opt/conda/lib/python3.6/site-packages/numpy/core/_dtype.py:319(_name_includes_bit_suffix)\n",
      "      100    0.001    0.000    0.006    0.000 /opt/conda/lib/python3.6/site-packages/numpy/core/_dtype.py:333(_name_get)\n",
      "    20200    0.019    0.000    0.019    0.000 /opt/conda/lib/python3.6/site-packages/numpy/core/fromnumeric.py:74(<dictcomp>)\n",
      "    20200    0.069    0.000    0.199    0.000 /opt/conda/lib/python3.6/site-packages/numpy/core/fromnumeric.py:73(_wrapreduction)\n",
      "    20000    0.012    0.000    0.012    0.000 /opt/conda/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2087(_sum_dispatcher)\n",
      "    20100    0.028    0.000    0.028    0.000 {method 'reshape' of 'numpy.ndarray' objects}\n",
      "     1746    0.026    0.000    0.026    0.000 {built-in method numpy.zeros}\n",
      "23200/21200    0.121    0.000    0.403    0.000 {built-in method numpy.core._multiarray_umath.implement_array_function}\n",
      "    20200    0.099    0.000    0.099    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
      "     1000    0.001    0.000    0.001    0.000 /opt/conda/lib/python3.6/site-packages/numpy/core/multiarray.py:77(empty_like)\n",
      "     1000    0.001    0.000    0.001    0.000 /opt/conda/lib/python3.6/site-packages/numpy/core/multiarray.py:1043(copyto)\n",
      "     1000    0.002    0.000    0.006    0.000 <__array_function__ internals>:2(empty_like)\n",
      "        5    0.000    0.000    0.014    0.003 <ipython-input-8-27d8d88b08a7>:1(init_random)\n",
      "       13    0.000    0.000    0.002    0.000 <ipython-input-20-73db301ae7d3>:1(get_timestamp)\n",
      "       11    0.001    0.000    0.002    0.000 <ipython-input-22-e107240f4d46>:1(update_status)\n",
      "        1    0.012    0.012    0.012    0.012 {built-in method _pickle.dump}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "       13    0.000    0.000    0.000    0.000 {built-in method _bisect.bisect_right}\n",
      "       13    0.000    0.000    0.000    0.000 {method 'strftime' of 'datetime.date' objects}\n",
      "       13    0.000    0.000    0.000    0.000 {built-in method now}\n",
      "       26    0.000    0.000    0.000    0.000 {method 'replace' of 'datetime.datetime' objects}\n",
      "       13    0.000    0.000    0.001    0.000 {method 'astimezone' of 'datetime.datetime' objects}\n",
      "       11    0.000    0.000    0.000    0.000 /opt/conda/lib/python3.6/_bootlocale.py:23(getpreferredencoding)\n",
      "       11    0.000    0.000    0.000    0.000 /opt/conda/lib/python3.6/codecs.py:185(__init__)\n",
      "       10    0.000    0.000    0.000    0.000 /opt/conda/lib/python3.6/codecs.py:213(setstate)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p = pstats.Stats(profile_filename)\n",
    "p.print_stats()\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistics sorted by run time. Note that we can specify the specific function to look at by changing inputs in \"print_stats\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Apr 25 19:59:39 2020    2020_04_25_15_58_54_Model_v1.2_profiling.prof\n",
      "\n",
      "         276418 function calls (274318 primitive calls) in 43.572 seconds\n",
      "\n",
      "   Ordered by: internal time, cumulative time\n",
      "   List reduced from 77 to 5 due to restriction <5>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "  200/100   36.859    0.184   36.964    0.370 <ipython-input-31-c1ccc69c24a4>:1(grad_nb)\n",
      "        1    2.694    2.694   43.570   43.570 <ipython-input-35-ca2d0ec31713>:1(train_ADAM)\n",
      "      100    0.918    0.009    1.267    0.013 <ipython-input-9-fd1817a619e0>:1(total_loss)\n",
      "    10000    0.710    0.000    1.188    0.000 <ipython-input-12-317b689125af>:1(decoder_forward)\n",
      "    10000    0.700    0.000    0.907    0.000 <ipython-input-10-0aeb9911c8e9>:1(encoder_forward)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p.sort_stats('time', 'cumulative').print_stats(5)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Apr 25 19:59:39 2020    2020_04_25_15_58_54_Model_v1.2_profiling.prof\n",
      "\n",
      "         276418 function calls (274318 primitive calls) in 43.572 seconds\n",
      "\n",
      "   Ordered by: internal time, cumulative time\n",
      "   List reduced from 77 to 1 due to restriction <'grad'>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "  200/100   36.859    0.184   36.964    0.370 <ipython-input-31-c1ccc69c24a4>:1(grad_nb)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p.sort_stats('time', 'cumulative').print_stats('grad')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistics sorted by number of calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.sort_stats('ncalls').print_stats(5)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following codes is to get the profile object directly (not used yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-r means return the profile object\n",
    "profile = %prun -r -q work(int(1e5))\n",
    "profile.sort_stats('cumtime').print_stats(5)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba\n",
    "from numba import jit, njit, prange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Minimalist version\n",
    "\n",
    "This section is to try to use minimal efforts to speed up. The goal here is to try make minimal changes to the code and use numba.\n",
    "\n",
    "Naming convension:\n",
    "    - functions which I enviziege will only have one version of numba implementation is named with \"_nb\".\n",
    "    - functions which later I'll need to further change for other nb implementation is named with \"_nb1\".\n",
    "\n",
    "From the results it seems that this version is able to speed up the runtime for \"grad\" from ~80 to ~35."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid_nb = jit(sigmoid, nopython=True, cache=True)\n",
    "sigmoid_gradient_nb = jit(sigmoid_gradient, nopython=True, cache=True)\n",
    "tanh_nb = jit(tanh, nopython=True, cache=True)\n",
    "tanh_gradient_nb = jit(tanh_gradient, nopython=True, cache=True)\n",
    "total_loss_nb = jit(total_loss, nopython=True, cache=True)\n",
    "sample_z_nb = jit(sample_z, nopython=True, cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit(cache=True)\n",
    "def encoder_forward_nb1(X, W, b):\n",
    "    \"\"\"\n",
    "    encoder forward propagation for each data point\n",
    "    X: dx by 1\n",
    "    \"\"\"\n",
    "    \n",
    "    q_W1, q_W2, q_W3, d,d = W\n",
    "    q_b1, q_b2, q_b3, d,d = b\n",
    "    \n",
    "    q_a1 = q_W1 @ X + q_b1\n",
    "    q_h1 = tanh_nb(q_a1)\n",
    "    q_mu = q_W2 @ q_h1 + q_b2\n",
    "    q_a2 = q_W3 @ q_h1 + q_b3\n",
    "    q_s2 = np.exp(q_a2)\n",
    "    \n",
    "    return q_h1.T, q_a2.T, q_mu.T, q_s2.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit(cache=True)\n",
    "def decoder_forward_nb1(W, b, z):\n",
    "    \"\"\"\n",
    "    decoder forward propagation for each data point and each sample latent variable\n",
    "    z: dz by 1\n",
    "    \"\"\"\n",
    "    \n",
    "    d,d,d, p_W4, p_W5 = W\n",
    "    d,d,d, p_b4, p_b5 = b\n",
    "    \n",
    "    p_a3 = p_W4 @ z + p_b4\n",
    "    p_h2 = tanh_nb(p_a3)\n",
    "            \n",
    "    p_a4 = p_W5 @ p_h2 + p_b5\n",
    "    y = sigmoid_nb(p_a4)\n",
    "    \n",
    "    return y.T, p_h2.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(cache=True)\n",
    "def batch_forward_nb1(Spec, X, W, b, eps):\n",
    "    \"\"\"forward propagation for one batch\"\"\"\n",
    "    d, M, L, d, d, dm, dz, d, d = Spec\n",
    "    \n",
    "    # initialize variables for calculating gradients/ loss by batch\n",
    "    q_h1 = np.zeros((M, dm))\n",
    "    q_mu = np.zeros((M, dz))\n",
    "    q_s2 = np.zeros((M, dz))\n",
    "    q_a2 = np.zeros((M, dz))\n",
    "    p_h2 = np.zeros((M, L, dm))\n",
    "    z = np.zeros((M, L, dz))\n",
    "    y = np.zeros((M, L, dx))\n",
    "    \n",
    "    for iD in range(M):\n",
    "        q_h1[iD,], q_a2[iD,], q_mu[iD,], q_s2[iD,] = encoder_forward_nb1(X[iD,].reshape(-1,1), W, b)\n",
    "\n",
    "        for iL in range(L):\n",
    "            z[iD,iL,] = sample_z_nb(q_mu[iD,], q_s2[iD,], eps[iL,])\n",
    "            y[iD,iL,], p_h2[iD,iL,] = decoder_forward_nb1(W, b, z[iD,iL,].reshape(-1,1))\n",
    "    \n",
    "    H = [q_h1, p_h2]\n",
    "    Lt = [q_mu, q_s2, z, eps]\n",
    "    loss = total_loss_nb(X, y, q_a2, q_mu, q_s2)\n",
    "    return (y, H, Lt, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(cache=True)\n",
    "def grad_nb1(X, y, W, b, H, Lt):\n",
    "    \"\"\"\"\n",
    "    batch gradient\n",
    "    inputs:\n",
    "        X: M by dx\n",
    "        y: M by L by dx\n",
    "        W: weights\n",
    "        b: bias\n",
    "        H: q_h1 (M by dm), p_h2 (M by L by dm)\n",
    "        Lt: eps (L by dz), z (M by L by dz), q_s2 (M by dz), q_mu (M by dz)\n",
    "    \"\"\"\n",
    "    q_W1, q_W2, q_W3, p_W4, p_W5 = W\n",
    "    q_b1, q_b2, q_b3, p_b4, p_b5 = b\n",
    "    q_h1, p_h2 = H\n",
    "    q_mu, q_s2, z, eps = Lt\n",
    "    \n",
    "    M = X.shape[0]\n",
    "    L = y.shape[1]\n",
    "    \n",
    "    # initialize gradient variables\n",
    "    \n",
    "    # L: loss; R: regularization; J: total target\n",
    "    dL_dW1 = dJ_dW1 = dR_dW1 = np.zeros_like(q_W1)\n",
    "    dL_db1 = dJ_db1 = dR_db1 = np.zeros_like(q_b1)\n",
    "    dL_dW2 = dJ_dW2 = dR_dW2 = np.zeros_like(q_W2)\n",
    "    dL_db2 = dJ_db2 = dR_db2 = np.zeros_like(q_b2)\n",
    "    dL_dW3 = dJ_dW3 = dR_dW3 = np.zeros_like(q_W3)\n",
    "    dL_db3 = dJ_db3 = dR_db3 = np.zeros_like(q_b3)\n",
    "    dL_dW4 = dJ_dW4 = np.zeros_like(p_W4)\n",
    "    dL_db4 = dJ_db4 = np.zeros_like(p_b4)    \n",
    "    dL_dW5 = dJ_dW5 = np.zeros_like(p_W5)\n",
    "    dL_db5 = dJ_db5 = np.zeros_like(p_b5)\n",
    "    \n",
    "    for iD in range(M):\n",
    "        for iL in range(L):\n",
    "            # back propagation for loss\n",
    "\n",
    "            #L_d4 = (np.divide(y[iD,iL,]-X[iD,], y[iD,iL,] * (1-y[iD,iL,])) * sigmoid_gradient(y[iD,iL,])).reshape(-1,1)\n",
    "            \n",
    "            # simplied L_d4\n",
    "            L_d4 = (y[iD,iL,]-X[iD,]).reshape(-1,1)\n",
    "            dL_dW5 = dL_dW5 + L_d4 @ p_h2[iD,iL,].reshape(1,-1)\n",
    "            dL_db5 = dL_db5 + L_d4\n",
    "            \n",
    "            L_d3 = p_W5.T @ L_d4 * tanh_gradient_nb(p_h2[iD,iL,]).reshape(-1,1)\n",
    "            dL_dW4 = dL_dW4 + L_d3 @ z[iD,iL,].reshape(1,-1)\n",
    "            dL_db4 = dL_db4 + L_d3\n",
    "            \n",
    "            L_d22 = p_W4.T @ L_d3 * eps[iL,].reshape(-1,1) * np.sqrt(q_s2[iD,]).reshape(-1,1) / 2\n",
    "            dL_dW3 = dL_dW3 + L_d22 @ q_h1[iD,].reshape(1,-1)\n",
    "            dL_db3 = dL_db3 + L_d22\n",
    "            \n",
    "            L_d21 = p_W4.T @ L_d3\n",
    "            dL_dW2 = dL_dW2 + L_d21 @ q_h1[iD,].reshape(1,-1)\n",
    "            dL_db2 = dL_db2 + L_d21\n",
    "            \n",
    "            L_d1 = (q_W2.T @ L_d21 + q_W3.T @ L_d22) * tanh_gradient_nb(q_h1[iD,]).reshape(-1,1)\n",
    "            dL_dW1 = dL_dW1 + L_d1 @ X[iD,].reshape(1,-1)\n",
    "            dL_db1 = dL_db1 + L_d1\n",
    "        \n",
    "        # back propagation for regularization\n",
    "        R_d22 = ((q_s2[iD,]-1)/2).reshape(-1,1)\n",
    "        dR_dW3 = dR_dW3 + R_d22 @ q_h1[iD,].reshape(1,-1)\n",
    "        dR_db3 = dR_db3 + R_d22\n",
    "\n",
    "        R_d21 = q_mu[iD,].reshape(-1,1)\n",
    "        dR_dW2 = dR_dW2 + R_d21 @ q_h1[iD,].reshape(1,-1)\n",
    "        dR_db2 = dR_db2 + R_d21\n",
    "\n",
    "        R_d1 = (q_W3.T @ R_d22 + q_W2.T @ R_d21) * tanh_gradient_nb(q_h1[iD,]).reshape(-1,1)\n",
    "        dR_dW1 = dR_dW1 + R_d1 @ X[iD,].reshape(1,-1)\n",
    "        dR_db1 = dR_db1 + R_d1\n",
    "    \n",
    "    dJ_dW1 = dL_dW1 / L / M + dR_dW1 / M\n",
    "    dJ_db1 = dL_db1 / L / M + dR_db1 / M\n",
    "    dJ_dW2 = dL_dW2 / L / M + dR_dW2 / M\n",
    "    dJ_db2 = dL_db2 / L / M + dR_db2 / M    \n",
    "    dJ_dW3 = dL_dW3 / L / M + dR_dW3 / M\n",
    "    dJ_db3 = dL_db3 / L / M + dR_db3 / M\n",
    "    dJ_dW4 = dL_dW4 / L / M\n",
    "    dJ_db4 = dL_db4 / L / M \n",
    "    dJ_dW5 = dL_dW5 / L / M\n",
    "    dJ_db5 = dL_db5 / L / M\n",
    "    \n",
    "    dW = [dJ_dW1, dJ_dW2, dJ_dW3, dJ_dW4, dJ_dW5]\n",
    "    db = [dJ_db1, dJ_db2, dJ_db3, dJ_db4, dJ_db5]\n",
    "    \n",
    "    return dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def train_ADAM_nb1(trainX, trainy, Spec, batch_forward, grad, W = \"\", b = \"\"):\n",
    "    \"\"\"train model with ADAM\"\"\"\n",
    "    \n",
    "    nBatch, M, L, std_const, dx, dm, dz, alpha, nP = Spec\n",
    "    \n",
    "    # parameters for ADAM algorithm\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    eps_stable = 1e-8\n",
    "    \n",
    "    # initiate parameters for ADAM\n",
    "    # need to use separate lines of codes otherwise they share the same reference\n",
    "    v_dW, v_db = init_random(dx, dm, dz, option = \"zeros\")\n",
    "    s_dW, s_db = init_random(dx, dm, dz, option = \"zeros\")\n",
    "    vc_dW, vc_db = init_random(dx, dm, dz, option = \"zeros\")\n",
    "    sc_dW, sc_db = init_random(dx, dm, dz, option = \"zeros\")\n",
    "    num_para = len(v_dW)\n",
    "    \n",
    "    # weights and bias initialization\n",
    "    if len(W) == len(b) == 0:\n",
    "        W, b = init_random(dx, dm, dz)\n",
    "\n",
    "    # loss\n",
    "    loss = np.zeros(nBatch)\n",
    "    \n",
    "    for iB in range(nBatch):\n",
    "        # sample a random batch\n",
    "        batchX, batchy = get_Batch(M, trainX, trainy)\n",
    "        X = batchX.reshape(M, dx) / std_const\n",
    "\n",
    "        # sample random noise for latent variable, assuming each batch uses the same random noise for now\n",
    "        eps = np.random.randn(L, dz)\n",
    "\n",
    "        y, H, Lt, loss[iB] = batch_forward(Spec, X, W, b, eps)\n",
    "        dW, db = grad(X, y, W, b, H, Lt)\n",
    "        \n",
    "        # ADAM\n",
    "        for i in range(num_para):\n",
    "            v_dW[i] = beta1*v_dW[i] + (1-beta1)*dW[i]\n",
    "            v_db[i] = beta1*v_db[i] + (1-beta1)*db[i]\n",
    "            s_dW[i] = beta2*s_dW[i] + (1-beta2)*np.power(dW[i],2)\n",
    "            s_db[i] = beta2*s_db[i] + (1-beta2)*np.power(db[i],2)\n",
    "        \n",
    "            vc_dW[i] = v_dW[i]/(1-beta1**(iB+1))\n",
    "            vc_db[i] = v_db[i]/(1-beta1**(iB+1))\n",
    "            sc_dW[i] = s_dW[i]/(1-beta2**(iB+1))\n",
    "            sc_db[i] = s_db[i]/(1-beta2**(iB+1))\n",
    "        \n",
    "            dW[i] = vc_dW[i] / (np.sqrt(sc_dW[i]) + eps_stable)\n",
    "            db[i] = vc_db[i] / (np.sqrt(sc_db[i]) + eps_stable)\n",
    "        \n",
    "        W, b = para_update(W, b, dW, db, alpha)\n",
    "\n",
    "    return Spec, W, b, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_filename = get_filename(\"profiling_v1\", ext = \".prof\")\n",
    "Spec = [100, M, L, std_const, dx, dm, dz, alpha, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-fa9db8bd10df>:1: NumbaWarning: \n",
      "Compilation is falling back to object mode WITH looplifting enabled because Function \"train_ADAM\" failed type inference due to: Untyped global name 'get_filename': cannot determine Numba type of <class 'function'>\n",
      "\n",
      "File \"<ipython-input-17-fa9db8bd10df>\", line 7:\n",
      "def train_ADAM(trainX, trainy, Spec, batch_forward, grad, W = \"\", b = \"\", doc = \"\"):\n",
      "    <source elided>\n",
      "    if doc != \"\":\n",
      "        status_file = get_filename(\"status\")\n",
      "        ^\n",
      "\n",
      "  def train_ADAM(trainX, trainy, Spec, batch_forward, grad, W = \"\", b = \"\", doc = \"\"):\n",
      "<ipython-input-17-fa9db8bd10df>:1: NumbaWarning: \n",
      "Compilation is falling back to object mode WITHOUT looplifting enabled because Function \"train_ADAM\" failed type inference due to: Untyped global name 'get_filename': cannot determine Numba type of <class 'function'>\n",
      "\n",
      "File \"<ipython-input-17-fa9db8bd10df>\", line 7:\n",
      "def train_ADAM(trainX, trainy, Spec, batch_forward, grad, W = \"\", b = \"\", doc = \"\"):\n",
      "    <source elided>\n",
      "    if doc != \"\":\n",
      "        status_file = get_filename(\"status\")\n",
      "        ^\n",
      "\n",
      "  def train_ADAM(trainX, trainy, Spec, batch_forward, grad, W = \"\", b = \"\", doc = \"\"):\n",
      "/opt/conda/lib/python3.6/site-packages/numba/object_mode_passes.py:178: NumbaWarning: Function \"train_ADAM\" was compiled in object mode without forceobj=True, but has lifted loops.\n",
      "\n",
      "File \"<ipython-input-17-fa9db8bd10df>\", line 4:\n",
      "def train_ADAM(trainX, trainy, Spec, batch_forward, grad, W = \"\", b = \"\", doc = \"\"):\n",
      "    <source elided>\n",
      "    \n",
      "    nBatch, M, L, std_const, dx, dm, dz, alpha, nP = Spec\n",
      "    ^\n",
      "\n",
      "  state.func_ir.loc))\n",
      "/opt/conda/lib/python3.6/site-packages/numba/object_mode_passes.py:188: NumbaDeprecationWarning: \n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\n",
      "\n",
      "For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\n",
      "File \"<ipython-input-17-fa9db8bd10df>\", line 4:\n",
      "def train_ADAM(trainX, trainy, Spec, batch_forward, grad, W = \"\", b = \"\", doc = \"\"):\n",
      "    <source elided>\n",
      "    \n",
      "    nBatch, M, L, std_const, dx, dm, dz, alpha, nP = Spec\n",
      "    ^\n",
      "\n",
      "  state.func_ir.loc))\n",
      "<ipython-input-17-fa9db8bd10df>:1: NumbaWarning: Cannot cache compiled function \"train_ADAM\" as it uses lifted loops\n",
      "  def train_ADAM(trainX, trainy, Spec, batch_forward, grad, W = \"\", b = \"\", doc = \"\"):\n",
      "<ipython-input-17-fa9db8bd10df>:1: NumbaWarning: \n",
      "Compilation is falling back to object mode WITHOUT looplifting enabled because Function \"train_ADAM\" failed type inference due to: Untyped global name 'get_Batch': cannot determine Numba type of <class 'function'>\n",
      "\n",
      "File \"<ipython-input-17-fa9db8bd10df>\", line 33:\n",
      "def train_ADAM(trainX, trainy, Spec, batch_forward, grad, W = \"\", b = \"\", doc = \"\"):\n",
      "    <source elided>\n",
      "        # sample a random batch\n",
      "        batchX, batchy = get_Batch(M, trainX, trainy)\n",
      "        ^\n",
      "\n",
      "  def train_ADAM(trainX, trainy, Spec, batch_forward, grad, W = \"\", b = \"\", doc = \"\"):\n",
      "/opt/conda/lib/python3.6/site-packages/numba/object_mode_passes.py:178: NumbaWarning: Function \"train_ADAM\" was compiled in object mode without forceobj=True.\n",
      "\n",
      "File \"<ipython-input-17-fa9db8bd10df>\", line 31:\n",
      "def train_ADAM(trainX, trainy, Spec, batch_forward, grad, W = \"\", b = \"\", doc = \"\"):\n",
      "    <source elided>\n",
      "    \n",
      "    for iB in range(nBatch):\n",
      "    ^\n",
      "\n",
      "  state.func_ir.loc))\n",
      "/opt/conda/lib/python3.6/site-packages/numba/object_mode_passes.py:188: NumbaDeprecationWarning: \n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\n",
      "\n",
      "For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\n",
      "File \"<ipython-input-17-fa9db8bd10df>\", line 31:\n",
      "def train_ADAM(trainX, trainy, Spec, batch_forward, grad, W = \"\", b = \"\", doc = \"\"):\n",
      "    <source elided>\n",
      "    \n",
      "    for iB in range(nBatch):\n",
      "    ^\n",
      "\n",
      "  state.func_ir.loc))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "*** Profile stats marshalled to file '2020_04_29_11_08_34_Model_v1.2_profiling_v1.prof'. \n"
     ]
    }
   ],
   "source": [
    "%prun -q -D $profile_filename train_ADAM_nb1(trainX, \\\n",
    "                                         trainy, Spec, batch_forward_nb1, grad_nb1, doc = \"Optimization v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Apr 29 15:09:32 2020    2020_04_29_11_08_34_Model_v1.2_profiling_v1.prof\n",
      "\n",
      "         8904696 function calls (8251444 primitive calls) in 57.791 seconds\n",
      "\n",
      "   Ordered by: internal time, cumulative time\n",
      "   List reduced from 1533 to 5 due to restriction <5>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "  200/100   31.275    0.156   31.372    0.314 <ipython-input-65-13a3e80c32ab>:1(grad_nb1)\n",
      "3309/3308    4.970    0.002    4.973    0.002 /opt/conda/lib/python3.6/site-packages/llvmlite/binding/ffi.py:112(__call__)\n",
      "      2/1    2.817    1.409   50.895   50.895 <ipython-input-17-fa9db8bd10df>:1(train_ADAM)\n",
      "  200/100    1.861    0.009    2.616    0.026 <ipython-input-61-421a667b6d90>:1(batch_forward_nb1)\n",
      "  1861957    1.079    0.000    1.093    0.000 {built-in method builtins.isinstance}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p = pstats.Stats(profile_filename)\n",
    "p.sort_stats('time', 'cumulative').print_stats(5)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Vectorization\n",
    "\n",
    "I realized that before I proceed further, it might be a good idea to make sure my baseline codes are vectorized as much as I can. In this version, I try at least removing the inner loop for all the functions and vectorize as much as I can. Testing without using numba, just using python, but basic functions are already using numba here. (e.g., tanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_forward_vec(X, W, b):\n",
    "    \"\"\"\n",
    "    encoder forward propagation - vectorized version\n",
    "    X: M by dx\n",
    "    \"\"\"\n",
    "    \n",
    "    q_W1, q_W2, q_W3, d,d = W\n",
    "    q_b1, q_b2, q_b3, d,d = b\n",
    "    \n",
    "    q_a1 = q_W1 @ X.T + q_b1 # dm by M\n",
    "    q_h1 = tanh_nb(q_a1) # dm by M\n",
    "    q_mu = q_W2 @ q_h1 + q_b2 # dz by M\n",
    "    q_a2 = q_W3 @ q_h1 + q_b3 # dz by M\n",
    "    q_s2 = np.exp(q_a2) # dz by M\n",
    "    \n",
    "    return q_h1.T, q_a2.T, q_mu.T, q_s2.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_z_vec(q_mu, q_s2, eps):\n",
    "    \"\"\"sample latent variable z - vectorized version\"\"\"\n",
    "    \n",
    "    M, dz = q_mu.shape \n",
    "\n",
    "    return q_mu.reshape(M,1,dz) + np.sqrt(q_s2).reshape(M,1,dz) * eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_forward_vec(W, b, z):\n",
    "    \"\"\"\n",
    "    decoder forward propagation - vectorized version\n",
    "    z: M by L by dz\n",
    "    \"\"\"\n",
    "    \n",
    "    d,d,d, p_W4, p_W5 = W\n",
    "    d,d,d, p_b4, p_b5 = b\n",
    "    \n",
    "    p_a3 = z @ p_W4.T + p_b4.T # M by L by dm\n",
    "    p_h2 = tanh_nb(p_a3) # M by L by dm\n",
    "            \n",
    "    p_a4 = p_h2 @ p_W5.T + p_b5.T # M by L by dx\n",
    "    y = sigmoid_nb(p_a4) # M by L by dx\n",
    "    \n",
    "    return y, p_h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_forward_vec(Spec, X, W, b, eps):\n",
    "    \"\"\"forward propagation for one batch - vectorized version\"\"\"\n",
    "    d, M, L, d, d, dm, dz, d, d = Spec\n",
    "    \n",
    "    # initialize variables for calculating gradients/ loss by batch\n",
    "    p_h2 = np.zeros((M, L, dm))\n",
    "    z = np.zeros((M, L, dz))\n",
    "    y = np.zeros((M, L, dx))\n",
    "    \n",
    "    q_h1, q_a2, q_mu, q_s2 = encoder_forward_vec(X, W, b)\n",
    "    z = sample_z_vec(q_mu, q_s2, eps)\n",
    "    y, p_h2 = decoder_forward_vec(W, b, z)\n",
    "    \n",
    "    H = [q_h1, p_h2]\n",
    "    Lt = [q_mu, q_s2, z, eps]\n",
    "    loss = total_loss_vec(X, y, q_a2, q_mu, q_s2)\n",
    "    \n",
    "    return (y, H, Lt, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"grad_vec\" is the vectorized version of \"grad\" without inner loop. However, not sure why, @jit doesn't work on it. It keeps reporting errors for cannot unboxing the list..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_vec(X, y, W, b, H, Lt):\n",
    "    \"\"\"\"\n",
    "    batch gradient\n",
    "    inputs:\n",
    "        X: M by dx\n",
    "        y: M by L by dx\n",
    "        W: weights\n",
    "        b: bias\n",
    "        H: q_h1 (M by dm), p_h2 (M by L by dm)\n",
    "        Lt: eps (L by dz), z (M by L by dz), q_s2 (M by dz), q_mu (M by dz)\n",
    "    \"\"\"\n",
    "    q_W1, q_W2, q_W3, p_W4, p_W5 = W\n",
    "    q_b1, q_b2, q_b3, p_b4, p_b5 = b\n",
    "    q_h1, p_h2 = H\n",
    "    q_mu, q_s2, z, eps = Lt\n",
    "    \n",
    "    M = X.shape[0]\n",
    "    L = y.shape[1]\n",
    "    \n",
    "    # initialize gradient variables\n",
    "    \n",
    "    # L: loss; R: regularization; J: total target\n",
    "    dL_dW1 = dJ_dW1 = dR_dW1 = np.zeros_like(q_W1)\n",
    "    dL_db1 = dJ_db1 = dR_db1 = np.zeros_like(q_b1)\n",
    "    dL_dW2 = dJ_dW2 = dR_dW2 = np.zeros_like(q_W2)\n",
    "    dL_db2 = dJ_db2 = dR_db2 = np.zeros_like(q_b2)\n",
    "    dL_dW3 = dJ_dW3 = dR_dW3 = np.zeros_like(q_W3)\n",
    "    dL_db3 = dJ_db3 = dR_db3 = np.zeros_like(q_b3)\n",
    "    dL_dW4 = dJ_dW4 = np.zeros_like(p_W4)\n",
    "    dL_db4 = dJ_db4 = np.zeros_like(p_b4)    \n",
    "    dL_dW5 = dJ_dW5 = np.zeros_like(p_W5)\n",
    "    dL_db5 = dJ_db5 = np.zeros_like(p_b5)\n",
    "    \n",
    "    for iD in range(M):\n",
    "        \n",
    "        # back propagation for loss\n",
    "        L_d4 = y[iD,] - X[iD,].reshape(1,-1) # L by dx\n",
    "        dL_dW5 = dL_dW5 + L_d4.T @ p_h2[iD,] # dx by dm\n",
    "        dL_db5 = dL_db5 + np.sum(L_d4, axis = 0).reshape(-1,1) # dx by 1\n",
    "        \n",
    "        L_d3 = L_d4 @ p_W5 * tanh_gradient_nb(p_h2[iD,]) # L by dm\n",
    "        dL_dW4 = dL_dW4 + L_d3.T @ z[iD,] # dm by dz\n",
    "        dL_db4 = dL_db4 + np.sum(L_d3, axis = 0).reshape(-1,1) # dm by 1\n",
    "        \n",
    "        L_d22 = (np.sum(L_d3 @ p_W4 * eps, axis=0)* np.sqrt(q_s2[iD,]) / 2).reshape(-1,1)  # dz by 1\n",
    "        dL_dW3 = dL_dW3 + L_d22 @ q_h1[iD,].reshape(1,-1) # dz by dm\n",
    "        dL_db3 = dL_db3 + L_d22 # dz by 1\n",
    "        \n",
    "        L_d21 = np.sum(L_d3 @ p_W4, axis = 0).reshape(-1,1) # dz by 1\n",
    "        dL_dW2 = dL_dW2 + L_d21 @ q_h1[iD,].reshape(1,-1) # dz by dm\n",
    "        dL_db2 = dL_db2 + L_d21 # dz by 1\n",
    "        \n",
    "        L_d1 = (q_W2.T @ L_d21 + q_W3.T @ L_d22) * tanh_gradient_nb(q_h1[iD,]).reshape(-1,1) #dm by 1\n",
    "        dL_dW1 = dL_dW1 + L_d1 @ X[iD,].reshape(1,-1) # dm by dx\n",
    "        dL_db1 = dL_db1 + L_d1 # dm by 1\n",
    "\n",
    "        # back propagation for regularization\n",
    "        R_d22 = ((q_s2[iD,]-1)/2).reshape(-1,1)\n",
    "        dR_dW3 = dR_dW3 + R_d22 @ q_h1[iD,].reshape(1,-1)\n",
    "        dR_db3 = dR_db3 + R_d22\n",
    "\n",
    "        R_d21 = q_mu[iD,].reshape(-1,1)\n",
    "        dR_dW2 = dR_dW2 + R_d21 @ q_h1[iD,].reshape(1,-1)\n",
    "        dR_db2 = dR_db2 + R_d21\n",
    "\n",
    "        R_d1 = (q_W3.T @ R_d22 + q_W2.T @ R_d21) * tanh_gradient_nb(q_h1[iD,]).reshape(-1,1)\n",
    "        dR_dW1 = dR_dW1 + R_d1 @ X[iD,].reshape(1,-1)\n",
    "        dR_db1 = dR_db1 + R_d1\n",
    "    \n",
    "    dJ_dW1 = dL_dW1 / L / M + dR_dW1 / M\n",
    "    dJ_db1 = dL_db1 / L / M + dR_db1 / M\n",
    "    dJ_dW2 = dL_dW2 / L / M + dR_dW2 / M\n",
    "    dJ_db2 = dL_db2 / L / M + dR_db2 / M    \n",
    "    dJ_dW3 = dL_dW3 / L / M + dR_dW3 / M\n",
    "    dJ_db3 = dL_db3 / L / M + dR_db3 / M\n",
    "    dJ_dW4 = dL_dW4 / L / M\n",
    "    dJ_db4 = dL_db4 / L / M \n",
    "    dJ_dW5 = dL_dW5 / L / M\n",
    "    dJ_db5 = dL_db5 / L / M\n",
    "    \n",
    "    dW = [dJ_dW1, dJ_dW2, dJ_dW3, dJ_dW4, dJ_dW5]\n",
    "    db = [dJ_db1, dJ_db2, dJ_db3, dJ_db4, dJ_db5]\n",
    "    \n",
    "    return dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_W1, q_W2, q_W3, p_W4, p_W5 = W\n",
    "q_b1, q_b2, q_b3, p_b4, p_b5 = b\n",
    "q_h1, p_h2 = H\n",
    "q_mu, q_s2, z, eps = Lt\n",
    "\n",
    "M = X.shape[0]\n",
    "L = y.shape[1]\n",
    "\n",
    "# initialize gradient variables\n",
    "\n",
    "# L: loss; R: regularization; J: total target\n",
    "dL_dW1 = dJ_dW1 = dR_dW1 = np.zeros_like(q_W1)\n",
    "dL_db1 = dJ_db1 = dR_db1 = np.zeros_like(q_b1)\n",
    "dL_dW2 = dJ_dW2 = dR_dW2 = np.zeros_like(q_W2)\n",
    "dL_db2 = dJ_db2 = dR_db2 = np.zeros_like(q_b2)\n",
    "dL_dW3 = dJ_dW3 = dR_dW3 = np.zeros_like(q_W3)\n",
    "dL_db3 = dJ_db3 = dR_db3 = np.zeros_like(q_b3)\n",
    "dL_dW4 = dJ_dW4 = np.zeros_like(p_W4)\n",
    "dL_db4 = dJ_db4 = np.zeros_like(p_b4)    \n",
    "dL_dW5 = dJ_dW5 = np.zeros_like(p_W5)\n",
    "dL_db5 = dJ_db5 = np.zeros_like(p_b5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 1)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iL = 0\n",
    "\n",
    "y_iL = y[:,iL,:] # M by dx\n",
    "p_h2_iL = p_h2[:,iL,:] # M by dm\n",
    "z_iL = z[:,iL,:] # M by dz\n",
    "\n",
    "# back propagation for loss\n",
    "L_d4 = y_iL - X # M by dx\n",
    "dL_dW5 = dL_dW5 + L_d4.T @ p_h2_iL # dx by dm\n",
    "dL_db5 = dL_db5 + np.sum(L_d4, axis = 0).reshape(-1,1) # dx by 1\n",
    "\n",
    "L_d3 = L_d4 @ p_W5 * tanh_gradient_nb(p_h2_iL) # M by dm\n",
    "dL_dW4 = dL_dW4 + L_d3.T @ z_iL # dm by dz\n",
    "dL_db4 = dL_db4 + np.sum(L_d3, axis = 0).reshape(-1,1) # dm by 1\n",
    "\n",
    "L_d22 = L_d3 @ p_W4 * eps[iL,] * np.sqrt(q_s2) / 2  # M by dz\n",
    "dL_dW3 = dL_dW3 + L_d22.T @ q_h1 # dz by dm\n",
    "dL_db3 = dL_db3 + np.sum(L_d22, axis = 0).reshape(-1,1) # dz by 1\n",
    "\n",
    "L_d21 = L_d3 @ p_W4 # M by dz\n",
    "dL_dW2 = dL_dW2 + L_d21.T @ q_h1 # dz by dm\n",
    "dL_db2 = dL_db2 + np.sum(L_d21, axis = 0).reshape(-1,1) # dz by 1\n",
    "\n",
    "L_d1 = (L_d21 @ q_W2  + L_d22 @ q_W3) * tanh_gradient_nb(q_h1) # M by dm\n",
    "dL_dW1 = dL_dW1 + L_d1.T @ X # dm by dx\n",
    "dL_db1 = dL_db1 + np.sum(L_d1, axis = 0).reshape(-1,1) # dm by 1\n",
    "\n",
    "dL_db1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 3)"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(L_d3 @ p_W4 * eps[iL,]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_vec2(X, y, W, b, H, Lt):\n",
    "    \"\"\"\"\n",
    "    batch gradient\n",
    "    inputs:\n",
    "        X: M by dx\n",
    "        y: M by L by dx\n",
    "        W: weights\n",
    "        b: bias\n",
    "        H: q_h1 (M by dm), p_h2 (M by L by dm)\n",
    "        Lt: eps (L by dz), z (M by L by dz), q_s2 (M by dz), q_mu (M by dz)\n",
    "    \"\"\"\n",
    "    q_W1, q_W2, q_W3, p_W4, p_W5 = W\n",
    "    q_b1, q_b2, q_b3, p_b4, p_b5 = b\n",
    "    q_h1, p_h2 = H\n",
    "    q_mu, q_s2, z, eps = Lt\n",
    "    \n",
    "    M = X.shape[0]\n",
    "    L = y.shape[1]\n",
    "    \n",
    "    # initialize gradient variables\n",
    "    \n",
    "    # L: loss; R: regularization; J: total target\n",
    "    dL_dW1 = dJ_dW1 = dR_dW1 = np.zeros_like(q_W1)\n",
    "    dL_db1 = dJ_db1 = dR_db1 = np.zeros_like(q_b1)\n",
    "    dL_dW2 = dJ_dW2 = dR_dW2 = np.zeros_like(q_W2)\n",
    "    dL_db2 = dJ_db2 = dR_db2 = np.zeros_like(q_b2)\n",
    "    dL_dW3 = dJ_dW3 = dR_dW3 = np.zeros_like(q_W3)\n",
    "    dL_db3 = dJ_db3 = dR_db3 = np.zeros_like(q_b3)\n",
    "    dL_dW4 = dJ_dW4 = np.zeros_like(p_W4)\n",
    "    dL_db4 = dJ_db4 = np.zeros_like(p_b4)    \n",
    "    dL_dW5 = dJ_dW5 = np.zeros_like(p_W5)\n",
    "    dL_db5 = dJ_db5 = np.zeros_like(p_b5)\n",
    "\n",
    "    # back propagation for loss\n",
    "    for iL in range(L):\n",
    "        y_iL = y[:,iL,:] # M by dx\n",
    "        p_h2_iL = p_h2[:,iL,:] # M by dm\n",
    "        z_iL = z[:,iL,:] # M by dz\n",
    "        \n",
    "        L_d4 = y_iL - X # M by dx\n",
    "        dL_dW5 = dL_dW5 + L_d4.T @ p_h2_iL # dx by dm\n",
    "        dL_db5 = dL_db5 + np.sum(L_d4, axis = 0).reshape(-1,1) # dx by 1\n",
    "        \n",
    "        L_d3 = L_d4 @ p_W5 * tanh_gradient_nb(p_h2_iL) # M by dm\n",
    "        dL_dW4 = dL_dW4 + L_d3.T @ z_iL # dm by dz\n",
    "        dL_db4 = dL_db4 + np.sum(L_d3, axis = 0).reshape(-1,1) # dm by 1\n",
    "        \n",
    "        L_d22 = L_d3 @ p_W4 * eps[iL,] * np.sqrt(q_s2) / 2  # M by dz\n",
    "        dL_dW3 = dL_dW3 + L_d22.T @ q_h1 # dz by dm\n",
    "        dL_db3 = dL_db3 + np.sum(L_d22, axis = 0).reshape(-1,1) # dz by 1\n",
    "        \n",
    "        L_d21 = L_d3 @ p_W4 # M by dz\n",
    "        dL_dW2 = dL_dW2 + L_d21.T @ q_h1 # dz by dm\n",
    "        dL_db2 = dL_db2 + np.sum(L_d21, axis = 0).reshape(-1,1) # dz by 1\n",
    "\n",
    "        L_d1 = (L_d21 @ q_W2 + L_d22 @ q_W3) * tanh_gradient_nb(q_h1) # M by dm\n",
    "        dL_dW1 = dL_dW1 + L_d1.T @ X # dm by dx\n",
    "        dL_db1 = dL_db1 + np.sum(L_d1, axis = 0).reshape(-1,1) # dm by 1\n",
    "\n",
    "    # back propagation for regularization\n",
    "    R_d22 = (q_s2 - 1)/2 # M by dz\n",
    "    dR_dW3 = dR_dW3 + R_d22.T @ q_h1 # dz by dm\n",
    "    dR_db3 = dR_db3 + np.sum(R_d22, axis = 0).reshape(-1,1) # dz by 1\n",
    "    \n",
    "    R_d21 = q_mu # M by dz\n",
    "    dR_dW2 = dR_dW2 + R_d21.T @ q_h1 # dz by dm\n",
    "    dR_db2 = dR_db2 + np.sum(R_d21, axis = 0).reshape(-1,1) # dm by 1\n",
    "\n",
    "    R_d1 = (R_d22 @ q_W3 + R_d21 @ q_W2) * tanh_gradient_nb(q_h1) # M by dm\n",
    "    dR_dW1 = dR_dW1 + R_d1.T @ X # dm by dx\n",
    "    dR_db1 = dR_db1 + np.sum(R_d1, axis = 0).reshape(-1,1) # dm by 1\n",
    "    \n",
    "    dJ_dW1 = dL_dW1 / L / M + dR_dW1 / M\n",
    "    dJ_db1 = dL_db1 / L / M + dR_db1 / M\n",
    "    dJ_dW2 = dL_dW2 / L / M + dR_dW2 / M\n",
    "    dJ_db2 = dL_db2 / L / M + dR_db2 / M    \n",
    "    dJ_dW3 = dL_dW3 / L / M + dR_dW3 / M\n",
    "    dJ_db3 = dL_db3 / L / M + dR_db3 / M\n",
    "    dJ_dW4 = dL_dW4 / L / M\n",
    "    dJ_db4 = dL_db4 / L / M \n",
    "    dJ_dW5 = dL_dW5 / L / M\n",
    "    dJ_db5 = dL_db5 / L / M\n",
    "    \n",
    "    dW = [dJ_dW1, dJ_dW2, dJ_dW3, dJ_dW4, dJ_dW5]\n",
    "    db = [dJ_db1, dJ_db2, dJ_db3, dJ_db4, dJ_db5]\n",
    "    \n",
    "    return dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_loss_vec(X, y, q_a2, q_mu, q_s2):\n",
    "    \"\"\"target total loss function - to minimize - vectorized version\"\"\"\n",
    "    \n",
    "    M, dx, L = X.shape[0], X.shape[1], y.shape[1]\n",
    "    \n",
    "    # reconstruction loss for each sample of latent variable\n",
    "    loss = -np.sum(X.reshape(M,1,dx) * np.log(y) + (1-X.reshape(M,1,dx))* np.log(1-y)) / L\n",
    "    \n",
    "    # KL divergence/ regularization\n",
    "    loss = (loss + np.sum(np.power(q_mu,2) + q_s2 - q_a2 - 1)/2)/M\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_filename = get_filename(\"profiling_v2\", ext = \".prof\")\n",
    "Spec = [100, M, L, std_const, dx, dm, dz, alpha, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-27cd83e81402>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'prun'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-q -D $profile_filename train_ADAM_nb1(trainX,                                          trainy, Spec, batch_forward_vec, grad_vec)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2093\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2094\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2095\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2096\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m</opt/conda/lib/python3.6/site-packages/decorator.py:decorator-gen-56>\u001b[0m in \u001b[0;36mprun\u001b[0;34m(self, parameter_s, cell)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mprun\u001b[0;34m(self, parameter_s, cell)\u001b[0m\n\u001b[1;32m    300\u001b[0m             \u001b[0marg_str\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'\\n'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0marg_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_splitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_with_profiler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_with_profiler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36m_run_with_profiler\u001b[0;34m(self, code, opts, namespace)\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0mprof\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprofile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m             \u001b[0mprof\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprof\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunctx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0msys_exit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSystemExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/cProfile.py\u001b[0m in \u001b[0;36mrunctx\u001b[0;34m(self, cmd, globals, locals)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<string>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-e51089a23302>\u001b[0m in \u001b[0;36mpara_update\u001b[0;34m(W, b, dW, db, alpha)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mpara_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"\"\"update weights and bias\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdW\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%prun -q -D $profile_filename train_ADAM_nb1(trainX, \\\n",
    "                                         trainy, Spec, batch_forward_vec, grad_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Apr 29 15:11:40 2020    2020_04_29_11_10_23_Model_v1.2_profiling_v2.prof\n",
      "\n",
      "         9404869 function calls (8754161 primitive calls) in 72.483 seconds\n",
      "\n",
      "   Ordered by: internal time, cumulative time\n",
      "   List reduced from 1529 to 5 due to restriction <5>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "      100   45.790    0.458   47.319    0.473 <ipython-input-105-c1a8bcbac274>:1(grad_vec)\n",
      "3753/3748    4.998    0.001    5.002    0.001 /opt/conda/lib/python3.6/site-packages/llvmlite/binding/ffi.py:112(__call__)\n",
      "      2/1    2.576    1.288   65.482   65.482 <ipython-input-17-fa9db8bd10df>:1(train_ADAM)\n",
      "  1899960    1.106    0.000    1.108    0.000 {built-in method builtins.isinstance}\n",
      "538009/221547    0.944    0.000    4.976    0.000 {method 'format' of 'str' objects}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p = pstats.Stats(profile_filename)\n",
    "p.sort_stats('time', 'cumulative').print_stats(5)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. vectorized numba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in cannot use nopython mode for matrix multiplication beyond 3-d using \"@\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss_vec_nb = jit(total_loss_vec, nopython=True, cache=True)\n",
    "sample_z_vec_nb = jit(sample_z_vec, nopython=True, cache=True)\n",
    "encoder_forward_vec_nb1 = jit(encoder_forward_vec, nopython=True, cache=True)\n",
    "decoder_forward_vec_nb1 = jit(decoder_forward_vec, cache=True)\n",
    "grad_vec_nb1 = jit(grad_vec, cache=True)\n",
    "grad_vec2_nb1 = jit(grad_vec2, cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(cache=True)\n",
    "def batch_forward_vec_nb1(Spec, X, W, b, eps):\n",
    "    \"\"\"forward propagation for one batch - vectorized version\"\"\"\n",
    "    d, M, L, d, d, dm, dz, d, d = Spec\n",
    "    \n",
    "    # initialize variables for calculating gradients/ loss by batch\n",
    "    p_h2 = np.zeros((M, L, dm))\n",
    "    z = np.zeros((M, L, dz))\n",
    "    y = np.zeros((M, L, dx))\n",
    "    \n",
    "    q_h1, q_a2, q_mu, q_s2 = encoder_forward_vec_nb1(X, W, b)\n",
    "    z = sample_z_vec_nb(q_mu, q_s2, eps)\n",
    "    y, p_h2 = decoder_forward_vec_nb1(W, b, z)\n",
    "    \n",
    "    H = [q_h1, p_h2]\n",
    "    Lt = [q_mu, q_s2, z, eps]\n",
    "    loss = total_loss_vec_nb(X, y, q_a2, q_mu, q_s2)\n",
    "    \n",
    "    return (y, H, Lt, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_filename = get_filename(\"profiling_v3\", ext = \".prof\")\n",
    "Spec = [100, M, L, std_const, dx, dm, dz, alpha, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-64-7ae8c8621fea>:1: NumbaWarning: \n",
      "Compilation is falling back to object mode WITH looplifting enabled because Function \"train_ADAM_nb1\" failed type inference due to: Untyped global name 'init_random': cannot determine Numba type of <class 'function'>\n",
      "\n",
      "File \"<ipython-input-64-7ae8c8621fea>\", line 14:\n",
      "def train_ADAM_nb1(trainX, trainy, Spec, batch_forward, grad, W = \"\", b = \"\"):\n",
      "    <source elided>\n",
      "    # need to use separate lines of codes otherwise they share the same reference\n",
      "    v_dW, v_db = init_random(dx, dm, dz, option = \"zeros\")\n",
      "    ^\n",
      "\n",
      "  @jit\n",
      "<ipython-input-64-7ae8c8621fea>:1: NumbaWarning: \n",
      "Compilation is falling back to object mode WITHOUT looplifting enabled because Function \"train_ADAM_nb1\" failed type inference due to: Untyped global name 'init_random': cannot determine Numba type of <class 'function'>\n",
      "\n",
      "File \"<ipython-input-64-7ae8c8621fea>\", line 14:\n",
      "def train_ADAM_nb1(trainX, trainy, Spec, batch_forward, grad, W = \"\", b = \"\"):\n",
      "    <source elided>\n",
      "    # need to use separate lines of codes otherwise they share the same reference\n",
      "    v_dW, v_db = init_random(dx, dm, dz, option = \"zeros\")\n",
      "    ^\n",
      "\n",
      "  @jit\n",
      "/opt/conda/lib/python3.6/site-packages/numba/object_mode_passes.py:178: NumbaWarning: Function \"train_ADAM_nb1\" was compiled in object mode without forceobj=True, but has lifted loops.\n",
      "\n",
      "File \"<ipython-input-64-7ae8c8621fea>\", line 5:\n",
      "def train_ADAM_nb1(trainX, trainy, Spec, batch_forward, grad, W = \"\", b = \"\"):\n",
      "    <source elided>\n",
      "    \n",
      "    nBatch, M, L, std_const, dx, dm, dz, alpha, nP = Spec\n",
      "    ^\n",
      "\n",
      "  state.func_ir.loc))\n",
      "/opt/conda/lib/python3.6/site-packages/numba/object_mode_passes.py:188: NumbaDeprecationWarning: \n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\n",
      "\n",
      "For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\n",
      "File \"<ipython-input-64-7ae8c8621fea>\", line 5:\n",
      "def train_ADAM_nb1(trainX, trainy, Spec, batch_forward, grad, W = \"\", b = \"\"):\n",
      "    <source elided>\n",
      "    \n",
      "    nBatch, M, L, std_const, dx, dm, dz, alpha, nP = Spec\n",
      "    ^\n",
      "\n",
      "  state.func_ir.loc))\n",
      "<ipython-input-64-7ae8c8621fea>:1: NumbaWarning: \n",
      "Compilation is falling back to object mode WITHOUT looplifting enabled because Function \"train_ADAM_nb1\" failed type inference due to: Untyped global name 'get_Batch': cannot determine Numba type of <class 'function'>\n",
      "\n",
      "File \"<ipython-input-64-7ae8c8621fea>\", line 29:\n",
      "def train_ADAM_nb1(trainX, trainy, Spec, batch_forward, grad, W = \"\", b = \"\"):\n",
      "    <source elided>\n",
      "        # sample a random batch\n",
      "        batchX, batchy = get_Batch(M, trainX, trainy)\n",
      "        ^\n",
      "\n",
      "  @jit\n",
      "/opt/conda/lib/python3.6/site-packages/numba/object_mode_passes.py:178: NumbaWarning: Function \"train_ADAM_nb1\" was compiled in object mode without forceobj=True.\n",
      "\n",
      "File \"<ipython-input-64-7ae8c8621fea>\", line 27:\n",
      "def train_ADAM_nb1(trainX, trainy, Spec, batch_forward, grad, W = \"\", b = \"\"):\n",
      "    <source elided>\n",
      "    \n",
      "    for iB in range(nBatch):\n",
      "    ^\n",
      "\n",
      "  state.func_ir.loc))\n",
      "/opt/conda/lib/python3.6/site-packages/numba/object_mode_passes.py:188: NumbaDeprecationWarning: \n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\n",
      "\n",
      "For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\n",
      "File \"<ipython-input-64-7ae8c8621fea>\", line 27:\n",
      "def train_ADAM_nb1(trainX, trainy, Spec, batch_forward, grad, W = \"\", b = \"\"):\n",
      "    <source elided>\n",
      "    \n",
      "    for iB in range(nBatch):\n",
      "    ^\n",
      "\n",
      "  state.func_ir.loc))\n",
      "<ipython-input-87-60a49735a231>:1: NumbaWarning: \n",
      "Compilation is falling back to object mode WITH looplifting enabled because Function \"batch_forward_vec_nb1\" failed type inference due to: Invalid use of type(CPUDispatcher(<function decoder_forward_vec at 0x7fad87f52510>)) with parameters (reflected list(array(float64, 2d, C)), reflected list(array(float64, 2d, C)), array(float64, 3d, C))\n",
      " * parameterized\n",
      "[1] During: resolving callee type: type(CPUDispatcher(<function decoder_forward_vec at 0x7fad87f52510>))\n",
      "[2] During: typing of call at <ipython-input-87-60a49735a231> (13)\n",
      "\n",
      "\n",
      "File \"<ipython-input-87-60a49735a231>\", line 13:\n",
      "def batch_forward_vec_nb1(Spec, X, W, b, eps):\n",
      "    <source elided>\n",
      "    z = sample_z_vec_nb(q_mu, q_s2, eps)\n",
      "    y, p_h2 = decoder_forward_vec_nb1(W, b, z)\n",
      "    ^\n",
      "\n",
      "  @jit(cache=True)\n",
      "/opt/conda/lib/python3.6/site-packages/numba/object_mode_passes.py:178: NumbaWarning: Function \"batch_forward_vec_nb1\" was compiled in object mode without forceobj=True.\n",
      "\n",
      "File \"<ipython-input-87-60a49735a231>\", line 2:\n",
      "@jit(cache=True)\n",
      "def batch_forward_vec_nb1(Spec, X, W, b, eps):\n",
      "^\n",
      "\n",
      "  state.func_ir.loc))\n",
      "/opt/conda/lib/python3.6/site-packages/numba/object_mode_passes.py:188: NumbaDeprecationWarning: \n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\n",
      "\n",
      "For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\n",
      "File \"<ipython-input-87-60a49735a231>\", line 2:\n",
      "@jit(cache=True)\n",
      "def batch_forward_vec_nb1(Spec, X, W, b, eps):\n",
      "^\n",
      "\n",
      "  state.func_ir.loc))\n",
      "<ipython-input-88-903b80429a66>:1: NumbaWarning: \n",
      "Compilation is falling back to object mode WITH looplifting enabled because Function \"grad_vec_nb1\" failed type inference due to: Internal error at <numba.typeinfer.CallConstraint object at 0x7fad7fe73588>.\n",
      "reshape() supports contiguous array only\n",
      "[1] During: resolving callee type: BoundFunction(array.reshape for array(float64, 1d, A))\n",
      "[2] During: typing of call at <ipython-input-88-903b80429a66> (47)\n",
      "\n",
      "Enable logging at debug level for details.\n",
      "\n",
      "File \"<ipython-input-88-903b80429a66>\", line 47:\n",
      "def grad_vec_nb1(X, y, W, b, H, Lt):\n",
      "    <source elided>\n",
      "        L_d22 = (np.sum(L_d3 @ p_W4 * eps, axis=0)* np.sqrt(q_s2[iD,]) / 2).reshape(-1,1)  # dz by 1\n",
      "        dL_dW3 = dL_dW3 + L_d22 @ q_h1[iD,].reshape(1,-1) # dz by dm\n",
      "        ^\n",
      "\n",
      "  @jit(cache=True)\n",
      "<ipython-input-88-903b80429a66>:1: NumbaWarning: \n",
      "Compilation is falling back to object mode WITHOUT looplifting enabled because Function \"grad_vec_nb1\" failed type inference due to: cannot determine Numba type of <class 'numba.dispatcher.LiftedLoop'>\n",
      "\n",
      "File \"<ipython-input-88-903b80429a66>\", line 35:\n",
      "def grad_vec_nb1(X, y, W, b, H, Lt):\n",
      "    <source elided>\n",
      "    \n",
      "    for iD in range(M):\n",
      "    ^\n",
      "\n",
      "  @jit(cache=True)\n",
      "/opt/conda/lib/python3.6/site-packages/numba/object_mode_passes.py:178: NumbaWarning: Function \"grad_vec_nb1\" was compiled in object mode without forceobj=True, but has lifted loops.\n",
      "\n",
      "File \"<ipython-input-88-903b80429a66>\", line 13:\n",
      "def grad_vec_nb1(X, y, W, b, H, Lt):\n",
      "    <source elided>\n",
      "    \"\"\"\n",
      "    q_W1, q_W2, q_W3, p_W4, p_W5 = W\n",
      "    ^\n",
      "\n",
      "  state.func_ir.loc))\n",
      "/opt/conda/lib/python3.6/site-packages/numba/object_mode_passes.py:188: NumbaDeprecationWarning: \n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\n",
      "\n",
      "For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\n",
      "File \"<ipython-input-88-903b80429a66>\", line 13:\n",
      "def grad_vec_nb1(X, y, W, b, H, Lt):\n",
      "    <source elided>\n",
      "    \"\"\"\n",
      "    q_W1, q_W2, q_W3, p_W4, p_W5 = W\n",
      "    ^\n",
      "\n",
      "  state.func_ir.loc))\n",
      "<ipython-input-88-903b80429a66>:1: NumbaWarning: Cannot cache compiled function \"grad_vec_nb1\" as it uses lifted loops\n",
      "  @jit(cache=True)\n",
      "<ipython-input-88-903b80429a66>:1: NumbaWarning: \n",
      "Compilation is falling back to object mode WITHOUT looplifting enabled because Function \"grad_vec_nb1\" failed type inference due to: Internal error at <numba.typeinfer.CallConstraint object at 0x7fad7a086f28>.\n",
      "reshape() supports contiguous array only\n",
      "[1] During: resolving callee type: BoundFunction(array.reshape for array(float64, 1d, A))\n",
      "[2] During: typing of call at <ipython-input-88-903b80429a66> (47)\n",
      "\n",
      "Enable logging at debug level for details.\n",
      "\n",
      "File \"<ipython-input-88-903b80429a66>\", line 47:\n",
      "def grad_vec_nb1(X, y, W, b, H, Lt):\n",
      "    <source elided>\n",
      "        L_d22 = (np.sum(L_d3 @ p_W4 * eps, axis=0)* np.sqrt(q_s2[iD,]) / 2).reshape(-1,1)  # dz by 1\n",
      "        dL_dW3 = dL_dW3 + L_d22 @ q_h1[iD,].reshape(1,-1) # dz by dm\n",
      "        ^\n",
      "\n",
      "  @jit(cache=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/numba/object_mode_passes.py:178: NumbaWarning: Function \"grad_vec_nb1\" was compiled in object mode without forceobj=True.\n",
      "\n",
      "File \"<ipython-input-88-903b80429a66>\", line 35:\n",
      "def grad_vec_nb1(X, y, W, b, H, Lt):\n",
      "    <source elided>\n",
      "    \n",
      "    for iD in range(M):\n",
      "    ^\n",
      "\n",
      "  state.func_ir.loc))\n",
      "/opt/conda/lib/python3.6/site-packages/numba/object_mode_passes.py:188: NumbaDeprecationWarning: \n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\n",
      "\n",
      "For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\n",
      "File \"<ipython-input-88-903b80429a66>\", line 35:\n",
      "def grad_vec_nb1(X, y, W, b, H, Lt):\n",
      "    <source elided>\n",
      "    \n",
      "    for iD in range(M):\n",
      "    ^\n",
      "\n",
      "  state.func_ir.loc))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "*** Profile stats marshalled to file '2020_04_29_14_27_41_Model_v1.2_profiling_v3.prof'. \n"
     ]
    }
   ],
   "source": [
    "%prun -q -D $profile_filename train_ADAM_nb1(trainX, \\\n",
    "                                         trainy, Spec, batch_forward_vec_nb1, grad_vec_nb1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Apr 29 18:29:16 2020    2020_04_29_14_27_41_Model_v1.2_profiling_v3.prof\n",
      "\n",
      "         17602681 function calls (16342310 primitive calls) in 95.051 seconds\n",
      "\n",
      "   Ordered by: internal time, cumulative time\n",
      "   List reduced from 1847 to 5 due to restriction <5>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "  200/100   43.794    0.219   59.720    0.597 <ipython-input-88-903b80429a66>:1(grad_vec_nb1)\n",
      "     6135    9.618    0.002    9.635    0.002 /opt/conda/lib/python3.6/site-packages/llvmlite/binding/ffi.py:112(__call__)\n",
      "      100    4.505    0.045    5.166    0.052 <ipython-input-31-47d76a1e376f>:1(decoder_forward_vec)\n",
      "      2/1    2.503    1.251   89.789   89.789 <ipython-input-64-7ae8c8621fea>:1(train_ADAM_nb1)\n",
      "  3699979    2.170    0.000    2.242    0.000 {built-in method builtins.isinstance}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p = pstats.Stats(profile_filename)\n",
    "p.sort_stats('time', 'cumulative').print_stats(5)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. non-vectorized numba nopython\n",
    "\n",
    "From the minimalist version, it seems that for now I cannot use nopython for some of the functions mainly because I'm using list to pass on variables. So try to change this as the next step to see how much speed up I can get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_Batch_nb = jit(get_Batch, nopython=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit(cache=True)\n",
    "def init_random_nb2(dx, dm, dz, option = \"xavier\"):\n",
    "    \"\"\"\n",
    "    parameter random initialization\n",
    "    xavier initialization for parameters\n",
    "    all zero for bias\n",
    "    \"\"\"\n",
    "    \n",
    "    # weights initialization\n",
    "    if option == \"zeros\":\n",
    "        # only for variables in ADAM algorithm, not to be used for true model parameters\n",
    "        q_W1 = np.zeros((dm, dx))\n",
    "        p_W5 = np.zeros((dx, dm))\n",
    "        q_W2 = np.zeros((dz, dm))\n",
    "        q_W3 = np.zeros((dz, dm))\n",
    "        p_W4 = np.zeros((dm, dz))\n",
    "    elif option == \"xavier\":\n",
    "        bound = np.sqrt(6)/ np.sqrt(dx + dm)\n",
    "        q_W1 = np.random.uniform(-bound, bound, (dm, dx))\n",
    "        p_W5 = np.random.uniform(-bound, bound, (dx, dm))\n",
    "        bound = np.sqrt(6)/ np.sqrt(dm + dz)\n",
    "        q_W2 = np.random.uniform(-bound, bound, (dz, dm))\n",
    "        q_W3 = np.random.uniform(-bound, bound, (dz, dm))\n",
    "        p_W4 = np.random.uniform(-bound, bound, (dm, dz))\n",
    "    \n",
    "    # bias initialization\n",
    "    q_b1 = np.zeros((dm, 1))\n",
    "    p_b5 = np.zeros((dx, 1))\n",
    "    q_b2 = np.zeros((dz, 1))\n",
    "    q_b3 = np.zeros((dz, 1))\n",
    "    p_b4 = np.zeros((dm, 1))\n",
    "\n",
    "    return q_W1, q_W2, q_W3, p_W4, p_W5, q_b1, q_b2, q_b3, p_b4, p_b5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit(cache=True)\n",
    "def encoder_forward_nb2(X, q_W1, q_W2, q_W3, q_b1, q_b2, q_b3):\n",
    "    \"\"\"\n",
    "    encoder forward propagation for each data point\n",
    "    X: dx by 1\n",
    "    \"\"\"\n",
    "    \n",
    "    q_a1 = q_W1 @ X + q_b1\n",
    "    q_h1 = tanh_nb(q_a1)\n",
    "    q_mu = q_W2 @ q_h1 + q_b2\n",
    "    q_a2 = q_W3 @ q_h1 + q_b3\n",
    "    q_s2 = np.exp(q_a2)\n",
    "    \n",
    "    return q_h1.T, q_a2.T, q_mu.T, q_s2.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit(cache=True)\n",
    "def decoder_forward_nb2(p_W4, p_W5, p_b4, p_b5, z):\n",
    "    \"\"\"\n",
    "    decoder forward propagation for each data point and each sample latent variable\n",
    "    z: dz by 1\n",
    "    \"\"\"\n",
    "    \n",
    "    p_a3 = p_W4 @ z + p_b4\n",
    "    p_h2 = tanh_nb(p_a3)\n",
    "            \n",
    "    p_a4 = p_W5 @ p_h2 + p_b5\n",
    "    y = sigmoid_nb(p_a4)\n",
    "    \n",
    "    return y.T, p_h2.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit(cache=True)\n",
    "def batch_forward_nb2(M, L, dm, dz, X, q_W1, q_W2, q_W3, p_W4, p_W5, q_b1, q_b2, q_b3, p_b4, p_b5, eps):\n",
    "    \"\"\"forward propagation for one batch\"\"\"\n",
    "    \n",
    "    # initialize variables for calculating gradients/ loss by batch\n",
    "    q_h1 = np.zeros((M, dm))\n",
    "    q_mu = np.zeros((M, dz))\n",
    "    q_s2 = np.zeros((M, dz))\n",
    "    q_a2 = np.zeros((M, dz))\n",
    "    p_h2 = np.zeros((M, L, dm))\n",
    "    z = np.zeros((M, L, dz))\n",
    "    y = np.zeros((M, L, dx))\n",
    "    \n",
    "    for iD in range(M):\n",
    "        q_h1[iD,], q_a2[iD,], q_mu[iD,], q_s2[iD,] =\\\n",
    "        encoder_forward_nb2(X[iD,].reshape(-1,1), q_W1, q_W2, q_W3, q_b1, q_b2, q_b3)\n",
    "\n",
    "        for iL in range(L):\n",
    "            z[iD,iL,] = sample_z_nb(q_mu[iD,], q_s2[iD,], eps[iL,])\n",
    "            y[iD,iL,], p_h2[iD,iL,] = decoder_forward_nb2(p_W4, p_W5, p_b4, p_b5, z[iD,iL,].reshape(-1,1))\n",
    "    \n",
    "    loss = total_loss_nb(X, y, q_a2, q_mu, q_s2)\n",
    "    \n",
    "    return y, q_h1, p_h2, q_mu, q_s2, z, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit(cache=True)\n",
    "def grad_nb2(X, y, q_W1, q_W2, q_W3, p_W4, p_W5, q_b1, q_b2, q_b3, p_b4, p_b5, q_h1, p_h2, q_mu, q_s2, z, eps):\n",
    "    \"\"\"\"\n",
    "    batch gradient\n",
    "    inputs:\n",
    "        X: M by dx\n",
    "        y: M by L by dx\n",
    "        W: weights\n",
    "        b: bias\n",
    "        H: q_h1 (M by dm), p_h2 (M by L by dm)\n",
    "        Lt: eps (L by dz), z (M by L by dz), q_s2 (M by dz), q_mu (M by dz)\n",
    "    \"\"\"\n",
    "    M = X.shape[0]\n",
    "    L = y.shape[1]\n",
    "    \n",
    "    # initialize gradient variables\n",
    "    \n",
    "    # L: loss; R: regularization; J: total target\n",
    "    dL_dW1 = dJ_dW1 = dR_dW1 = np.zeros_like(q_W1)\n",
    "    dL_db1 = dJ_db1 = dR_db1 = np.zeros_like(q_b1)\n",
    "    dL_dW2 = dJ_dW2 = dR_dW2 = np.zeros_like(q_W2)\n",
    "    dL_db2 = dJ_db2 = dR_db2 = np.zeros_like(q_b2)\n",
    "    dL_dW3 = dJ_dW3 = dR_dW3 = np.zeros_like(q_W3)\n",
    "    dL_db3 = dJ_db3 = dR_db3 = np.zeros_like(q_b3)\n",
    "    dL_dW4 = dJ_dW4 = np.zeros_like(p_W4)\n",
    "    dL_db4 = dJ_db4 = np.zeros_like(p_b4)    \n",
    "    dL_dW5 = dJ_dW5 = np.zeros_like(p_W5)\n",
    "    dL_db5 = dJ_db5 = np.zeros_like(p_b5)\n",
    "    \n",
    "    for iD in range(M):\n",
    "        for iL in range(L):\n",
    "            # back propagation for loss\n",
    "\n",
    "            #L_d4 = (np.divide(y[iD,iL,]-X[iD,], y[iD,iL,] * (1-y[iD,iL,])) * sigmoid_gradient(y[iD,iL,])).reshape(-1,1)\n",
    "            \n",
    "            # simplied L_d4\n",
    "            L_d4 = (y[iD,iL,]-X[iD,]).reshape(-1,1)\n",
    "            dL_dW5 = dL_dW5 + L_d4 @ p_h2[iD,iL,].reshape(1,-1)\n",
    "            dL_db5 = dL_db5 + L_d4\n",
    "            \n",
    "            L_d3 = p_W5.T @ L_d4 * tanh_gradient_nb(p_h2[iD,iL,]).reshape(-1,1)\n",
    "            dL_dW4 = dL_dW4 + L_d3 @ z[iD,iL,].reshape(1,-1)\n",
    "            dL_db4 = dL_db4 + L_d3\n",
    "            \n",
    "            L_d22 = p_W4.T @ L_d3 * eps[iL,].reshape(-1,1) * np.sqrt(q_s2[iD,]).reshape(-1,1) / 2\n",
    "            dL_dW3 = dL_dW3 + L_d22 @ q_h1[iD,].reshape(1,-1)\n",
    "            dL_db3 = dL_db3 + L_d22\n",
    "            \n",
    "            L_d21 = p_W4.T @ L_d3\n",
    "            dL_dW2 = dL_dW2 + L_d21 @ q_h1[iD,].reshape(1,-1)\n",
    "            dL_db2 = dL_db2 + L_d21\n",
    "            \n",
    "            L_d1 = (q_W2.T @ L_d21 + q_W3.T @ L_d22) * tanh_gradient_nb(q_h1[iD,]).reshape(-1,1)\n",
    "            dL_dW1 = dL_dW1 + L_d1 @ X[iD,].reshape(1,-1)\n",
    "            dL_db1 = dL_db1 + L_d1\n",
    "        \n",
    "        # back propagation for regularization\n",
    "        R_d22 = ((q_s2[iD,]-1)/2).reshape(-1,1)\n",
    "        dR_dW3 = dR_dW3 + R_d22 @ q_h1[iD,].reshape(1,-1)\n",
    "        dR_db3 = dR_db3 + R_d22\n",
    "\n",
    "        R_d21 = q_mu[iD,].reshape(-1,1)\n",
    "        dR_dW2 = dR_dW2 + R_d21 @ q_h1[iD,].reshape(1,-1)\n",
    "        dR_db2 = dR_db2 + R_d21\n",
    "\n",
    "        R_d1 = (q_W3.T @ R_d22 + q_W2.T @ R_d21) * tanh_gradient_nb(q_h1[iD,]).reshape(-1,1)\n",
    "        dR_dW1 = dR_dW1 + R_d1 @ X[iD,].reshape(1,-1)\n",
    "        dR_db1 = dR_db1 + R_d1\n",
    "    \n",
    "    dJ_dW1 = dL_dW1 / L / M + dR_dW1 / M\n",
    "    dJ_db1 = dL_db1 / L / M + dR_db1 / M\n",
    "    dJ_dW2 = dL_dW2 / L / M + dR_dW2 / M\n",
    "    dJ_db2 = dL_db2 / L / M + dR_db2 / M    \n",
    "    dJ_dW3 = dL_dW3 / L / M + dR_dW3 / M\n",
    "    dJ_db3 = dL_db3 / L / M + dR_db3 / M\n",
    "    dJ_dW4 = dL_dW4 / L / M\n",
    "    dJ_db4 = dL_db4 / L / M \n",
    "    dJ_dW5 = dL_dW5 / L / M\n",
    "    dJ_db5 = dL_db5 / L / M\n",
    "    \n",
    "    return dJ_dW1, dJ_dW2, dJ_dW3, dJ_dW4, dJ_dW5, dJ_db1, dJ_db2, dJ_db3, dJ_db4, dJ_db5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def train_ADAM_nb2(trainX, trainy, nBatch, M, L, std_const, dx, dm, dz, alpha, batch_forward, grad):\n",
    "    \"\"\"train model with ADAM\"\"\"\n",
    "\n",
    "    # parameters for ADAM algorithm\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    eps_stable = 1e-8\n",
    "    \n",
    "    # initiate parameters for ADAM\n",
    "    # need to use separate lines of codes otherwise they share the same reference\n",
    "    v_dW1, v_dW2, v_dW3, v_dW4, v_dW5, v_db1, v_db2, v_db3, v_db4, v_db5 = \\\n",
    "    init_random_nb2(dx, dm, dz, option = \"zeros\")\n",
    "    s_dW1, s_dW2, s_dW3, s_dW4, s_dW5, s_db1, s_db2, s_db3, s_db4, s_db5 = \\\n",
    "    init_random_nb2(dx, dm, dz, option = \"zeros\")\n",
    "    vc_dW1, vc_dW2, vc_dW3, vc_dW4, vc_dW5, vc_db1, vc_db2, vc_db3, vc_db4, vc_db5 = \\\n",
    "    init_random_nb2(dx, dm, dz, option = \"zeros\")\n",
    "    sc_dW1, sc_dW2, sc_dW3, sc_dW4, sc_dW5, sc_db1, sc_db2, sc_db3, sc_db4, sc_db5 = \\\n",
    "    init_random_nb2(dx, dm, dz, option = \"zeros\")\n",
    "    \n",
    "    # weights and bias initialization\n",
    "    W1, W2, W3, W4, W5, b1, b2, b3, b4, b5 = init_random_nb2(dx, dm, dz)\n",
    "\n",
    "    # loss\n",
    "    loss = np.zeros(nBatch)\n",
    "    \n",
    "    for iB in range(nBatch):\n",
    "        # sample a random batch\n",
    "        batchX, batchy = get_Batch_nb(M, trainX, trainy)\n",
    "        X = batchX.reshape(M, dx) / std_const\n",
    "\n",
    "        # sample random noise for latent variable, assuming each batch uses the same random noise for now\n",
    "        eps = np.random.randn(L, dz)\n",
    "\n",
    "        y, q_h1, p_h2, q_mu, q_s2, z, loss[iB] = \\\n",
    "        batch_forward(M, L, dm, dz, X, W1, W2, W3, W4, W5, b1, b2, b3, b4, b5, eps)\n",
    "\n",
    "        dW1, dW2, dW3, dW4, dW5, db1, db2, db3, db4, db5 = \\\n",
    "        grad(X, y, W1, W2, W3, W4, W5, b1, b2, b3, b4, b5, q_h1, p_h2, q_mu, q_s2, z, eps)\n",
    "        \n",
    "        # ADAM\n",
    "        v_dW1 = beta1*v_dW1 + (1-beta1)*dW1\n",
    "        v_dW2 = beta1*v_dW2 + (1-beta1)*dW2\n",
    "        v_dW3 = beta1*v_dW3 + (1-beta1)*dW3\n",
    "        v_dW4 = beta1*v_dW4 + (1-beta1)*dW4\n",
    "        v_dW5 = beta1*v_dW5 + (1-beta1)*dW5\n",
    "        \n",
    "        v_db1 = beta1*v_db1 + (1-beta1)*db1\n",
    "        v_db2 = beta1*v_db2 + (1-beta1)*db2\n",
    "        v_db3 = beta1*v_db3 + (1-beta1)*db3\n",
    "        v_db4 = beta1*v_db4 + (1-beta1)*db4\n",
    "        v_db5 = beta1*v_db5 + (1-beta1)*db5\n",
    "        \n",
    "        s_dW1 = beta2*s_dW1 + (1-beta2)*np.power(dW1,2)\n",
    "        s_dW2 = beta2*s_dW2 + (1-beta2)*np.power(dW2,2)\n",
    "        s_dW3 = beta2*s_dW3 + (1-beta2)*np.power(dW3,2)\n",
    "        s_dW4 = beta2*s_dW4 + (1-beta2)*np.power(dW4,2)\n",
    "        s_dW5 = beta2*s_dW5 + (1-beta2)*np.power(dW5,2)\n",
    "        \n",
    "        s_db1 = beta2*s_db1 + (1-beta2)*np.power(db1,2)\n",
    "        s_db2 = beta2*s_db2 + (1-beta2)*np.power(db2,2)\n",
    "        s_db3 = beta2*s_db3 + (1-beta2)*np.power(db3,2)\n",
    "        s_db4 = beta2*s_db4 + (1-beta2)*np.power(db4,2)\n",
    "        s_db5 = beta2*s_db5 + (1-beta2)*np.power(db5,2)\n",
    "        \n",
    "        vc_dW1 = v_dW1/(1-beta1**(iB+1))\n",
    "        vc_dW2 = v_dW2/(1-beta1**(iB+1))\n",
    "        vc_dW3 = v_dW3/(1-beta1**(iB+1))\n",
    "        vc_dW4 = v_dW4/(1-beta1**(iB+1))\n",
    "        vc_dW5 = v_dW5/(1-beta1**(iB+1))\n",
    "        \n",
    "        vc_db1 = v_db1/(1-beta1**(iB+1))\n",
    "        vc_db2 = v_db2/(1-beta1**(iB+1))\n",
    "        vc_db3 = v_db3/(1-beta1**(iB+1))\n",
    "        vc_db4 = v_db4/(1-beta1**(iB+1))\n",
    "        vc_db5 = v_db5/(1-beta1**(iB+1))\n",
    "        \n",
    "        sc_dW1 = s_dW1/(1-beta2**(iB+1))\n",
    "        sc_dW2 = s_dW2/(1-beta2**(iB+1))\n",
    "        sc_dW3 = s_dW3/(1-beta2**(iB+1))\n",
    "        sc_dW4 = s_dW4/(1-beta2**(iB+1))\n",
    "        sc_dW5 = s_dW5/(1-beta2**(iB+1))\n",
    "        \n",
    "        sc_db1 = s_db1/(1-beta2**(iB+1))\n",
    "        sc_db2 = s_db2/(1-beta2**(iB+1))\n",
    "        sc_db3 = s_db3/(1-beta2**(iB+1))\n",
    "        sc_db4 = s_db4/(1-beta2**(iB+1))\n",
    "        sc_db5 = s_db5/(1-beta2**(iB+1))\n",
    "        \n",
    "        dW1 = vc_dW1 / (np.sqrt(sc_dW1) + eps_stable)\n",
    "        dW2 = vc_dW2 / (np.sqrt(sc_dW2) + eps_stable)\n",
    "        dW3 = vc_dW3 / (np.sqrt(sc_dW3) + eps_stable)\n",
    "        dW4 = vc_dW4 / (np.sqrt(sc_dW4) + eps_stable)\n",
    "        dW5 = vc_dW5 / (np.sqrt(sc_dW5) + eps_stable)\n",
    "        \n",
    "        db1 = vc_db1 / (np.sqrt(sc_db1) + eps_stable)\n",
    "        db2 = vc_db2 / (np.sqrt(sc_db2) + eps_stable)\n",
    "        db3 = vc_db3 / (np.sqrt(sc_db3) + eps_stable)\n",
    "        db4 = vc_db4 / (np.sqrt(sc_db4) + eps_stable)\n",
    "        db5 = vc_db5 / (np.sqrt(sc_db5) + eps_stable)\n",
    "        \n",
    "        W1 = W1 - alpha * dW1\n",
    "        W2 = W2 - alpha * dW2\n",
    "        W3 = W3 - alpha * dW3\n",
    "        W4 = W4 - alpha * dW4\n",
    "        W5 = W5 - alpha * dW5\n",
    "        \n",
    "        b1 = b1 - alpha * db1\n",
    "        b2 = b2 - alpha * db2\n",
    "        b3 = b3 - alpha * db3\n",
    "        b4 = b4 - alpha * db4\n",
    "        b5 = b5 - alpha * db5\n",
    "\n",
    "    return W1, W2, W3, W4, W5, b1, b2, b3, b4, b5, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ADAM_v2(trainX, trainy, Spec, batch_forward, grad, W = \"\", b = \"\", doc = \"\"):\n",
    "    \"\"\"train model with ADAM\"\"\"\n",
    "    \n",
    "    nBatch, M, L, std_const, dx, dm, dz, alpha, nP = Spec\n",
    "    \n",
    "    if doc != \"\":\n",
    "        status_file = get_filename(\"status\")\n",
    "        para_file = get_filename(\"parameter\")\n",
    "        update_status(status_file, \"x\", \"Training starts: \" + get_timestamp() + \"\\n\")\n",
    "    \n",
    "    # parameters for ADAM algorithm\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    eps_stable = 1e-8\n",
    "    \n",
    "    # initiate parameters for ADAM\n",
    "    # need to use separate lines of codes otherwise they share the same reference\n",
    "    v_dW, v_db = init_random(dx, dm, dz, option = \"zeros\")\n",
    "    s_dW, s_db = init_random(dx, dm, dz, option = \"zeros\")\n",
    "    vc_dW, vc_db = init_random(dx, dm, dz, option = \"zeros\")\n",
    "    sc_dW, sc_db = init_random(dx, dm, dz, option = \"zeros\")\n",
    "    num_para = len(v_dW)\n",
    "    \n",
    "    # weights and bias initialization\n",
    "    if len(W) == len(b) == 0:\n",
    "        W, b = init_random(dx, dm, dz)\n",
    "        q_W1, q_W2, q_W3, p_W4, p_W5 = W\n",
    "        q_b1, q_b2, q_b3, p_b4, p_b5 = b\n",
    "\n",
    "    # loss\n",
    "    loss = np.zeros(nBatch)\n",
    "    \n",
    "    for iB in range(nBatch):\n",
    "        # sample a random batch\n",
    "        batchX, batchy = get_Batch_nb(M, trainX, trainy)\n",
    "        X = batchX.reshape(M, dx) / std_const\n",
    "\n",
    "        # sample random noise for latent variable, assuming each batch uses the same random noise for now\n",
    "        eps = np.random.randn(L, dz)\n",
    "\n",
    "        y, q_h1, p_h2, q_mu, q_s2, z, loss[iB] = \\\n",
    "        batch_forward(M, L, dm, dz, X, q_W1, q_W2, q_W3, p_W4, p_W5, q_b1, q_b2, q_b3, p_b4, p_b5, eps)\n",
    "        dW1, dW2, dW3, dW4, dW5, db1, db2, db3, db4, db5 = \\\n",
    "        grad(X, y, q_W1, q_W2, q_W3, p_W4, p_W5, q_b1, q_b2, q_b3, p_b4, p_b5, q_h1, p_h2, q_mu, q_s2, z, eps)\n",
    "        \n",
    "        dW = [dW1, dW2, dW3, dW4, dW5]\n",
    "        db = [db1, db2, db3, db4, db5]\n",
    "        \n",
    "        # ADAM\n",
    "        for i in range(num_para):\n",
    "            v_dW[i] = beta1*v_dW[i] + (1-beta1)*dW[i]\n",
    "            v_db[i] = beta1*v_db[i] + (1-beta1)*db[i]\n",
    "            s_dW[i] = beta2*s_dW[i] + (1-beta2)*np.power(dW[i],2)\n",
    "            s_db[i] = beta2*s_db[i] + (1-beta2)*np.power(db[i],2)\n",
    "        \n",
    "            vc_dW[i] = v_dW[i]/(1-beta1**(iB+1))\n",
    "            vc_db[i] = v_db[i]/(1-beta1**(iB+1))\n",
    "            sc_dW[i] = s_dW[i]/(1-beta2**(iB+1))\n",
    "            sc_db[i] = s_db[i]/(1-beta2**(iB+1))\n",
    "        \n",
    "            dW[i] = vc_dW[i] / (np.sqrt(sc_dW[i]) + eps_stable)\n",
    "            db[i] = vc_db[i] / (np.sqrt(sc_db[i]) + eps_stable)\n",
    "        \n",
    "        W, b = para_update(W, b, dW, db, alpha)\n",
    "        \n",
    "        if doc != \"\":\n",
    "            if (iB+1) % nP == 0:\n",
    "                # append status file\n",
    "                update_status(status_file, \"a\", \"Batch \" + str(iB+1) + \" : \" + get_timestamp() + \"\\n\")\n",
    "    \n",
    "    # save parameter to file\n",
    "    if doc != \"\":\n",
    "        save_para(para_file, doc = doc, Spec = Spec, W = W, b = b, loss = loss)\n",
    "\n",
    "    return Spec, W, b, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_filename = get_filename(\"profiling_v4\", ext = \".prof\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "*** Profile stats marshalled to file '2020_04_29_12_40_54_Model_v1.2_profiling_v4.prof'. \n"
     ]
    }
   ],
   "source": [
    "%prun -q -D $profile_filename train_ADAM_nb2(\\\n",
    "            trainX, trainy, 100, M, L, std_const, dx, dm, dz, alpha, batch_forward_nb2, grad_nb2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Apr 29 16:43:35 2020    2020_04_29_12_40_54_Model_v1.2_profiling_v4.prof\n",
      "\n",
      "         12881649 function calls (12080480 primitive calls) in 122.129 seconds\n",
      "\n",
      "   Ordered by: internal time, cumulative time\n",
      "   List reduced from 2234 to 5 due to restriction <5>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1   65.758   65.758   65.758   65.758 <ipython-input-178-3f9731fa5ee6>:1(train_ADAM_nb)\n",
      "24186/24185   32.793    0.001   32.902    0.001 /opt/conda/lib/python3.6/site-packages/llvmlite/binding/ffi.py:112(__call__)\n",
      "  2149003    1.379    0.000    1.753    0.000 {built-in method builtins.isinstance}\n",
      "271606/53787    1.025    0.000    1.680    0.000 /opt/conda/lib/python3.6/site-packages/numba/ir.py:313(_rec_list_vars)\n",
      "449073/193240    0.736    0.000    3.443    0.000 {method 'format' of 'str' objects}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p = pstats.Stats(profile_filename)\n",
    "p.sort_stats('time', 'cumulative').print_stats(5)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. parallelization based vectorized version with numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_random_nb1 = jit(init_random, nopython=True)\n",
    "para_update_nb = jit(para_update, nopython=True, cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(cache=True)\n",
    "def batch_forward_vec_nb2(Spec, X, W, b, eps):\n",
    "    \"\"\"forward propagation for one mini batch - full vectorized version\"\"\"\n",
    "    d, M, L, d, dx, dm, dz, d, d = Spec\n",
    "\n",
    "    p_h2 = np.zeros((M, L, dm))\n",
    "    z = np.zeros((M, L, dz))\n",
    "    y = np.zeros((M, L, dx))\n",
    "    \n",
    "    q_h1, q_a2, q_mu, q_s2 = encoder_forward_vec_nb1(X, W, b)\n",
    "    z = sample_z_vec_nb(q_mu, q_s2, eps)\n",
    "    y, p_h2 = decoder_forward_vec_nb1(W, b, z)\n",
    "    \n",
    "    loss = total_loss_vec_nb(X, y, q_a2, q_mu, q_s2)\n",
    "\n",
    "    return y, q_h1, p_h2, q_mu, q_s2, z, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def grad_vec2_nb2(X, y, q_W1, q_W2, q_W3, p_W4, p_W5, q_b1, q_b2, q_b3, p_b4, p_b5, q_h1, p_h2, q_mu, q_s2, z, eps):\n",
    "    \"\"\"\"\n",
    "    batch gradient calculation - vectorized version\n",
    "    not using lists for parameters to enable numba nopython\n",
    "    \n",
    "    inputs:\n",
    "        X: Data [M by dx]\n",
    "        y: Model results [M by L by dx]\n",
    "        q_W1, q_W2, q_W3: Weights for encoder\n",
    "        p_W4, p_W5: Weights for decoder\n",
    "        q_b1, q_b2, q_b3: Bias for encoder\n",
    "        p_b4, p_b5: Bias for decoder\n",
    "        q_h1 (M by dm), p_h2 (M by L by dm): intermediate activation variables\n",
    "        eps (L by dz), z (M by L by dz), q_s2 (M by dz), q_mu (M by dz): for sampling latent variables from posterior\n",
    "    \"\"\"\n",
    "    \n",
    "    M = X.shape[0]\n",
    "    L = y.shape[1]\n",
    "    \n",
    "    # initialize gradient variables\n",
    "    \n",
    "    # L: loss; R: regularization; J: total target\n",
    "    dL_dW1 = dJ_dW1 = dR_dW1 = np.zeros_like(q_W1)\n",
    "    dL_db1 = dJ_db1 = dR_db1 = np.zeros_like(q_b1)\n",
    "    dL_dW2 = dJ_dW2 = dR_dW2 = np.zeros_like(q_W2)\n",
    "    dL_db2 = dJ_db2 = dR_db2 = np.zeros_like(q_b2)\n",
    "    dL_dW3 = dJ_dW3 = dR_dW3 = np.zeros_like(q_W3)\n",
    "    dL_db3 = dJ_db3 = dR_db3 = np.zeros_like(q_b3)\n",
    "    dL_dW4 = dJ_dW4 = np.zeros_like(p_W4)\n",
    "    dL_db4 = dJ_db4 = np.zeros_like(p_b4)    \n",
    "    dL_dW5 = dJ_dW5 = np.zeros_like(p_W5)\n",
    "    dL_db5 = dJ_db5 = np.zeros_like(p_b5)\n",
    "\n",
    "    # back propagation for loss\n",
    "    for iL in range(L):\n",
    "        y_iL = y[:,iL,:] # M by dx\n",
    "        p_h2_iL = p_h2[:,iL,:] # M by dm\n",
    "        z_iL = z[:,iL,:] # M by dz\n",
    "        \n",
    "        L_d4 = y_iL - X # M by dx\n",
    "        dL_dW5 = dL_dW5 + L_d4.T @ p_h2_iL # dx by dm\n",
    "        dL_db5 = dL_db5 + np.sum(L_d4, axis = 0).reshape(-1,1) # dx by 1\n",
    "        \n",
    "        L_d3 = L_d4 @ p_W5 * tanh_gradient_nb(p_h2_iL) # M by dm\n",
    "        dL_dW4 = dL_dW4 + L_d3.T @ z_iL # dm by dz\n",
    "        dL_db4 = dL_db4 + np.sum(L_d3, axis = 0).reshape(-1,1) # dm by 1\n",
    "        \n",
    "        L_d22 = L_d3 @ p_W4 * eps[iL,] * np.sqrt(q_s2) / 2  # M by dz\n",
    "        dL_dW3 = dL_dW3 + L_d22.T @ q_h1 # dz by dm\n",
    "        dL_db3 = dL_db3 + np.sum(L_d22, axis = 0).reshape(-1,1) # dz by 1\n",
    "        \n",
    "        L_d21 = L_d3 @ p_W4 # M by dz\n",
    "        dL_dW2 = dL_dW2 + L_d21.T @ q_h1 # dz by dm\n",
    "        dL_db2 = dL_db2 + np.sum(L_d21, axis = 0).reshape(-1,1) # dz by 1\n",
    "\n",
    "        L_d1 = (L_d21 @ q_W2 + L_d22 @ q_W3) * tanh_gradient_nb(q_h1) # M by dm\n",
    "        dL_dW1 = dL_dW1 + L_d1.T @ X # dm by dx\n",
    "        dL_db1 = dL_db1 + np.sum(L_d1, axis = 0).reshape(-1,1) # dm by 1\n",
    "\n",
    "    # back propagation for regularization\n",
    "    R_d22 = (q_s2 - 1)/2 # M by dz\n",
    "    dR_dW3 = dR_dW3 + R_d22.T @ q_h1 # dz by dm\n",
    "    dR_db3 = dR_db3 + np.sum(R_d22, axis = 0).reshape(-1,1) # dz by 1\n",
    "    \n",
    "    R_d21 = q_mu # M by dz\n",
    "    dR_dW2 = dR_dW2 + R_d21.T @ q_h1 # dz by dm\n",
    "    dR_db2 = dR_db2 + np.sum(R_d21, axis = 0).reshape(-1,1) # dm by 1\n",
    "\n",
    "    R_d1 = (R_d22 @ q_W3 + R_d21 @ q_W2) * tanh_gradient_nb(q_h1) # M by dm\n",
    "    dR_dW1 = dR_dW1 + R_d1.T @ X # dm by dx\n",
    "    dR_db1 = dR_db1 + np.sum(R_d1, axis = 0).reshape(-1,1) # dm by 1\n",
    "    \n",
    "    dJ_dW1 = dL_dW1 / L / M + dR_dW1 / M\n",
    "    dJ_db1 = dL_db1 / L / M + dR_db1 / M\n",
    "    dJ_dW2 = dL_dW2 / L / M + dR_dW2 / M\n",
    "    dJ_db2 = dL_db2 / L / M + dR_db2 / M    \n",
    "    dJ_dW3 = dL_dW3 / L / M + dR_dW3 / M\n",
    "    dJ_db3 = dL_db3 / L / M + dR_db3 / M\n",
    "    dJ_dW4 = dL_dW4 / L / M\n",
    "    dJ_db4 = dL_db4 / L / M \n",
    "    dJ_dW5 = dL_dW5 / L / M\n",
    "    dJ_db5 = dL_db5 / L / M\n",
    "    \n",
    "    return dJ_dW1, dJ_dW2, dJ_dW3, dJ_dW4, dJ_dW5, dJ_db1, dJ_db2, dJ_db3, dJ_db4, dJ_db5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(parallel=True)\n",
    "def train_ADAM_nb1_pl(trainX, trainy, Spec, batch_forward, grad, W = \"\", b = \"\"):\n",
    "    \"\"\"train model with ADAM\"\"\"\n",
    "    \n",
    "    nBatch, M, L, std_const, dx, dm, dz, alpha, nP = Spec\n",
    "    \n",
    "    # parameters for ADAM algorithm\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    eps_stable = 1e-8\n",
    "    \n",
    "    # initiate parameters for ADAM\n",
    "    # need to use separate lines of codes otherwise they share the same reference\n",
    "    v_dW, v_db = init_random_nb1(dx, dm, dz, option = \"zeros\")\n",
    "    s_dW, s_db = init_random_nb1(dx, dm, dz, option = \"zeros\")\n",
    "    vc_dW, vc_db = init_random_nb1(dx, dm, dz, option = \"zeros\")\n",
    "    sc_dW, sc_db = init_random_nb1(dx, dm, dz, option = \"zeros\")\n",
    "    num_para = len(v_dW)\n",
    "    \n",
    "    # weights and bias initialization\n",
    "    if len(W) == len(b) == 0:\n",
    "        W, b = init_random_nb1(dx, dm, dz)\n",
    "    \n",
    "    q_W1, q_W2, q_W3, p_W4, p_W5 = W\n",
    "    q_b1, q_b2, q_b3, p_b4, p_b5 = b\n",
    "\n",
    "    # loss\n",
    "    loss = np.zeros(nBatch)\n",
    "    \n",
    "    for iB in prange(nBatch):\n",
    "        # sample a random batch\n",
    "        batchX, batchy = get_Batch_nb(M, trainX, trainy)\n",
    "        X = batchX.reshape(M, dx) / std_const\n",
    "\n",
    "        # sample random noise for latent variable, assuming each batch uses the same random noise for now\n",
    "        eps = np.random.randn(L, dz)\n",
    "\n",
    "        y, q_h1, p_h2, q_mu, q_s2, z, loss[iB] = batch_forward(Spec, X, W, b, eps)\n",
    "        \n",
    "        dJ_dW1, dJ_dW2, dJ_dW3, dJ_dW4, dJ_dW5, dJ_db1, dJ_db2, dJ_db3, dJ_db4, dJ_db5 =\\\n",
    "        grad(X, y, q_W1, q_W2, q_W3, p_W4, p_W5, q_b1, q_b2, q_b3, p_b4, p_b5, q_h1, p_h2, q_mu, q_s2, z, eps)\n",
    "        \n",
    "        dW = [dJ_dW1, dJ_dW2, dJ_dW3, dJ_dW4, dJ_dW5]\n",
    "        db = [dJ_db1, dJ_db2, dJ_db3, dJ_db4, dJ_db5]\n",
    "        \n",
    "        # ADAM\n",
    "        for i in range(num_para):\n",
    "            v_dW[i] = beta1*v_dW[i] + (1-beta1)*dW[i]\n",
    "            v_db[i] = beta1*v_db[i] + (1-beta1)*db[i]\n",
    "            s_dW[i] = beta2*s_dW[i] + (1-beta2)*np.power(dW[i],2)\n",
    "            s_db[i] = beta2*s_db[i] + (1-beta2)*np.power(db[i],2)\n",
    "        \n",
    "            vc_dW[i] = v_dW[i]/(1-beta1**(iB+1))\n",
    "            vc_db[i] = v_db[i]/(1-beta1**(iB+1))\n",
    "            sc_dW[i] = s_dW[i]/(1-beta2**(iB+1))\n",
    "            sc_db[i] = s_db[i]/(1-beta2**(iB+1))\n",
    "        \n",
    "            dW[i] = vc_dW[i] / (np.sqrt(sc_dW[i]) + eps_stable)\n",
    "            db[i] = vc_db[i] / (np.sqrt(sc_db[i]) + eps_stable)\n",
    "        \n",
    "        W, b = para_update_nb(W, b, dW, db, alpha)\n",
    "\n",
    "    return Spec, W, b, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_filename = get_filename(\"profiling_v5\", ext = \".prof\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-257-ec939d784413>:1: NumbaWarning: \n",
      "Compilation is falling back to object mode WITH looplifting enabled because Function \"train_ADAM_nb1_pl\" failed type inference due to: Cannot unify unicode_type and list(array(float64, 2d, C)) for 'W', defined at <ipython-input-257-ec939d784413> (5)\n",
      "\n",
      "File \"<ipython-input-257-ec939d784413>\", line 5:\n",
      "def train_ADAM_nb1_pl(trainX, trainy, Spec, batch_forward, grad, W = \"\", b = \"\"):\n",
      "    <source elided>\n",
      "    \n",
      "    nBatch, M, L, std_const, dx, dm, dz, alpha, nP = Spec\n",
      "    ^\n",
      "\n",
      "[1] During: typing of assignment at <ipython-input-257-ec939d784413> (22)\n",
      "\n",
      "File \"<ipython-input-257-ec939d784413>\", line 22:\n",
      "def train_ADAM_nb1_pl(trainX, trainy, Spec, batch_forward, grad, W = \"\", b = \"\"):\n",
      "    <source elided>\n",
      "    if len(W) == len(b) == 0:\n",
      "        W, b = init_random_nb1(dx, dm, dz)\n",
      "        ^\n",
      "\n",
      "  @jit(parallel=True)\n",
      "<ipython-input-257-ec939d784413>:1: NumbaWarning: \n",
      "Compilation is falling back to object mode WITHOUT looplifting enabled because Function \"train_ADAM_nb1_pl\" failed type inference due to: cannot determine Numba type of <class 'numba.dispatcher.LiftedLoop'>\n",
      "\n",
      "File \"<ipython-input-257-ec939d784413>\", line 30:\n",
      "def train_ADAM_nb1_pl(trainX, trainy, Spec, batch_forward, grad, W = \"\", b = \"\"):\n",
      "    <source elided>\n",
      "    \n",
      "    for iB in prange(nBatch):\n",
      "    ^\n",
      "\n",
      "  @jit(parallel=True)\n",
      "/opt/conda/lib/python3.6/site-packages/numba/object_mode_passes.py:178: NumbaWarning: Function \"train_ADAM_nb1_pl\" was compiled in object mode without forceobj=True, but has lifted loops.\n",
      "\n",
      "File \"<ipython-input-257-ec939d784413>\", line 5:\n",
      "def train_ADAM_nb1_pl(trainX, trainy, Spec, batch_forward, grad, W = \"\", b = \"\"):\n",
      "    <source elided>\n",
      "    \n",
      "    nBatch, M, L, std_const, dx, dm, dz, alpha, nP = Spec\n",
      "    ^\n",
      "\n",
      "  state.func_ir.loc))\n",
      "/opt/conda/lib/python3.6/site-packages/numba/object_mode_passes.py:188: NumbaDeprecationWarning: \n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\n",
      "\n",
      "For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\n",
      "File \"<ipython-input-257-ec939d784413>\", line 5:\n",
      "def train_ADAM_nb1_pl(trainX, trainy, Spec, batch_forward, grad, W = \"\", b = \"\"):\n",
      "    <source elided>\n",
      "    \n",
      "    nBatch, M, L, std_const, dx, dm, dz, alpha, nP = Spec\n",
      "    ^\n",
      "\n",
      "  state.func_ir.loc))\n",
      "<ipython-input-257-ec939d784413>:1: NumbaWarning: \n",
      "Compilation is falling back to object mode WITHOUT looplifting enabled because Function \"train_ADAM_nb1_pl\" failed type inference due to: Invalid use of type(CPUDispatcher(<function batch_forward_vec_nb2 at 0x7fad7e0e79d8>)) with parameters (reflected list(int64), array(float64, 2d, C), reflected list(array(float64, 2d, C)), reflected list(array(float64, 2d, C)), array(float64, 2d, C))\n",
      " * parameterized\n",
      "[1] During: resolving callee type: type(CPUDispatcher(<function batch_forward_vec_nb2 at 0x7fad7e0e79d8>))\n",
      "[2] During: typing of call at <ipython-input-257-ec939d784413> (38)\n",
      "\n",
      "\n",
      "File \"<ipython-input-257-ec939d784413>\", line 38:\n",
      "def train_ADAM_nb1_pl(trainX, trainy, Spec, batch_forward, grad, W = \"\", b = \"\"):\n",
      "    <source elided>\n",
      "\n",
      "        y, q_h1, p_h2, q_mu, q_s2, z, loss[iB] = batch_forward(Spec, X, W, b, eps)\n",
      "        ^\n",
      "\n",
      "  @jit(parallel=True)\n",
      "/opt/conda/lib/python3.6/site-packages/numba/object_mode_passes.py:178: NumbaWarning: Function \"train_ADAM_nb1_pl\" was compiled in object mode without forceobj=True.\n",
      "\n",
      "File \"<ipython-input-257-ec939d784413>\", line 30:\n",
      "def train_ADAM_nb1_pl(trainX, trainy, Spec, batch_forward, grad, W = \"\", b = \"\"):\n",
      "    <source elided>\n",
      "    \n",
      "    for iB in prange(nBatch):\n",
      "    ^\n",
      "\n",
      "  state.func_ir.loc))\n",
      "/opt/conda/lib/python3.6/site-packages/numba/object_mode_passes.py:188: NumbaDeprecationWarning: \n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\n",
      "\n",
      "For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\n",
      "File \"<ipython-input-257-ec939d784413>\", line 30:\n",
      "def train_ADAM_nb1_pl(trainX, trainy, Spec, batch_forward, grad, W = \"\", b = \"\"):\n",
      "    <source elided>\n",
      "    \n",
      "    for iB in prange(nBatch):\n",
      "    ^\n",
      "\n",
      "  state.func_ir.loc))\n",
      "<ipython-input-256-b95d3c7e2b3a>:42: NumbaPerformanceWarning: '@' is faster on contiguous arrays, called on (array(float64, 2d, F), array(float64, 2d, A))\n",
      "  dL_dW5 = dL_dW5 + L_d4.T @ p_h2_iL # dx by dm\n",
      "<ipython-input-256-b95d3c7e2b3a>:46: NumbaPerformanceWarning: '@' is faster on contiguous arrays, called on (array(float64, 2d, F), array(float64, 2d, A))\n",
      "  dL_dW4 = dL_dW4 + L_d3.T @ z_iL # dm by dz\n",
      "/opt/conda/lib/python3.6/site-packages/numba/typing/npydecl.py:958: NumbaPerformanceWarning: '@' is faster on contiguous arrays, called on (array(float64, 2d, F), array(float64, 2d, A))\n",
      "  warnings.warn(NumbaPerformanceWarning(msg))\n",
      "/opt/conda/lib/python3.6/site-packages/numba/typing/npydecl.py:958: NumbaPerformanceWarning: '@' is faster on contiguous arrays, called on (array(float64, 2d, F), array(float64, 2d, A))\n",
      "  warnings.warn(NumbaPerformanceWarning(msg))\n",
      "/opt/conda/lib/python3.6/site-packages/numba/ir_utils.py:2041: NumbaPendingDeprecationWarning: \n",
      "Encountered the use of a type that is scheduled for deprecation: type 'reflected list' found for argument 'W' of function 'para_update'.\n",
      "\n",
      "For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-reflection-for-list-and-set-types\n",
      "\n",
      "File \"<ipython-input-212-1aebc6f77775>\", line 1:\n",
      "def para_update(W, b, dW, db, alpha):\n",
      "^\n",
      "\n",
      "  warnings.warn(NumbaPendingDeprecationWarning(msg, loc=loc))\n",
      "/opt/conda/lib/python3.6/site-packages/numba/ir_utils.py:2041: NumbaPendingDeprecationWarning: \n",
      "Encountered the use of a type that is scheduled for deprecation: type 'reflected list' found for argument 'b' of function 'para_update'.\n",
      "\n",
      "For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-reflection-for-list-and-set-types\n",
      "\n",
      "File \"<ipython-input-212-1aebc6f77775>\", line 1:\n",
      "def para_update(W, b, dW, db, alpha):\n",
      "^\n",
      "\n",
      "  warnings.warn(NumbaPendingDeprecationWarning(msg, loc=loc))\n",
      "/opt/conda/lib/python3.6/site-packages/numba/ir_utils.py:2041: NumbaPendingDeprecationWarning: \n",
      "Encountered the use of a type that is scheduled for deprecation: type 'reflected list' found for argument 'dW' of function 'para_update'.\n",
      "\n",
      "For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-reflection-for-list-and-set-types\n",
      "\n",
      "File \"<ipython-input-212-1aebc6f77775>\", line 1:\n",
      "def para_update(W, b, dW, db, alpha):\n",
      "^\n",
      "\n",
      "  warnings.warn(NumbaPendingDeprecationWarning(msg, loc=loc))\n",
      "/opt/conda/lib/python3.6/site-packages/numba/ir_utils.py:2041: NumbaPendingDeprecationWarning: \n",
      "Encountered the use of a type that is scheduled for deprecation: type 'reflected list' found for argument 'db' of function 'para_update'.\n",
      "\n",
      "For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-reflection-for-list-and-set-types\n",
      "\n",
      "File \"<ipython-input-212-1aebc6f77775>\", line 1:\n",
      "def para_update(W, b, dW, db, alpha):\n",
      "^\n",
      "\n",
      "  warnings.warn(NumbaPendingDeprecationWarning(msg, loc=loc))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "*** Profile stats marshalled to file '2020_04_29_22_30_39_Model_v1.2_profiling_v5.prof'. \n"
     ]
    }
   ],
   "source": [
    "%prun -q -D $profile_filename train_ADAM_nb1_pl(\\\n",
    "            trainX, trainy, Spec, batch_forward_vec_nb2, grad_vec2_nb2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Apr 29 20:31:28 2020    2020_04_29_16_30_35_Model_v1.2_profiling_v5.prof\n",
      "\n",
      "         21454369 function calls (19958106 primitive calls) in 52.514 seconds\n",
      "\n",
      "   Ordered by: internal time, cumulative time\n",
      "   List reduced from 2492 to 5 due to restriction <5>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "23511/23510   13.747    0.001   13.883    0.001 /opt/conda/lib/python3.6/site-packages/llvmlite/binding/ffi.py:112(__call__)\n",
      "  4187682    2.536    0.000    2.863    0.000 {built-in method builtins.isinstance}\n",
      "   198384    2.095    0.000    4.994    0.000 /opt/conda/lib/python3.6/site-packages/llvmlite/ir/instructions.py:14(__init__)\n",
      "1045450/436519    1.782    0.000    8.994    0.000 {method 'format' of 'str' objects}\n",
      "400284/82174    1.465    0.000    2.416    0.000 /opt/conda/lib/python3.6/site-packages/numba/ir.py:313(_rec_list_vars)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p = pstats.Stats(profile_filename)\n",
    "p.sort_stats('time', 'cumulative').print_stats(5)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timing of all versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Total training time on 10 batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "Spec = [10, M, 1, std_const, dx, dm, dz, alpha, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.18 s ± 106 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit train_ADAM(trainX, trainy, Spec, batch_forward, grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.74 s ± 140 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit train_ADAM_nb1(trainX, trainy, Spec, batch_forward_nb1, grad_nb1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.96 s ± 141 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit train_ADAM_nb1(trainX, trainy, Spec, batch_forward_vec, grad_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "612 ms ± 22.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit train_ADAM_nb1(trainX, trainy, Spec, batch_forward_vec, grad_vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184 ms ± 3.56 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit train_ADAM_nb1(trainX, trainy, Spec, batch_forward_vec_nb1, grad_vec2_nb1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.4 s ± 130 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit train_ADAM_nb2(trainX, trainy, 10, M, L, std_const, dx, dm, dz, alpha, batch_forward_nb2, grad_nb2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.82 s ± 181 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit train_ADAM_v2(trainX, trainy, Spec, batch_forward_nb2, grad_nb2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179 ms ± 5.13 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit train_ADAM_nb1_pl(trainX, trainy, Spec, batch_forward_vec_nb1, grad_vec2_nb1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146 ms ± 3.7 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit train_ADAM_nb1_pl(trainX, trainy, Spec, batch_forward_vec_nb2, grad_vec2_nb2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Gradient function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 0.00001\n",
    "nBatch = 1 # number of mini-batch to train\n",
    "M = 100 # batch size\n",
    "L = 5 # sample size\n",
    "std_const = 255 # to standardize data\n",
    "Xdim1, Xdim2 = trainX[0].shape[0], trainX[0].shape[1]\n",
    "dx = Xdim1 * Xdim2 # dimension of the input\n",
    "dm = 500 # dimension of the hidden layer\n",
    "dz = 3 # dimension of latent variable\n",
    "alpha = 0.005 # learning rate\n",
    "nP = 1000 # print out status every nP batches\n",
    "\n",
    "Spec = [nBatch, M, L, std_const, dx, dm, dz, alpha, nP]\n",
    "\n",
    "W,b = init_random(dx, dm, dz)\n",
    "eps = np.random.randn(L, dz)\n",
    "batchX, batchy = get_Batch(M, trainX, trainy)\n",
    "X = batchX.reshape(M, dx) / std_const\n",
    "\n",
    "y, H, Lt, loss = batch_forward(Spec, X, W, b, eps)\n",
    "q_W1, q_W2, q_W3, p_W4, p_W5 = W\n",
    "q_b1, q_b2, q_b3, p_b4, p_b5 = b\n",
    "q_h1, p_h2 = H\n",
    "q_mu, q_s2, z, eps = Lt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.66 s ± 89.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "1.34 s ± 93.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "496 ms ± 58.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "37.6 ms ± 1.31 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "37.3 ms ± 2.27 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "1.25 s ± 86.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit grad(X, y, W, b, H, Lt)\n",
    "%timeit grad_nb1(X, y, W, b, H, Lt)\n",
    "%timeit grad_vec(X, y, W, b, H, Lt)\n",
    "%timeit grad_vec2(X, y, W, b, H, Lt)\n",
    "%timeit grad_vec2_nb1(X, y, W, b, H, Lt)\n",
    "%timeit grad_nb2(X, y, q_W1, q_W2, q_W3, p_W4, p_W5, q_b1, q_b2, q_b3, p_b4, p_b5, q_h1, p_h2, q_mu, q_s2, z, eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't unbox heterogeneous list: array(float64, 2d, C) != array(float64, 3d, C)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-846d494a90b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'timeit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'grad_vec_nb1(X, y, W, b, H, Lt)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2093\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2094\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2095\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2096\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m</opt/conda/lib/python3.6/site-packages/decorator.py:decorator-gen-61>\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1096\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m                 \u001b[0mnumber\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m                 \u001b[0mtime_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtime_number\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mtiming\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgcold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<magic-timeit>\u001b[0m in \u001b[0;36minner\u001b[0;34m(_it, _timer)\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't unbox heterogeneous list: array(float64, 2d, C) != array(float64, 3d, C)"
     ]
    }
   ],
   "source": [
    "%timeit grad_vec_nb1(X, y, W, b, H, Lt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Gradient checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 0.00001\n",
    "nBatch = 1 # number of mini-batch to train\n",
    "M = 100 # batch size\n",
    "L = 2 # sample size\n",
    "std_const = 255 # to standardize data\n",
    "\n",
    "Xdim1, Xdim2 = trainX[0].shape[0], trainX[0].shape[1]\n",
    "dx = Xdim1 * Xdim2 # dimension of the input\n",
    "dm = 500 # dimension of the hidden layer\n",
    "dz = 3 # dimension of latent variable\n",
    "\n",
    "alpha = 0.005 # learning rate\n",
    "\n",
    "nP = 1000 # print out status every nP batches\n",
    "\n",
    "Spec = [nBatch, M, L, std_const, dx, dm, dz, alpha, nP]\n",
    "\n",
    "W,b = init_random(dx, dm, dz)\n",
    "q_W1, q_W2, q_W3, p_W4, p_W5 = W\n",
    "q_b1, q_b2, q_b3, p_b4, p_b5 = b\n",
    "eps = np.random.randn(L, dz)\n",
    "batchX, batchy = get_Batch(M, trainX, trainy)\n",
    "X = batchX.reshape(M, dx) / std_const"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, H, Lt, loss1 = batch_forward(Spec, X, W, b, eps)\n",
    "q_h1, p_h2 = H\n",
    "q_mu, q_s2, z, eps = Lt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient checking for \"listed\" versions of the \"grad\" functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dW, db = grad_2(X, y, W, b, H, Lt)\n",
    "dL_dW1, dL_dW2, dL_dW3, dL_dW4, dL_dW5 = dW\n",
    "dL_db1, dL_db2, dL_db3, dL_db4, dL_db5 = db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient checking for \"unlisted\" versions of the \"grad\" functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "dL_dW1, dL_dW2, dL_dW3, dL_dW4, dL_dW5, dL_db1, dL_db2, dL_db3, dL_db4, dL_db5 =\\\n",
    "grad_vec2_nb2(X, y, q_W1, q_W2, q_W3, p_W4, p_W5, q_b1, q_b2, q_b3, p_b4, p_b5, q_h1, p_h2, q_mu, q_s2, z, eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var manual grad func\n",
      "W5:  54.452012250294494 54.45201226235029\n",
      "b5:  291.84314797703337 291.84314797754985\n",
      "W4:  -1.1431547704887635 -1.143154759753081\n",
      "b4:  -8.464074215908113 -8.464074213740366\n",
      "W3:  1.9408656044106463 1.9408656001786984\n",
      "b3:  0.3268551154178567 0.326855132992273\n",
      "W2:  -9.200528995734203 -9.200529000943583\n",
      "b2:  -1.2780836300407827 -1.2780836079548612\n",
      "W1:  -139.95724227129358 -139.9571582101807\n",
      "b1:  -1.347006144669649 -1.3470061525999781\n"
     ]
    }
   ],
   "source": [
    "# gradient checking\n",
    "p_W5_add = p_W5 + delta\n",
    "p_W5_red = p_W5 - delta\n",
    "\n",
    "p_b5_add = p_b5 + delta\n",
    "p_b5_red = p_b5 - delta\n",
    "\n",
    "p_W4_add = p_W4 + delta\n",
    "p_W4_red = p_W4 - delta\n",
    "\n",
    "p_b4_add = p_b4 + delta\n",
    "p_b4_red = p_b4 - delta\n",
    "\n",
    "q_W3_add = q_W3 + delta\n",
    "q_W3_red = q_W3 - delta\n",
    "\n",
    "q_b3_add = q_b3 + delta\n",
    "q_b3_red = q_b3 - delta\n",
    "\n",
    "q_W2_add = q_W2 + delta\n",
    "q_W2_red = q_W2 - delta\n",
    "\n",
    "q_b2_add = q_b2 + delta\n",
    "q_b2_red = q_b2 - delta\n",
    "\n",
    "q_W1_add = q_W1 + delta\n",
    "q_W1_red = q_W1 - delta\n",
    "\n",
    "q_b1_add = q_b1 + delta\n",
    "q_b1_red = q_b1 - delta\n",
    "\n",
    "W5_add = [q_W1, q_W2, q_W3, p_W4, p_W5_add]\n",
    "W5_red = [q_W1, q_W2, q_W3, p_W4, p_W5_red]\n",
    "\n",
    "b5_add = [q_b1, q_b2, q_b3, p_b4, p_b5_add]\n",
    "b5_red = [q_b1, q_b2, q_b3, p_b4, p_b5_red]\n",
    "\n",
    "W4_add = [q_W1, q_W2, q_W3, p_W4_add, p_W5]\n",
    "W4_red = [q_W1, q_W2, q_W3, p_W4_red, p_W5]\n",
    "\n",
    "b4_add = [q_b1, q_b2, q_b3, p_b4_add, p_b5]\n",
    "b4_red = [q_b1, q_b2, q_b3, p_b4_red, p_b5]\n",
    "\n",
    "W3_add = [q_W1, q_W2, q_W3_add, p_W4, p_W5]\n",
    "W3_red = [q_W1, q_W2, q_W3_red, p_W4, p_W5]\n",
    "\n",
    "b3_add = [q_b1, q_b2, q_b3_add, p_b4, p_b5]\n",
    "b3_red = [q_b1, q_b2, q_b3_red, p_b4, p_b5]\n",
    "\n",
    "W2_add = [q_W1, q_W2_add, q_W3, p_W4, p_W5]\n",
    "W2_red = [q_W1, q_W2_red, q_W3, p_W4, p_W5]\n",
    "\n",
    "b2_add = [q_b1, q_b2_add, q_b3, p_b4, p_b5]\n",
    "b2_red = [q_b1, q_b2_red, q_b3, p_b4, p_b5]\n",
    "\n",
    "W1_add = [q_W1_add, q_W2, q_W3, p_W4, p_W5]\n",
    "W1_red = [q_W1_red, q_W2, q_W3, p_W4, p_W5]\n",
    "\n",
    "b1_add = [q_b1_add, q_b2, q_b3, p_b4, p_b5]\n",
    "b1_red = [q_b1_red, q_b2, q_b3, p_b4, p_b5]\n",
    "\n",
    "d,d,d,loss_W5_add = batch_forward(Spec, X, W5_add, b, eps)\n",
    "d,d,d,loss_W5_red = batch_forward(Spec, X, W5_red, b, eps)\n",
    "\n",
    "d,d,d,loss_b5_add = batch_forward(Spec, X, W, b5_add, eps)\n",
    "d,d,d,loss_b5_red = batch_forward(Spec, X, W, b5_red, eps)\n",
    "\n",
    "d,d,d,loss_W4_add = batch_forward(Spec, X, W4_add, b, eps)\n",
    "d,d,d,loss_W4_red = batch_forward(Spec, X, W4_red, b, eps)\n",
    "\n",
    "d,d,d,loss_b4_add = batch_forward(Spec, X, W, b4_add, eps)\n",
    "d,d,d,loss_b4_red = batch_forward(Spec, X, W, b4_red, eps)\n",
    "\n",
    "d,d,d,loss_W3_add = batch_forward(Spec, X, W3_add, b, eps)\n",
    "d,d,d,loss_W3_red = batch_forward(Spec, X, W3_red, b, eps)\n",
    "\n",
    "d,d,d,loss_b3_add = batch_forward(Spec, X, W, b3_add, eps)\n",
    "d,d,d,loss_b3_red = batch_forward(Spec, X, W, b3_red, eps)\n",
    "\n",
    "d,d,d,loss_W2_add = batch_forward(Spec, X, W2_add, b, eps)\n",
    "d,d,d,loss_W2_red = batch_forward(Spec, X, W2_red, b, eps)\n",
    "\n",
    "d,d,d,loss_b2_add = batch_forward(Spec, X, W, b2_add, eps)\n",
    "d,d,d,loss_b2_red = batch_forward(Spec, X, W, b2_red, eps)\n",
    "\n",
    "d,d,d,loss_W1_add = batch_forward(Spec, X, W1_add, b, eps)\n",
    "d,d,d,loss_W1_red = batch_forward(Spec, X, W1_red, b, eps)\n",
    "\n",
    "d,d,d,loss_b1_add = batch_forward(Spec, X, W, b1_add, eps)\n",
    "d,d,d,loss_b1_red = batch_forward(Spec, X, W, b1_red, eps)\n",
    "\n",
    "print(\"var\", \"manual\", \"grad func\")\n",
    "print(\"W5: \", (loss_W5_add - loss_W5_red)/2/delta, np.sum(dL_dW5))\n",
    "print(\"b5: \", (loss_b5_add - loss_b5_red)/2/delta, np.sum(dL_db5))\n",
    "print(\"W4: \", (loss_W4_add - loss_W4_red)/2/delta, np.sum(dL_dW4))\n",
    "print(\"b4: \", (loss_b4_add - loss_b4_red)/2/delta, np.sum(dL_db4))\n",
    "print(\"W3: \", (loss_W3_add - loss_W3_red)/2/delta, np.sum(dL_dW3))\n",
    "print(\"b3: \", (loss_b3_add - loss_b3_red)/2/delta, np.sum(dL_db3))\n",
    "print(\"W2: \", (loss_W2_add - loss_W2_red)/2/delta, np.sum(dL_dW2))\n",
    "print(\"b2: \", (loss_b2_add - loss_b2_red)/2/delta, np.sum(dL_db2))\n",
    "print(\"W1: \", (loss_W1_add - loss_W1_red)/2/delta, np.sum(dL_dW1))\n",
    "print(\"b1: \", (loss_b1_add - loss_b1_red)/2/delta, np.sum(dL_db1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Optimized functions vs original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "nBatch = 1 # number of mini-batch to train\n",
    "M = 100 # batch size\n",
    "L = 2 # sample size\n",
    "std_const = 255 # to standardize data\n",
    "\n",
    "Xdim1, Xdim2 = trainX[0].shape[0], trainX[0].shape[1]\n",
    "dx = Xdim1 * Xdim2 # dimension of the input\n",
    "dm = 500 # dimension of the hidden layer\n",
    "dz = 3 # dimension of latent variable\n",
    "\n",
    "alpha = 0.005 # learning rate\n",
    "\n",
    "nP = 1000 # print out status every nP batches\n",
    "\n",
    "Spec = [nBatch, M, L, std_const, dx, dm, dz, alpha, nP]\n",
    "\n",
    "W,b = init_random(dx, dm, dz)\n",
    "q_W1, q_W2, q_W3, p_W4, p_W5 = W\n",
    "q_b1, q_b2, q_b3, p_b4, p_b5 = b\n",
    "eps = np.random.randn(L, dz)\n",
    "batchX, batchy = get_Batch(M, trainX, trainy)\n",
    "X = batchX.reshape(M, dx) / std_const"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 batch_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_original, H_original, Lt_original, loss_original = batch_forward(Spec, X, W, b, eps)\n",
    "q_h1_original, p_h2_original = H_original\n",
    "q_mu_original, q_s2_original, z_original, eps_original = Lt_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_new, q_h1_new, p_h2_new, q_mu_new, q_s2_new, z_new, loss_new = batch_forward_vec_nb2(Spec, X, W, b, eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_new, q_h1_new, p_h2_new, q_mu_new, q_s2_new, z_new, loss_new = \\\n",
    "batch_forward_nb2(M, L, dm, dz, X, q_W1, q_W2, q_W3, p_W4, p_W5, q_b1, q_b2, q_b3, p_b4, p_b5, eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(np.allclose(y_original, y_new))\n",
    "print(np.allclose(q_h1_original, q_h1_new))\n",
    "print(np.allclose(p_h2_original, p_h2_new))\n",
    "print(np.allclose(q_mu_original, q_mu_new))\n",
    "print(np.allclose(q_s2_original, q_s2_new))\n",
    "print(np.allclose(z_original, z_new))\n",
    "print(np.allclose(loss_original, loss_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, H, Lt, loss = batch_forward(Spec, X, W, b, eps)\n",
    "dW_ori, db_ori = grad(X, y, W, b, H, Lt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "nBatch, M, L, std_const, dx, dm, dz, alpha, nP = Spec\n",
    "q_W1, q_W2, q_W3, p_W4, p_W5 = W\n",
    "q_b1, q_b2, q_b3, p_b4, p_b5 = b\n",
    "q_h1, p_h2 = H\n",
    "q_mu, q_s2, z, eps = Lt\n",
    "dJ_dW1, dJ_dW2, dJ_dW3, dJ_dW4, dJ_dW5, dJ_db1, dJ_db2, dJ_db3, dJ_db4, dJ_db5 = \\\n",
    "grad_nb2(X, y, q_W1, q_W2, q_W3, p_W4, p_W5, q_b1, q_b2, q_b3, p_b4, p_b5, q_h1, p_h2, q_mu, q_s2, z, eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(np.allclose(dW_ori[0], dJ_dW1))\n",
    "print(np.allclose(dW_ori[1], dJ_dW2))\n",
    "print(np.allclose(dW_ori[2], dJ_dW3))\n",
    "print(np.allclose(dW_ori[3], dJ_dW4))\n",
    "print(np.allclose(dW_ori[4], dJ_dW5))\n",
    "\n",
    "print(np.allclose(db_ori[0], dJ_db1))\n",
    "print(np.allclose(db_ori[1], dJ_db2))\n",
    "print(np.allclose(db_ori[2], dJ_db3))\n",
    "print(np.allclose(db_ori[3], dJ_db4))\n",
    "print(np.allclose(db_ori[4], dJ_db5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't unbox heterogeneous list: array(float64, 2d, C) != array(float64, 3d, C)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-57b837c8f44b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdW_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_vec_nb1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: can't unbox heterogeneous list: array(float64, 2d, C) != array(float64, 3d, C)"
     ]
    }
   ],
   "source": [
    "dW_new, db_new = grad_vec_nb1(X, y, W, b, H, Lt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(np.allclose(dW_ori[i], dW_new[i]))\n",
    "    print(np.allclose(db_ori[i], db_new[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. sample_z function testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test sample_z\n",
    "a = np.array([1,2])\n",
    "b = np.array([4,9])\n",
    "eps = np.array([0.1,0.2])\n",
    "\n",
    "sample_z(a,b,eps) # returns (dz,)\n",
    "\n",
    "tz = np.zeros((M, L, dz))\n",
    "tz[0,0,] = sample_z(a,b,eps)\n",
    "tz[0,0,].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. parameter update function testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check parameter update\n",
    "W1, b1 = para_update(W, b, dW, db, alpha)\n",
    "q_W1_n, q_W2_n, q_W3_n, p_W4_n, p_W5_n = W1\n",
    "q_b1_n, q_b2_n, q_b3_n, p_b4_n, p_b5_n = b1\n",
    "\n",
    "dJ_dW1, dJ_dW2, dJ_dW3, dJ_dW4, dJ_dW5 = dW\n",
    "dJ_db1, dJ_db2, dJ_db3, dJ_db4, dJ_db5 = db\n",
    "\n",
    "print(np.allclose(q_W1_n+alpha*dJ_dW1, q_W1))\n",
    "print(np.allclose(q_b1_n+alpha*dJ_db1, q_b1))\n",
    "print(np.allclose(q_W2_n+alpha*dJ_dW2, q_W2))\n",
    "print(np.allclose(q_b2_n+alpha*dJ_db2, q_b2))\n",
    "print(np.allclose(q_W3_n+alpha*dJ_dW3, q_W3))\n",
    "print(np.allclose(q_b3_n+alpha*dJ_db3, q_b3))\n",
    "print(np.allclose(p_W4_n+alpha*dJ_dW4, p_W4))\n",
    "print(np.allclose(p_b4_n+alpha*dJ_db4, p_b4))\n",
    "print(np.allclose(p_W5_n+alpha*dJ_dW5, p_W5))\n",
    "print(np.allclose(p_b5_n+alpha*dJ_db5, p_b5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before any change:  1 1\n",
      "after chancing a2[0] only:  7 7\n"
     ]
    }
   ],
   "source": [
    "# test for function return list\n",
    "\"\"\"\n",
    "during implementation of ADAM, I first initiated all the s/v/sc/vc variables in one line of codes \n",
    "by assigning them to the same function returns from init_random.\n",
    "Later I realized in this way, they share the same reference that by changing one of them,\n",
    "I'll accidentally change the values of everything else.\n",
    "This test is to prove this.\n",
    "\"\"\"\n",
    "def test_by_ref():\n",
    "    a = [1,2,3]\n",
    "    b = [4,5,6]\n",
    "    return a,b\n",
    "\n",
    "a1,b1 = a2, b2 = test_by_ref()\n",
    "print(\"before any change: \", a1[0], a2[0])\n",
    "a2[0] = a2[0]* 2 + 5\n",
    "print(\"after chancing a2[0] only: \",a1[0], a2[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
